{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"ragcrawl","text":"<p> Recursive website crawler producing LLM-ready knowledge base artifacts </p> <p>ragcrawl is a Python library for crawling websites and producing clean, structured content optimized for Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) systems.</p> <ul> <li> <p> Quick Start</p> <p>Get up and running in minutes with our quickstart guide.</p> <p> Getting Started</p> </li> <li> <p> User Guide</p> <p>Learn how to crawl websites, sync updates, and export content.</p> <p> User Guide</p> </li> <li> <p> Configuration</p> <p>Customize crawler behavior, storage, and output formats.</p> <p> Configuration</p> </li> <li> <p> API Reference</p> <p>Complete Python API documentation with examples.</p> <p> API Reference</p> </li> </ul>"},{"location":"#features","title":"Features","text":""},{"location":"#clean-markdown-output","title":"Clean Markdown Output","text":"<p>Convert web pages to clean, readable Markdown while preserving semantic structure like headings, code blocks, and lists.</p> Python<pre><code>from ragcrawl import CrawlJob, CrawlerConfig\n\nconfig = CrawlerConfig(\n    seeds=[\"https://docs.example.com\"],\n    max_pages=100,\n)\n\njob = CrawlJob(config)\nresult = await job.run()\n\nfor doc in result.documents:\n    print(f\"# {doc.title}\\n{doc.markdown[:200]}...\")\n</code></pre>"},{"location":"#incremental-sync","title":"Incremental Sync","text":"<p>Efficiently update your knowledge base with only changed content using sitemap detection, ETags, and content hashing.</p> Python<pre><code>from ragcrawl import SyncJob, SyncConfig\n\nconfig = SyncConfig(\n    site_id=\"site_abc123\",\n    use_sitemap=True,\n)\n\njob = SyncJob(config)\nresult = await job.run()\n\nprint(f\"New: {result.stats.pages_new}\")\nprint(f\"Updated: {result.stats.pages_changed}\")\n</code></pre>"},{"location":"#rag-ready-chunking","title":"RAG-Ready Chunking","text":"<p>Built-in chunking strategies optimized for embedding models with heading-aware and token-based options.</p> Python<pre><code>from ragcrawl.chunking import HeadingChunker\n\nchunker = HeadingChunker(max_tokens=500)\nchunks = chunker.chunk_documents(result.documents)\n\nfor chunk in chunks:\n    # Ready for embedding API\n    print(f\"Section: {chunk.section_path}\")\n    print(f\"Tokens: {chunk.token_estimate}\")\n</code></pre>"},{"location":"#flexible-storage","title":"Flexible Storage","text":"<p>Choose between DuckDB for local development or DynamoDB for cloud deployments.</p> DuckDB (Local)DynamoDB (Cloud) Python<pre><code>from ragcrawl.config import StorageConfig, DuckDBConfig\n\nconfig = StorageConfig(\n    backend=DuckDBConfig(path=\"./crawler.duckdb\")\n)\n</code></pre> Python<pre><code>from ragcrawl.config import StorageConfig, DynamoDBConfig\n\nconfig = StorageConfig(\n    backend=DynamoDBConfig(\n        table_prefix=\"ragcrawl_\",\n        region=\"us-west-2\",\n    )\n)\n</code></pre>"},{"location":"#installation","title":"Installation","text":"pipuv Bash<pre><code># Basic installation\npip install ragcrawl\n\n# With browser rendering\npip install ragcrawl[browser]\n\n# With DynamoDB support\npip install ragcrawl[dynamodb]\n\n# Full installation\npip install ragcrawl[all]\n</code></pre> Bash<pre><code>uv pip install ragcrawl\n</code></pre>"},{"location":"#cli-quick-start","title":"CLI Quick Start","text":"Bash<pre><code># Crawl a documentation site\nragcrawl crawl https://docs.example.com --max-pages 100\n\n# Sync for updates\nragcrawl sync site_abc123\n\n# List crawled sites\nragcrawl sites\n\n# View crawl history\nragcrawl runs site_abc123\n</code></pre>"},{"location":"#use-cases","title":"Use Cases","text":"Use Case Description Documentation RAG Build Q&amp;A systems from technical docs Knowledge Base Create searchable internal wikis Content Migration Extract structured content from websites Research Collect and analyze web content"},{"location":"#architecture","title":"Architecture","text":"Text Only<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Fetcher   \u2502\u2500\u2500\u2500\u2500\u25b6\u2502 Extractor\u2502\u2500\u2500\u2500\u2500\u25b6\u2502  Storage  \u2502\n\u2502 (HTTP/Browser)    \u2502 (HTML\u2192MD)\u2502     \u2502(DuckDB/Dyn)\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502                                    \u2502\n       \u25bc                                    \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Frontier   \u2502     \u2502 Chunker  \u2502\u25c0\u2500\u2500\u2500\u2500\u2502  Export   \u2502\n\u2502   Queue     \u2502     \u2502(Heading/ \u2502     \u2502(JSON/JSONL)\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502  Token)  \u2502     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                          \u2502\n                          \u25bc\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502 Publisher \u2502\n                    \u2502(Single/   \u2502\n                    \u2502  Multi)   \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li> <p> Installation</p> <p>Detailed installation instructions</p> </li> <li> <p> Quickstart</p> <p>Start crawling in 5 minutes</p> </li> <li> <p> CLI Reference</p> <p>Command-line interface guide</p> </li> <li> <p> GitHub</p> <p>Report issues and contribute</p> </li> </ul>"},{"location":"#community","title":"Community","text":"<p>We welcome contributions from the community! Here's how you can get involved:</p> <ul> <li> <p> Contributing</p> <p>Learn how to contribute code, documentation, and more</p> </li> <li> <p> Code of Conduct</p> <p>Our community standards and expectations</p> </li> <li> <p> Support</p> <p>Get help and report issues</p> </li> <li> <p> Changelog</p> <p>Release history and updates</p> </li> </ul>"},{"location":"#license","title":"License","text":"<p>ragcrawl is licensed under the Apache License 2.0. See the LICENSE file for details.</p>"},{"location":"api/","title":"API Reference","text":"<p>Complete Python API documentation for ragcrawl.</p>"},{"location":"api/#overview","title":"Overview","text":"<p>ragcrawl is organized into the following modules:</p> Module Description Core Main entry points (CrawlJob, SyncJob) Models Data models (Document, Page, Chunk, etc.) Storage Storage backends (DuckDB, DynamoDB) Chunking Content chunking (HeadingChunker, TokenChunker) Export Export and publishing (JSON, Markdown) Filters URL filtering and normalization"},{"location":"api/#quick-reference","title":"Quick Reference","text":""},{"location":"api/#crawling","title":"Crawling","text":"Python<pre><code>import asyncio\nfrom ragcrawl import CrawlJob\nfrom ragcrawl.config import CrawlerConfig\n\nconfig = CrawlerConfig(\n    seeds=[\"https://docs.example.com\"],\n    max_pages=100,\n    max_depth=5,\n)\n\njob = CrawlJob(config)\nresult = asyncio.run(job.run())\n\nprint(f\"Crawled {result.stats.pages_crawled} pages\")\n</code></pre>"},{"location":"api/#syncing","title":"Syncing","text":"Python<pre><code>import asyncio\nfrom ragcrawl import SyncJob\nfrom ragcrawl.config import SyncConfig\n\nconfig = SyncConfig(\n    site_id=\"site_abc123\",\n    use_sitemap=True,\n)\n\njob = SyncJob(config)\nresult = asyncio.run(job.run())\n\nprint(f\"Updated {result.stats.pages_changed} pages\")\n</code></pre>"},{"location":"api/#chunking","title":"Chunking","text":"Python<pre><code>from ragcrawl.chunking import HeadingChunker\n\nchunker = HeadingChunker(max_tokens=500)\nchunks = chunker.chunk_documents(result.documents)\n</code></pre>"},{"location":"api/#exporting","title":"Exporting","text":"Python<pre><code>from ragcrawl.export import JSONLExporter\nfrom pathlib import Path\n\nexporter = JSONLExporter()\nexporter.export_documents(result.documents, Path(\"output.jsonl\"))\n</code></pre>"},{"location":"api/#module-documentation","title":"Module Documentation","text":"<ul> <li> <p> Core</p> <p><code>CrawlJob</code> and <code>SyncJob</code> - main entry points for crawling operations.</p> </li> <li> <p> Models</p> <p>Data models: <code>Document</code>, <code>Page</code>, <code>PageVersion</code>, <code>Chunk</code>, <code>Site</code>, <code>CrawlRun</code></p> </li> <li> <p> Storage</p> <p>Storage backends: <code>DuckDBBackend</code>, <code>DynamoDBBackend</code></p> </li> <li> <p> Chunking</p> <p>Content chunkers: <code>HeadingChunker</code>, <code>TokenChunker</code></p> </li> <li> <p> Export</p> <p>Exporters and publishers: <code>JSONExporter</code>, <code>SinglePagePublisher</code>, <code>MultiPagePublisher</code></p> </li> <li> <p> Filters</p> <p>URL filtering: <code>LinkFilter</code>, <code>PatternMatcher</code>, <code>URLNormalizer</code></p> </li> </ul>"},{"location":"api/#configuration-classes","title":"Configuration Classes","text":"Class Module Description <code>CrawlerConfig</code> <code>ragcrawl.config</code> Crawl settings <code>SyncConfig</code> <code>ragcrawl.config</code> Sync settings <code>StorageConfig</code> <code>ragcrawl.config</code> Storage backend config <code>OutputConfig</code> <code>ragcrawl.config</code> Output format config <code>DuckDBConfig</code> <code>ragcrawl.config.storage_config</code> DuckDB settings <code>DynamoDBConfig</code> <code>ragcrawl.config.storage_config</code> DynamoDB settings"},{"location":"api/#imports","title":"Imports","text":"Python<pre><code># Main classes\nfrom ragcrawl import CrawlJob, SyncJob\nfrom ragcrawl.config import CrawlerConfig, SyncConfig\n\n# Storage\nfrom ragcrawl.config.storage_config import StorageConfig, DuckDBConfig, DynamoDBConfig\nfrom ragcrawl.storage import create_storage_backend\n\n# Models\nfrom ragcrawl.models import Document, Page, PageVersion, Chunk, Site, CrawlRun\n\n# Chunking\nfrom ragcrawl.chunking import HeadingChunker, TokenChunker\n\n# Export\nfrom ragcrawl.export import JSONExporter, JSONLExporter\nfrom ragcrawl.output import SinglePagePublisher, MultiPagePublisher\n\n# Filters\nfrom ragcrawl.filters import LinkFilter, PatternMatcher, URLNormalizer\n</code></pre>"},{"location":"api/#type-hints","title":"Type Hints","text":"<p>ragcrawl is fully typed. Enable type checking in your IDE:</p> Python<pre><code>from ragcrawl import CrawlJob\nfrom ragcrawl.config import CrawlerConfig\n\nconfig: CrawlerConfig = CrawlerConfig(...)\njob: CrawlJob = CrawlJob(config)\n</code></pre>"},{"location":"api/#async-api","title":"Async API","text":"<p>Core operations are async:</p> Python<pre><code>import asyncio\n\nasync def main():\n    job = CrawlJob(config)\n    result = await job.run()\n    return result\n\nresult = asyncio.run(main())\n</code></pre> <p>Or use the sync wrapper:</p> Python<pre><code>from ragcrawl import CrawlJob\n\njob = CrawlJob(config)\nresult = asyncio.run(job.run())\n</code></pre>"},{"location":"api/chunking/","title":"Chunking API","text":"<p>ragcrawl provides chunking utilities to prepare content for embedding models.</p>"},{"location":"api/chunking/#overview","title":"Overview","text":"Chunker Description Use Case <code>HeadingChunker</code> Splits by markdown headings Preserve document structure <code>TokenChunker</code> Splits by token count Fixed-size chunks for embeddings"},{"location":"api/chunking/#headingchunker","title":"HeadingChunker","text":"<p>Splits content at heading boundaries while respecting token limits.</p>"},{"location":"api/chunking/#usage","title":"Usage","text":"Python<pre><code>from ragcrawl.chunking import HeadingChunker\nfrom ragcrawl.models import Document\n\nchunker = HeadingChunker(\n    max_tokens=500,\n    min_chunk_chars=100,\n    heading_levels=[1, 2, 3],\n)\n\n# Chunk a single document\nchunks = chunker.chunk(document)\n\n# Chunk multiple documents\nall_chunks = chunker.chunk_documents(documents)\n\nfor chunk in chunks:\n    print(f\"Section: {' &gt; '.join(chunk.section_path)}\")\n    print(f\"Tokens: ~{chunk.token_estimate}\")\n    print(chunk.content[:200])\n    print()\n</code></pre>"},{"location":"api/chunking/#configuration","title":"Configuration","text":"Option Type Default Description <code>max_tokens</code> int 500 Maximum tokens per chunk <code>min_chunk_chars</code> int 100 Minimum characters for a chunk <code>heading_levels</code> list[int] [1,2,3] Heading levels to split on <code>include_heading</code> bool True Include heading in chunk <code>preserve_code_blocks</code> bool True Keep code blocks intact"},{"location":"api/chunking/#tokenchunker","title":"TokenChunker","text":"<p>Splits content into fixed-size chunks by token count.</p>"},{"location":"api/chunking/#usage_1","title":"Usage","text":"Python<pre><code>from ragcrawl.chunking import TokenChunker\n\nchunker = TokenChunker(\n    max_tokens=500,\n    overlap_tokens=50,\n    encoding_name=\"cl100k_base\",\n)\n\nchunks = chunker.chunk(document)\n\nfor chunk in chunks:\n    print(f\"Chunk {chunk.chunk_index + 1}/{chunk.total_chunks}\")\n    print(f\"Tokens: {chunk.token_estimate}\")\n</code></pre>"},{"location":"api/chunking/#configuration_1","title":"Configuration","text":"Option Type Default Description <code>max_tokens</code> int 500 Maximum tokens per chunk <code>overlap_tokens</code> int 50 Overlap between chunks <code>encoding_name</code> str \"cl100k_base\" Tokenizer encoding"},{"location":"api/chunking/#choosing-a-chunker","title":"Choosing a Chunker","text":""},{"location":"api/chunking/#use-headingchunker-when","title":"Use HeadingChunker when:","text":"<ul> <li>Document has clear heading structure</li> <li>Semantic boundaries are important</li> <li>Context preservation matters</li> <li>Building hierarchical indexes</li> </ul>"},{"location":"api/chunking/#use-tokenchunker-when","title":"Use TokenChunker when:","text":"<ul> <li>Fixed chunk sizes needed</li> <li>Document lacks clear structure</li> <li>Maximum control over chunk boundaries</li> <li>Optimizing for specific embedding models</li> </ul>"},{"location":"api/chunking/#custom-chunking","title":"Custom Chunking","text":"<p>Implement the <code>Chunker</code> protocol for custom logic:</p> Python<pre><code>from ragcrawl.chunking import Chunker\nfrom ragcrawl.models import Document, Chunk\n\nclass CustomChunker(Chunker):\n    def chunk(self, document: Document) -&gt; list[Chunk]:\n        chunks = []\n        # Your chunking logic here\n        return chunks\n\n    def chunk_documents(self, documents: list[Document]) -&gt; list[Chunk]:\n        all_chunks = []\n        for doc in documents:\n            all_chunks.extend(self.chunk(doc))\n        return all_chunks\n</code></pre>"},{"location":"api/chunking/#integration-with-embedding-apis","title":"Integration with Embedding APIs","text":"Python<pre><code>from ragcrawl.chunking import HeadingChunker\nimport openai\n\nchunker = HeadingChunker(max_tokens=500)\nchunks = chunker.chunk_documents(documents)\n\n# Prepare texts with context\ntexts = []\nfor chunk in chunks:\n    # Add section context as prefix\n    prefix = \" &gt; \".join(chunk.section_path) if chunk.section_path else \"\"\n    text = f\"[{prefix}]\\n\\n{chunk.content}\" if prefix else chunk.content\n    texts.append(text)\n\n# Get embeddings\nresponse = openai.embeddings.create(\n    model=\"text-embedding-3-small\",\n    input=texts,\n)\n\n# Store with metadata\nfor i, embedding in enumerate(response.data):\n    store_vector(\n        id=chunks[i].chunk_id,\n        vector=embedding.embedding,\n        metadata={\n            \"doc_id\": chunks[i].doc_id,\n            \"section\": chunks[i].section_path,\n            \"heading\": chunks[i].heading,\n        }\n    )\n</code></pre>"},{"location":"api/chunking/#module-reference","title":"Module Reference","text":"<p>Content chunking for RAG pipelines.</p>"},{"location":"api/chunking/#ragcrawl.chunking.HeadingChunker","title":"HeadingChunker","text":"Python<pre><code>HeadingChunker(\n    min_chunk_size: int = 100,\n    max_chunk_size: int = 2000,\n    heading_levels: list[int] | None = None,\n    include_heading_in_chunk: bool = True,\n    overlap_size: int = 0,\n)\n</code></pre> <p>               Bases: <code>Chunker</code></p> <p>Chunks markdown content by headings.</p> <p>Creates chunks that respect document structure by splitting at heading boundaries while maintaining context.</p> <p>Initialize heading chunker.</p> PARAMETER DESCRIPTION <code>min_chunk_size</code> <p>Minimum chunk size in characters.</p> <p> TYPE: <code>int</code> DEFAULT: <code>100</code> </p> <code>max_chunk_size</code> <p>Maximum chunk size in characters.</p> <p> TYPE: <code>int</code> DEFAULT: <code>2000</code> </p> <code>heading_levels</code> <p>Heading levels to split on (default: [1, 2, 3]).</p> <p> TYPE: <code>list[int] | None</code> DEFAULT: <code>None</code> </p> <code>include_heading_in_chunk</code> <p>Include the heading in chunk content.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>overlap_size</code> <p>Characters to overlap between chunks.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> Source code in <code>src/ragcrawl/chunking/heading_chunker.py</code> Python<pre><code>def __init__(\n    self,\n    min_chunk_size: int = 100,\n    max_chunk_size: int = 2000,\n    heading_levels: list[int] | None = None,\n    include_heading_in_chunk: bool = True,\n    overlap_size: int = 0,\n) -&gt; None:\n    \"\"\"\n    Initialize heading chunker.\n\n    Args:\n        min_chunk_size: Minimum chunk size in characters.\n        max_chunk_size: Maximum chunk size in characters.\n        heading_levels: Heading levels to split on (default: [1, 2, 3]).\n        include_heading_in_chunk: Include the heading in chunk content.\n        overlap_size: Characters to overlap between chunks.\n    \"\"\"\n    self.min_chunk_size = min_chunk_size\n    self.max_chunk_size = max_chunk_size\n    self.heading_levels = heading_levels or [1, 2, 3]\n    self.include_heading_in_chunk = include_heading_in_chunk\n    self.overlap_size = overlap_size\n\n    # Regex for markdown headings\n    self._heading_pattern = re.compile(\n        r\"^(#{1,6})\\s+(.+?)$\", re.MULTILINE\n    )\n</code></pre>"},{"location":"api/chunking/#ragcrawl.chunking.HeadingChunker.chunk","title":"chunk","text":"Python<pre><code>chunk(document: Document) -&gt; list[Chunk]\n</code></pre> <p>Chunk document by headings.</p> PARAMETER DESCRIPTION <code>document</code> <p>Document to chunk.</p> <p> TYPE: <code>Document</code> </p> RETURNS DESCRIPTION <code>list[Chunk]</code> <p>List of chunks.</p> Source code in <code>src/ragcrawl/chunking/heading_chunker.py</code> Python<pre><code>def chunk(self, document: Document) -&gt; list[Chunk]:\n    \"\"\"\n    Chunk document by headings.\n\n    Args:\n        document: Document to chunk.\n\n    Returns:\n        List of chunks.\n    \"\"\"\n    content = document.markdown\n    if not content:\n        return []\n\n    # Parse sections\n    sections = self._parse_sections(content)\n\n    if not sections:\n        # No headings found, treat as single chunk\n        return self._create_single_chunk(document, content)\n\n    # Create chunks from sections\n    chunks = []\n    section_path_stack: list[str] = []\n\n    for i, section in enumerate(sections):\n        # Update section path\n        while section_path_stack and len(section_path_stack) &gt;= section.level:\n            section_path_stack.pop()\n        section_path_stack.append(section.heading)\n        section_path = \" &gt; \".join(section_path_stack)\n\n        # Build chunk content\n        if self.include_heading_in_chunk:\n            chunk_content = f\"{'#' * section.level} {section.heading}\\n\\n{section.content}\"\n        else:\n            chunk_content = section.content\n\n        # Handle oversized sections\n        if len(chunk_content) &gt; self.max_chunk_size:\n            sub_chunks = self._split_large_section(\n                document,\n                chunk_content,\n                section,\n                section_path,\n                len(chunks),\n            )\n            chunks.extend(sub_chunks)\n        elif len(chunk_content) &gt;= self.min_chunk_size:\n            chunk = self._create_chunk(\n                document=document,\n                content=chunk_content,\n                index=len(chunks),\n                start_offset=section.start_offset,\n                end_offset=section.end_offset,\n                section_path=section_path,\n                heading=section.heading,\n                heading_level=section.level,\n            )\n            chunks.append(chunk)\n        # else: skip chunks that are too small\n\n    # Update total_chunks\n    for chunk in chunks:\n        chunk.total_chunks = len(chunks)\n\n    return chunks\n</code></pre>"},{"location":"api/chunking/#ragcrawl.chunking.HeadingChunker.estimate_tokens","title":"estimate_tokens","text":"Python<pre><code>estimate_tokens(text: str) -&gt; int\n</code></pre> <p>Estimate token count (roughly 4 chars per token).</p> Source code in <code>src/ragcrawl/chunking/heading_chunker.py</code> Python<pre><code>def estimate_tokens(self, text: str) -&gt; int:\n    \"\"\"Estimate token count (roughly 4 chars per token).\"\"\"\n    return len(text) // 4\n</code></pre>"},{"location":"api/chunking/#ragcrawl.chunking.TokenChunker","title":"TokenChunker","text":"Python<pre><code>TokenChunker(\n    chunk_size: int = 512,\n    chunk_overlap: int = 50,\n    encoding_name: str = \"cl100k_base\",\n    separators: list[str] | None = None,\n)\n</code></pre> <p>               Bases: <code>Chunker</code></p> <p>Chunks content by token count.</p> <p>Uses tiktoken for accurate token counting and respects natural text boundaries (sentences, paragraphs).</p> <p>Initialize token chunker.</p> PARAMETER DESCRIPTION <code>chunk_size</code> <p>Target chunk size in tokens.</p> <p> TYPE: <code>int</code> DEFAULT: <code>512</code> </p> <code>chunk_overlap</code> <p>Token overlap between chunks.</p> <p> TYPE: <code>int</code> DEFAULT: <code>50</code> </p> <code>encoding_name</code> <p>Tiktoken encoding name.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'cl100k_base'</code> </p> <code>separators</code> <p>Text separators to try, in order.</p> <p> TYPE: <code>list[str] | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/ragcrawl/chunking/token_chunker.py</code> Python<pre><code>def __init__(\n    self,\n    chunk_size: int = 512,\n    chunk_overlap: int = 50,\n    encoding_name: str = \"cl100k_base\",\n    separators: list[str] | None = None,\n) -&gt; None:\n    \"\"\"\n    Initialize token chunker.\n\n    Args:\n        chunk_size: Target chunk size in tokens.\n        chunk_overlap: Token overlap between chunks.\n        encoding_name: Tiktoken encoding name.\n        separators: Text separators to try, in order.\n    \"\"\"\n    self.chunk_size = chunk_size\n    self.chunk_overlap = chunk_overlap\n    self.encoding_name = encoding_name\n    self.separators = separators or [\"\\n\\n\", \"\\n\", \". \", \" \"]\n\n    self._encoding = None\n</code></pre>"},{"location":"api/chunking/#ragcrawl.chunking.TokenChunker.encoding","title":"encoding  <code>property</code>","text":"Python<pre><code>encoding\n</code></pre> <p>Get or create tiktoken encoding.</p>"},{"location":"api/chunking/#ragcrawl.chunking.TokenChunker.chunk","title":"chunk","text":"Python<pre><code>chunk(document: Document) -&gt; list[Chunk]\n</code></pre> <p>Chunk document by token count.</p> PARAMETER DESCRIPTION <code>document</code> <p>Document to chunk.</p> <p> TYPE: <code>Document</code> </p> RETURNS DESCRIPTION <code>list[Chunk]</code> <p>List of chunks.</p> Source code in <code>src/ragcrawl/chunking/token_chunker.py</code> Python<pre><code>def chunk(self, document: Document) -&gt; list[Chunk]:\n    \"\"\"\n    Chunk document by token count.\n\n    Args:\n        document: Document to chunk.\n\n    Returns:\n        List of chunks.\n    \"\"\"\n    content = document.markdown\n    if not content:\n        return []\n\n    # Split content into chunks\n    text_chunks = self._split_text(content)\n\n    # Create Chunk objects\n    chunks = []\n    current_offset = 0\n\n    for i, text in enumerate(text_chunks):\n        # Find actual offset in original content\n        start_offset = content.find(text[:50], current_offset)\n        if start_offset == -1:\n            start_offset = current_offset\n        end_offset = start_offset + len(text)\n        current_offset = end_offset - self.chunk_overlap * 4  # Approximate\n\n        chunk = Chunk(\n            chunk_id=generate_chunk_id(document.doc_id, i),\n            doc_id=document.doc_id,\n            page_id=document.page_id,\n            version_id=document.version_id,\n            content=text,\n            content_type=\"markdown\",\n            chunk_index=i,\n            total_chunks=len(text_chunks),\n            start_offset=start_offset,\n            end_offset=end_offset,\n            char_count=len(text),\n            word_count=len(text.split()),\n            token_estimate=self.estimate_tokens(text),\n            section_path=None,\n            heading=document.title,\n            heading_level=None,\n            source_url=document.source_url,\n            title=document.title,\n            chunker_type=\"token\",\n            overlap_tokens=self.chunk_overlap if i &gt; 0 else 0,\n        )\n        chunks.append(chunk)\n\n    return chunks\n</code></pre>"},{"location":"api/chunking/#ragcrawl.chunking.TokenChunker.estimate_tokens","title":"estimate_tokens","text":"Python<pre><code>estimate_tokens(text: str) -&gt; int\n</code></pre> <p>Estimate token count.</p> <p>Uses tiktoken if available, otherwise approximates.</p> Source code in <code>src/ragcrawl/chunking/token_chunker.py</code> Python<pre><code>def estimate_tokens(self, text: str) -&gt; int:\n    \"\"\"\n    Estimate token count.\n\n    Uses tiktoken if available, otherwise approximates.\n    \"\"\"\n    if self.encoding:\n        return len(self.encoding.encode(text))\n\n    # Fallback: approximately 4 characters per token\n    return len(text) // 4\n</code></pre>"},{"location":"api/chunking/#ragcrawl.chunking.Chunker","title":"Chunker","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for content chunkers.</p> <p>Chunkers split documents into segments optimized for embedding and retrieval in RAG pipelines.</p>"},{"location":"api/chunking/#ragcrawl.chunking.Chunker.chunk","title":"chunk  <code>abstractmethod</code>","text":"Python<pre><code>chunk(document: Document) -&gt; list[Chunk]\n</code></pre> <p>Chunk a document into segments.</p> PARAMETER DESCRIPTION <code>document</code> <p>Document to chunk.</p> <p> TYPE: <code>Document</code> </p> RETURNS DESCRIPTION <code>list[Chunk]</code> <p>List of chunks.</p> Source code in <code>src/ragcrawl/chunking/chunker.py</code> Python<pre><code>@abstractmethod\ndef chunk(self, document: Document) -&gt; list[Chunk]:\n    \"\"\"\n    Chunk a document into segments.\n\n    Args:\n        document: Document to chunk.\n\n    Returns:\n        List of chunks.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/chunking/#ragcrawl.chunking.Chunker.estimate_tokens","title":"estimate_tokens  <code>abstractmethod</code>","text":"Python<pre><code>estimate_tokens(text: str) -&gt; int\n</code></pre> <p>Estimate token count for text.</p> PARAMETER DESCRIPTION <code>text</code> <p>Text to estimate.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>int</code> <p>Estimated token count.</p> Source code in <code>src/ragcrawl/chunking/chunker.py</code> Python<pre><code>@abstractmethod\ndef estimate_tokens(self, text: str) -&gt; int:\n    \"\"\"\n    Estimate token count for text.\n\n    Args:\n        text: Text to estimate.\n\n    Returns:\n        Estimated token count.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/core/","title":"Core API","text":"<p>The core module contains the main entry points for crawling and syncing.</p>"},{"location":"api/core/#overview","title":"Overview","text":"Class Description CrawlJob Execute website crawls SyncJob Incremental sync operations"},{"location":"api/core/#quick-start","title":"Quick Start","text":""},{"location":"api/core/#crawling","title":"Crawling","text":"Python<pre><code>import asyncio\nfrom ragcrawl.config import CrawlerConfig\nfrom ragcrawl.core import CrawlJob\n\nconfig = CrawlerConfig(\n    seeds=[\"https://docs.example.com\"],\n    max_pages=100,\n)\n\njob = CrawlJob(config)\nresult = asyncio.run(job.run())\n\nprint(f\"Crawled {result.stats.pages_crawled} pages\")\n</code></pre>"},{"location":"api/core/#syncing","title":"Syncing","text":"Python<pre><code>import asyncio\nfrom ragcrawl.config import SyncConfig\nfrom ragcrawl.core import SyncJob\n\nconfig = SyncConfig(\n    site_id=\"site_abc123\",\n    use_sitemap=True,\n)\n\njob = SyncJob(config)\nresult = asyncio.run(job.run())\n\nprint(f\"Updated {result.stats.pages_changed} pages\")\n</code></pre>"},{"location":"api/core/#module-reference","title":"Module Reference","text":"<p>Core crawling logic for ragcrawl.</p>"},{"location":"api/core/#ragcrawl.core.CrawlJob","title":"CrawlJob","text":"Python<pre><code>CrawlJob(config: CrawlerConfig)\n</code></pre> <p>Main crawl job orchestrator.</p> <p>Coordinates the frontier, fetcher, extractor, and storage to perform a complete crawl.</p> <p>Initialize a crawl job.</p> PARAMETER DESCRIPTION <code>config</code> <p>Crawler configuration.</p> <p> TYPE: <code>CrawlerConfig</code> </p> Source code in <code>src/ragcrawl/core/crawl_job.py</code> Python<pre><code>def __init__(self, config: CrawlerConfig) -&gt; None:\n    \"\"\"\n    Initialize a crawl job.\n\n    Args:\n        config: Crawler configuration.\n    \"\"\"\n    self.config = config\n\n    # Generate IDs\n    self.site_id = config.site_id or generate_site_id(config.seeds)\n    self.run_id = generate_run_id()\n\n    # Initialize components (lazy)\n    self._storage: StorageBackend | None = None\n    self._fetcher: Crawl4AIFetcher | None = None\n    self._robots: RobotsChecker | None = None\n    self._frontier: Frontier | None = None\n    self._scheduler: DomainScheduler | None = None\n    self._extractor: ContentExtractor | None = None\n    self._quality_gate: QualityGate | None = None\n    self._link_filter: LinkFilter | None = None\n\n    # Tracking\n    self._metrics = MetricsCollector()\n    self._logger = CrawlLoggerAdapter(self.run_id, self.site_id)\n    self._crawl_run: CrawlRun | None = None\n    self._documents: list[Document] = []\n</code></pre>"},{"location":"api/core/#ragcrawl.core.CrawlJob.run","title":"run  <code>async</code>","text":"Python<pre><code>run() -&gt; CrawlResult\n</code></pre> <p>Execute the crawl job.</p> RETURNS DESCRIPTION <code>CrawlResult</code> <p>CrawlResult with statistics and documents.</p> Source code in <code>src/ragcrawl/core/crawl_job.py</code> Python<pre><code>async def run(self) -&gt; CrawlResult:\n    \"\"\"\n    Execute the crawl job.\n\n    Returns:\n        CrawlResult with statistics and documents.\n    \"\"\"\n    start_time = datetime.now()\n\n    try:\n        # Initialize\n        self._init_components()\n\n        # Create/update site record\n        await self._save_site()\n\n        # Create crawl run record\n        self._crawl_run = CrawlRun(\n            run_id=self.run_id,\n            site_id=self.site_id,\n            config_snapshot=self.config.model_dump(exclude={\"on_page\", \"on_error\", \"on_change_detected\", \"redaction_hook\"}),\n            seeds=self.config.seeds,\n        )\n        self._crawl_run.mark_started()\n        self._storage.save_run(self._crawl_run)\n\n        self._logger.run_started(\n            self.config.seeds,\n            {\"max_pages\": self.config.max_pages, \"max_depth\": self.config.max_depth},\n        )\n\n        # Add seeds to frontier\n        await self._frontier.add_seeds(self.config.seeds)\n\n        # Main crawl loop\n        await self._crawl_loop()\n\n        # Finalize\n        metrics = self._metrics.finalize()\n        self._crawl_run.stats = CrawlStats(\n            pages_discovered=metrics.pages_discovered,\n            pages_crawled=metrics.pages_crawled,\n            pages_failed=metrics.pages_failed,\n            pages_skipped=metrics.pages_skipped,\n            pages_changed=metrics.pages_changed,\n            pages_new=metrics.pages_new,\n            total_bytes_downloaded=metrics.total_bytes,\n            total_fetch_time_ms=metrics.total_fetch_time_ms,\n            total_extraction_time_ms=metrics.total_extraction_time_ms,\n            avg_fetch_latency_ms=metrics.avg_fetch_latency_ms,\n            status_codes=dict(metrics.status_codes),\n            errors_by_type=dict(metrics.errors_by_type),\n        )\n        self._crawl_run.frontier_size = self._frontier.size\n        self._crawl_run.max_depth_reached = self._frontier.max_depth_reached\n\n        partial = metrics.pages_failed &gt; 0\n        self._crawl_run.mark_completed(partial=partial)\n        self._storage.save_run(self._crawl_run)\n\n        duration = (datetime.now() - start_time).total_seconds()\n        self._logger.run_completed(metrics.to_dict(), duration)\n\n        return CrawlResult(\n            run_id=self.run_id,\n            site_id=self.site_id,\n            success=True,\n            stats=self._crawl_run.stats,\n            documents=self._documents,\n            duration_seconds=duration,\n        )\n\n    except Exception as e:\n        logger.error(\"Crawl job failed\", error=str(e))\n\n        if self._crawl_run:\n            self._crawl_run.mark_failed(str(e))\n            self._storage.save_run(self._crawl_run)\n\n        self._logger.run_failed(str(e))\n\n        return CrawlResult(\n            run_id=self.run_id,\n            site_id=self.site_id,\n            success=False,\n            error=str(e),\n            duration_seconds=(datetime.now() - start_time).total_seconds(),\n        )\n\n    finally:\n        # Cleanup\n        if self._fetcher:\n            await self._fetcher.close()\n        if self._storage:\n            self._storage.close()\n</code></pre>"},{"location":"api/core/#ragcrawl.core.SyncJob","title":"SyncJob","text":"Python<pre><code>SyncJob(config: SyncConfig)\n</code></pre> <p>Incremental sync job for detecting content changes.</p> <p>Uses multiple strategies: 1. Sitemap lastmod (if available) 2. HTTP conditional requests (ETag/Last-Modified) 3. Content hash diffing (fallback)</p> <p>Initialize sync job.</p> PARAMETER DESCRIPTION <code>config</code> <p>Sync configuration.</p> <p> TYPE: <code>SyncConfig</code> </p> Source code in <code>src/ragcrawl/core/sync_job.py</code> Python<pre><code>def __init__(self, config: SyncConfig) -&gt; None:\n    \"\"\"\n    Initialize sync job.\n\n    Args:\n        config: Sync configuration.\n    \"\"\"\n    self.config = config\n    self.site_id = config.site_id\n    self.run_id = generate_run_id()\n\n    # Components\n    self._storage: StorageBackend | None = None\n    self._fetcher: Crawl4AIFetcher | None = None\n    self._extractor: ContentExtractor | None = None\n    self._sitemap_parser: SitemapParser | None = None\n    self._change_detector: ChangeDetector | None = None\n    self._revalidator: Revalidator | None = None\n\n    # Tracking\n    self._metrics = MetricsCollector()\n    self._logger = CrawlLoggerAdapter(self.run_id, self.site_id)\n    self._changed_pages: list[str] = []\n    self._deleted_pages: list[str] = []\n</code></pre>"},{"location":"api/core/#ragcrawl.core.SyncJob.run","title":"run  <code>async</code>","text":"Python<pre><code>run() -&gt; SyncResult\n</code></pre> <p>Execute the sync job.</p> RETURNS DESCRIPTION <code>SyncResult</code> <p>SyncResult with changed and deleted pages.</p> Source code in <code>src/ragcrawl/core/sync_job.py</code> Python<pre><code>async def run(self) -&gt; SyncResult:\n    \"\"\"\n    Execute the sync job.\n\n    Returns:\n        SyncResult with changed and deleted pages.\n    \"\"\"\n    start_time = datetime.now()\n\n    try:\n        self._init_components()\n\n        # Verify site exists\n        site = self._storage.get_site(self.site_id)\n        if not site:\n            raise ValueError(f\"Site not found: {self.site_id}\")\n\n        # Create sync run record\n        crawl_run = CrawlRun(\n            run_id=self.run_id,\n            site_id=self.site_id,\n            is_sync=True,\n            config_snapshot=self.config.model_dump(\n                exclude={\"on_page\", \"on_change_detected\", \"on_deletion_detected\", \"on_error\"}\n            ),\n        )\n        crawl_run.mark_started()\n        self._storage.save_run(crawl_run)\n\n        # Get pages to check\n        pages = await self._get_pages_to_check()\n\n        logger.info(\"Starting sync\", site_id=self.site_id, pages_to_check=len(pages))\n\n        # Process pages\n        for page in pages:\n            await self._process_page(page)\n\n            # Check limit\n            if self.config.max_pages and self._metrics.metrics.pages_crawled &gt;= self.config.max_pages:\n                break\n\n        # Finalize\n        metrics = self._metrics.finalize()\n        crawl_run.stats = CrawlStats(\n            pages_crawled=metrics.pages_crawled,\n            pages_changed=metrics.pages_changed,\n            pages_unchanged=metrics.pages_unchanged,\n            pages_deleted=metrics.pages_deleted,\n            pages_failed=metrics.pages_failed,\n        )\n        crawl_run.mark_completed(partial=metrics.pages_failed &gt; 0)\n        self._storage.save_run(crawl_run)\n\n        # Update site\n        site.last_sync_at = datetime.now()\n        self._storage.save_site(site)\n\n        duration = (datetime.now() - start_time).total_seconds()\n\n        return SyncResult(\n            run_id=self.run_id,\n            site_id=self.site_id,\n            success=True,\n            stats=crawl_run.stats,\n            changed_pages=self._changed_pages,\n            deleted_pages=self._deleted_pages,\n            duration_seconds=duration,\n        )\n\n    except Exception as e:\n        logger.error(\"Sync job failed\", error=str(e))\n        return SyncResult(\n            run_id=self.run_id,\n            site_id=self.site_id,\n            success=False,\n            error=str(e),\n            duration_seconds=(datetime.now() - start_time).total_seconds(),\n        )\n\n    finally:\n        if self._fetcher:\n            await self._fetcher.close()\n        if self._storage:\n            self._storage.close()\n</code></pre>"},{"location":"api/core/crawl-job/","title":"CrawlJob","text":"<p>The <code>CrawlJob</code> class is the main entry point for crawling websites.</p>"},{"location":"api/core/crawl-job/#overview","title":"Overview","text":"<p><code>CrawlJob</code> orchestrates the entire crawling process:</p> <ol> <li>Initializes the storage backend</li> <li>Creates or retrieves the site record</li> <li>Manages the URL frontier</li> <li>Coordinates fetching, extraction, and storage</li> <li>Tracks statistics and handles errors</li> </ol>"},{"location":"api/core/crawl-job/#usage","title":"Usage","text":""},{"location":"api/core/crawl-job/#basic-crawl","title":"Basic Crawl","text":"Python<pre><code>import asyncio\nfrom ragcrawl.config import CrawlerConfig\nfrom ragcrawl.core import CrawlJob\n\nconfig = CrawlerConfig(\n    seeds=[\"https://docs.example.com\"],\n    max_pages=100,\n    max_depth=5,\n)\n\njob = CrawlJob(config)\nresult = asyncio.run(job.run())\n\n# Access results\nprint(f\"Pages crawled: {result.stats.pages_crawled}\")\nprint(f\"Pages failed: {result.stats.pages_failed}\")\n\nfor doc in result.documents:\n    print(f\"- {doc.title}: {doc.source_url}\")\n</code></pre>"},{"location":"api/core/crawl-job/#with-callbacks","title":"With Callbacks","text":"Python<pre><code>from ragcrawl.hooks import CrawlCallbacks\n\nclass MyCallbacks(CrawlCallbacks):\n    def on_page_crawled(self, page, version):\n        print(f\"Crawled: {page.url}\")\n\n    def on_page_error(self, url, error):\n        print(f\"Error: {url} - {error}\")\n\njob = CrawlJob(config, callbacks=MyCallbacks())\nresult = asyncio.run(job.run())\n</code></pre>"},{"location":"api/core/crawl-job/#graceful-stop","title":"Graceful Stop","text":"Python<pre><code>import asyncio\nimport signal\n\njob = CrawlJob(config)\n\ndef handle_signal(sig, frame):\n    asyncio.create_task(job.stop())\n\nsignal.signal(signal.SIGINT, handle_signal)\n\nresult = asyncio.run(job.run())\n</code></pre>"},{"location":"api/core/crawl-job/#configuration","title":"Configuration","text":"<p>See CrawlerConfig for all options.</p> <p>Key options:</p> Option Type Description <code>seeds</code> list[str] Starting URLs <code>max_pages</code> int Maximum pages to crawl <code>max_depth</code> int Maximum link depth <code>delay_seconds</code> float Delay between requests <code>include_patterns</code> list[str] URL patterns to include <code>exclude_patterns</code> list[str] URL patterns to exclude"},{"location":"api/core/crawl-job/#api-reference","title":"API Reference","text":""},{"location":"api/core/crawl-job/#ragcrawl.core.crawl_job.CrawlJob","title":"CrawlJob","text":"Python<pre><code>CrawlJob(config: CrawlerConfig)\n</code></pre> <p>Main crawl job orchestrator.</p> <p>Coordinates the frontier, fetcher, extractor, and storage to perform a complete crawl.</p> <p>Initialize a crawl job.</p> PARAMETER DESCRIPTION <code>config</code> <p>Crawler configuration.</p> <p> TYPE: <code>CrawlerConfig</code> </p> Source code in <code>src/ragcrawl/core/crawl_job.py</code> Python<pre><code>def __init__(self, config: CrawlerConfig) -&gt; None:\n    \"\"\"\n    Initialize a crawl job.\n\n    Args:\n        config: Crawler configuration.\n    \"\"\"\n    self.config = config\n\n    # Generate IDs\n    self.site_id = config.site_id or generate_site_id(config.seeds)\n    self.run_id = generate_run_id()\n\n    # Initialize components (lazy)\n    self._storage: StorageBackend | None = None\n    self._fetcher: Crawl4AIFetcher | None = None\n    self._robots: RobotsChecker | None = None\n    self._frontier: Frontier | None = None\n    self._scheduler: DomainScheduler | None = None\n    self._extractor: ContentExtractor | None = None\n    self._quality_gate: QualityGate | None = None\n    self._link_filter: LinkFilter | None = None\n\n    # Tracking\n    self._metrics = MetricsCollector()\n    self._logger = CrawlLoggerAdapter(self.run_id, self.site_id)\n    self._crawl_run: CrawlRun | None = None\n    self._documents: list[Document] = []\n</code></pre>"},{"location":"api/core/crawl-job/#ragcrawl.core.crawl_job.CrawlJob.run","title":"run  <code>async</code>","text":"Python<pre><code>run() -&gt; CrawlResult\n</code></pre> <p>Execute the crawl job.</p> RETURNS DESCRIPTION <code>CrawlResult</code> <p>CrawlResult with statistics and documents.</p> Source code in <code>src/ragcrawl/core/crawl_job.py</code> Python<pre><code>async def run(self) -&gt; CrawlResult:\n    \"\"\"\n    Execute the crawl job.\n\n    Returns:\n        CrawlResult with statistics and documents.\n    \"\"\"\n    start_time = datetime.now()\n\n    try:\n        # Initialize\n        self._init_components()\n\n        # Create/update site record\n        await self._save_site()\n\n        # Create crawl run record\n        self._crawl_run = CrawlRun(\n            run_id=self.run_id,\n            site_id=self.site_id,\n            config_snapshot=self.config.model_dump(exclude={\"on_page\", \"on_error\", \"on_change_detected\", \"redaction_hook\"}),\n            seeds=self.config.seeds,\n        )\n        self._crawl_run.mark_started()\n        self._storage.save_run(self._crawl_run)\n\n        self._logger.run_started(\n            self.config.seeds,\n            {\"max_pages\": self.config.max_pages, \"max_depth\": self.config.max_depth},\n        )\n\n        # Add seeds to frontier\n        await self._frontier.add_seeds(self.config.seeds)\n\n        # Main crawl loop\n        await self._crawl_loop()\n\n        # Finalize\n        metrics = self._metrics.finalize()\n        self._crawl_run.stats = CrawlStats(\n            pages_discovered=metrics.pages_discovered,\n            pages_crawled=metrics.pages_crawled,\n            pages_failed=metrics.pages_failed,\n            pages_skipped=metrics.pages_skipped,\n            pages_changed=metrics.pages_changed,\n            pages_new=metrics.pages_new,\n            total_bytes_downloaded=metrics.total_bytes,\n            total_fetch_time_ms=metrics.total_fetch_time_ms,\n            total_extraction_time_ms=metrics.total_extraction_time_ms,\n            avg_fetch_latency_ms=metrics.avg_fetch_latency_ms,\n            status_codes=dict(metrics.status_codes),\n            errors_by_type=dict(metrics.errors_by_type),\n        )\n        self._crawl_run.frontier_size = self._frontier.size\n        self._crawl_run.max_depth_reached = self._frontier.max_depth_reached\n\n        partial = metrics.pages_failed &gt; 0\n        self._crawl_run.mark_completed(partial=partial)\n        self._storage.save_run(self._crawl_run)\n\n        duration = (datetime.now() - start_time).total_seconds()\n        self._logger.run_completed(metrics.to_dict(), duration)\n\n        return CrawlResult(\n            run_id=self.run_id,\n            site_id=self.site_id,\n            success=True,\n            stats=self._crawl_run.stats,\n            documents=self._documents,\n            duration_seconds=duration,\n        )\n\n    except Exception as e:\n        logger.error(\"Crawl job failed\", error=str(e))\n\n        if self._crawl_run:\n            self._crawl_run.mark_failed(str(e))\n            self._storage.save_run(self._crawl_run)\n\n        self._logger.run_failed(str(e))\n\n        return CrawlResult(\n            run_id=self.run_id,\n            site_id=self.site_id,\n            success=False,\n            error=str(e),\n            duration_seconds=(datetime.now() - start_time).total_seconds(),\n        )\n\n    finally:\n        # Cleanup\n        if self._fetcher:\n            await self._fetcher.close()\n        if self._storage:\n            self._storage.close()\n</code></pre>"},{"location":"api/core/sync-job/","title":"SyncJob","text":"<p>The <code>SyncJob</code> class handles incremental updates to previously crawled sites.</p>"},{"location":"api/core/sync-job/#overview","title":"Overview","text":"<p><code>SyncJob</code> efficiently updates your knowledge base by:</p> <ol> <li>Checking sitemap for new/updated URLs</li> <li>Using conditional requests (ETags, Last-Modified)</li> <li>Comparing content hashes for changes</li> <li>Marking deleted pages as tombstones</li> </ol>"},{"location":"api/core/sync-job/#usage","title":"Usage","text":""},{"location":"api/core/sync-job/#basic-sync","title":"Basic Sync","text":"Python<pre><code>import asyncio\nfrom ragcrawl.config import SyncConfig\nfrom ragcrawl.core import SyncJob\n\nconfig = SyncConfig(\n    site_id=\"site_abc123\",\n    use_sitemap=True,\n    use_conditional_requests=True,\n)\n\njob = SyncJob(config)\nresult = asyncio.run(job.run())\n\n# Check what changed\nprint(f\"New pages: {result.stats.pages_new}\")\nprint(f\"Updated pages: {result.stats.pages_changed}\")\nprint(f\"Deleted pages: {result.stats.pages_deleted}\")\nprint(f\"Unchanged: {result.stats.pages_unchanged}\")\n</code></pre>"},{"location":"api/core/sync-job/#sync-with-age-filter","title":"Sync with Age Filter","text":"<p>Only sync pages that haven't been checked recently:</p> Python<pre><code>config = SyncConfig(\n    site_id=\"site_abc123\",\n    max_age_hours=24,  # Only pages not synced in 24 hours\n)\n\njob = SyncJob(config)\nresult = asyncio.run(job.run())\n</code></pre>"},{"location":"api/core/sync-job/#sync-with-page-limit","title":"Sync with Page Limit","text":"Python<pre><code>config = SyncConfig(\n    site_id=\"site_abc123\",\n    max_pages=100,  # Stop after 100 pages\n)\n\njob = SyncJob(config)\nresult = asyncio.run(job.run())\n</code></pre>"},{"location":"api/core/sync-job/#sync-strategies","title":"Sync Strategies","text":"<p>SyncJob uses multiple strategies in order of efficiency:</p>"},{"location":"api/core/sync-job/#1-sitemap","title":"1. Sitemap","text":"<p>If available, the sitemap provides: - List of all current URLs - Last modification dates - Change frequency hints</p> Python<pre><code>config = SyncConfig(\n    site_id=\"site_abc123\",\n    use_sitemap=True,  # Default: True\n)\n</code></pre>"},{"location":"api/core/sync-job/#2-conditional-requests","title":"2. Conditional Requests","text":"<p>Uses HTTP headers to avoid downloading unchanged content:</p> Python<pre><code>config = SyncConfig(\n    site_id=\"site_abc123\",\n    use_conditional_requests=True,  # Default: True\n)\n</code></pre> <p>Supports: - <code>If-None-Match</code> with ETags - <code>If-Modified-Since</code> with Last-Modified dates</p>"},{"location":"api/core/sync-job/#3-content-hash-comparison","title":"3. Content Hash Comparison","text":"<p>As a fallback, compares content hashes:</p> Python<pre><code># This happens automatically when conditional requests\n# don't indicate a change but content differs\n</code></pre>"},{"location":"api/core/sync-job/#configuration","title":"Configuration","text":"<p>See SyncConfig for all options.</p> Option Type Default Description <code>site_id</code> str required Site to sync <code>max_pages</code> int None Maximum pages to sync <code>max_age_hours</code> float None Only sync pages older than N hours <code>use_sitemap</code> bool True Use sitemap for discovery <code>use_conditional_requests</code> bool True Use ETags/Last-Modified"},{"location":"api/core/sync-job/#api-reference","title":"API Reference","text":""},{"location":"api/core/sync-job/#ragcrawl.core.sync_job.SyncJob","title":"SyncJob","text":"Python<pre><code>SyncJob(config: SyncConfig)\n</code></pre> <p>Incremental sync job for detecting content changes.</p> <p>Uses multiple strategies: 1. Sitemap lastmod (if available) 2. HTTP conditional requests (ETag/Last-Modified) 3. Content hash diffing (fallback)</p> <p>Initialize sync job.</p> PARAMETER DESCRIPTION <code>config</code> <p>Sync configuration.</p> <p> TYPE: <code>SyncConfig</code> </p> Source code in <code>src/ragcrawl/core/sync_job.py</code> Python<pre><code>def __init__(self, config: SyncConfig) -&gt; None:\n    \"\"\"\n    Initialize sync job.\n\n    Args:\n        config: Sync configuration.\n    \"\"\"\n    self.config = config\n    self.site_id = config.site_id\n    self.run_id = generate_run_id()\n\n    # Components\n    self._storage: StorageBackend | None = None\n    self._fetcher: Crawl4AIFetcher | None = None\n    self._extractor: ContentExtractor | None = None\n    self._sitemap_parser: SitemapParser | None = None\n    self._change_detector: ChangeDetector | None = None\n    self._revalidator: Revalidator | None = None\n\n    # Tracking\n    self._metrics = MetricsCollector()\n    self._logger = CrawlLoggerAdapter(self.run_id, self.site_id)\n    self._changed_pages: list[str] = []\n    self._deleted_pages: list[str] = []\n</code></pre>"},{"location":"api/core/sync-job/#ragcrawl.core.sync_job.SyncJob.run","title":"run  <code>async</code>","text":"Python<pre><code>run() -&gt; SyncResult\n</code></pre> <p>Execute the sync job.</p> RETURNS DESCRIPTION <code>SyncResult</code> <p>SyncResult with changed and deleted pages.</p> Source code in <code>src/ragcrawl/core/sync_job.py</code> Python<pre><code>async def run(self) -&gt; SyncResult:\n    \"\"\"\n    Execute the sync job.\n\n    Returns:\n        SyncResult with changed and deleted pages.\n    \"\"\"\n    start_time = datetime.now()\n\n    try:\n        self._init_components()\n\n        # Verify site exists\n        site = self._storage.get_site(self.site_id)\n        if not site:\n            raise ValueError(f\"Site not found: {self.site_id}\")\n\n        # Create sync run record\n        crawl_run = CrawlRun(\n            run_id=self.run_id,\n            site_id=self.site_id,\n            is_sync=True,\n            config_snapshot=self.config.model_dump(\n                exclude={\"on_page\", \"on_change_detected\", \"on_deletion_detected\", \"on_error\"}\n            ),\n        )\n        crawl_run.mark_started()\n        self._storage.save_run(crawl_run)\n\n        # Get pages to check\n        pages = await self._get_pages_to_check()\n\n        logger.info(\"Starting sync\", site_id=self.site_id, pages_to_check=len(pages))\n\n        # Process pages\n        for page in pages:\n            await self._process_page(page)\n\n            # Check limit\n            if self.config.max_pages and self._metrics.metrics.pages_crawled &gt;= self.config.max_pages:\n                break\n\n        # Finalize\n        metrics = self._metrics.finalize()\n        crawl_run.stats = CrawlStats(\n            pages_crawled=metrics.pages_crawled,\n            pages_changed=metrics.pages_changed,\n            pages_unchanged=metrics.pages_unchanged,\n            pages_deleted=metrics.pages_deleted,\n            pages_failed=metrics.pages_failed,\n        )\n        crawl_run.mark_completed(partial=metrics.pages_failed &gt; 0)\n        self._storage.save_run(crawl_run)\n\n        # Update site\n        site.last_sync_at = datetime.now()\n        self._storage.save_site(site)\n\n        duration = (datetime.now() - start_time).total_seconds()\n\n        return SyncResult(\n            run_id=self.run_id,\n            site_id=self.site_id,\n            success=True,\n            stats=crawl_run.stats,\n            changed_pages=self._changed_pages,\n            deleted_pages=self._deleted_pages,\n            duration_seconds=duration,\n        )\n\n    except Exception as e:\n        logger.error(\"Sync job failed\", error=str(e))\n        return SyncResult(\n            run_id=self.run_id,\n            site_id=self.site_id,\n            success=False,\n            error=str(e),\n            duration_seconds=(datetime.now() - start_time).total_seconds(),\n        )\n\n    finally:\n        if self._fetcher:\n            await self._fetcher.close()\n        if self._storage:\n            self._storage.close()\n</code></pre>"},{"location":"api/export/","title":"Export API","text":"<p>ragcrawl provides exporters and publishers for outputting crawled content.</p>"},{"location":"api/export/#overview","title":"Overview","text":""},{"location":"api/export/#exporters","title":"Exporters","text":"<p>Export data to structured formats:</p> Exporter Format Description <code>JSONExporter</code> .json Single JSON array <code>JSONLExporter</code> .jsonl JSON Lines format"},{"location":"api/export/#publishers","title":"Publishers","text":"<p>Output markdown files:</p> Publisher Output Description <code>SinglePagePublisher</code> One file Combined markdown <code>MultiPagePublisher</code> Directory Preserves structure"},{"location":"api/export/#jsonexporter","title":"JSONExporter","text":"<p>Export documents to a single JSON file.</p> Python<pre><code>from ragcrawl.export import JSONExporter\nfrom pathlib import Path\n\nexporter = JSONExporter(indent=2)\n\n# Export documents\nexporter.export_documents(documents, Path(\"output.json\"))\n\n# Export chunks\nexporter.export_chunks(chunks, Path(\"chunks.json\"))\n</code></pre>"},{"location":"api/export/#output-format","title":"Output Format","text":"JSON<pre><code>[\n  {\n    \"doc_id\": \"abc123\",\n    \"source_url\": \"https://example.com/page\",\n    \"title\": \"Page Title\",\n    \"markdown\": \"# Page Title\\n\\nContent...\",\n    \"status_code\": 200,\n    \"word_count\": 500\n  }\n]\n</code></pre>"},{"location":"api/export/#jsonlexporter","title":"JSONLExporter","text":"<p>Export documents to JSON Lines format (one JSON object per line).</p> Python<pre><code>from ragcrawl.export import JSONLExporter\nfrom pathlib import Path\n\nexporter = JSONLExporter()\n\n# Export documents\nexporter.export_documents(documents, Path(\"output.jsonl\"))\n</code></pre>"},{"location":"api/export/#output-format_1","title":"Output Format","text":"Text Only<pre><code>{\"doc_id\":\"abc123\",\"source_url\":\"https://example.com/page1\",...}\n{\"doc_id\":\"def456\",\"source_url\":\"https://example.com/page2\",...}\n</code></pre>"},{"location":"api/export/#singlepagepublisher","title":"SinglePagePublisher","text":"<p>Combine all documents into a single markdown file.</p> Python<pre><code>from ragcrawl.output import SinglePagePublisher\nfrom ragcrawl.config import OutputConfig, OutputMode\n\nconfig = OutputConfig(\n    mode=OutputMode.SINGLE,\n    root_dir=\"./output\",\n    single_file_name=\"knowledge_base.md\",\n    include_toc=True,\n    include_metadata=True,\n)\n\npublisher = SinglePagePublisher(config)\nfiles = publisher.publish(documents)\n\nprint(f\"Created: {files[0]}\")\n</code></pre>"},{"location":"api/export/#configuration","title":"Configuration","text":"Option Type Default Description <code>single_file_name</code> str \"output.md\" Output filename <code>include_toc</code> bool True Add table of contents <code>include_metadata</code> bool True Add source URLs"},{"location":"api/export/#multipagepublisher","title":"MultiPagePublisher","text":"<p>Output documents preserving URL structure.</p> Python<pre><code>from ragcrawl.output import MultiPagePublisher\nfrom ragcrawl.config import OutputConfig, OutputMode\n\nconfig = OutputConfig(\n    mode=OutputMode.MULTI,\n    root_dir=\"./output\",\n    rewrite_links=True,\n    generate_index=True,\n    include_metadata=True,\n)\n\npublisher = MultiPagePublisher(config)\nfiles = publisher.publish(documents)\n\nprint(f\"Created {len(files)} files\")\n</code></pre>"},{"location":"api/export/#output-structure","title":"Output Structure","text":"Text Only<pre><code>output/\n\u251c\u2500\u2500 index.md\n\u251c\u2500\u2500 example.com/\n\u2502   \u251c\u2500\u2500 docs/\n\u2502   \u2502   \u251c\u2500\u2500 getting-started.md\n\u2502   \u2502   \u2514\u2500\u2500 api-reference.md\n\u2502   \u2514\u2500\u2500 blog/\n\u2502       \u2514\u2500\u2500 post-1.md\n</code></pre>"},{"location":"api/export/#configuration_1","title":"Configuration","text":"Option Type Default Description <code>root_dir</code> str \"./output\" Output directory <code>rewrite_links</code> bool True Rewrite internal links <code>generate_index</code> bool True Create index file <code>include_metadata</code> bool True Add frontmatter"},{"location":"api/export/#output-configuration","title":"Output Configuration","text":"Python<pre><code>from ragcrawl.config import OutputConfig, OutputMode\n\nconfig = OutputConfig(\n    mode=OutputMode.MULTI,  # or SINGLE\n    root_dir=\"./output\",\n\n    # Single-page options\n    single_file_name=\"combined.md\",\n    include_toc=True,\n\n    # Multi-page options\n    rewrite_links=True,\n    generate_index=True,\n\n    # Common options\n    include_metadata=True,\n)\n</code></pre>"},{"location":"api/export/#module-reference","title":"Module Reference","text":"<p>Export functionality for ragcrawl.</p> <p>Output publishing for ragcrawl.</p>"},{"location":"api/export/#ragcrawl.export.JSONExporter","title":"JSONExporter","text":"Python<pre><code>JSONExporter(\n    indent: int | None = 2,\n    include_html: bool = False,\n    include_diagnostics: bool = True,\n)\n</code></pre> <p>               Bases: <code>Exporter</code></p> <p>Exports documents and chunks as JSON.</p> <p>Initialize JSON exporter.</p> PARAMETER DESCRIPTION <code>indent</code> <p>JSON indentation (None for compact).</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>2</code> </p> <code>include_html</code> <p>Include HTML content in export.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>include_diagnostics</code> <p>Include diagnostic info.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Source code in <code>src/ragcrawl/export/json_exporter.py</code> Python<pre><code>def __init__(\n    self,\n    indent: int | None = 2,\n    include_html: bool = False,\n    include_diagnostics: bool = True,\n) -&gt; None:\n    \"\"\"\n    Initialize JSON exporter.\n\n    Args:\n        indent: JSON indentation (None for compact).\n        include_html: Include HTML content in export.\n        include_diagnostics: Include diagnostic info.\n    \"\"\"\n    self.indent = indent\n    self.include_html = include_html\n    self.include_diagnostics = include_diagnostics\n</code></pre>"},{"location":"api/export/#ragcrawl.export.JSONExporter.export_document","title":"export_document","text":"Python<pre><code>export_document(\n    document: Document, path: Path | None = None\n) -&gt; str | None\n</code></pre> <p>Export a document as JSON.</p> Source code in <code>src/ragcrawl/export/json_exporter.py</code> Python<pre><code>def export_document(\n    self, document: Document, path: Path | None = None\n) -&gt; str | None:\n    \"\"\"Export a document as JSON.\"\"\"\n    data = self._document_to_dict(document)\n    json_str = json.dumps(data, indent=self.indent, default=self._json_serializer)\n\n    if path:\n        path.parent.mkdir(parents=True, exist_ok=True)\n        path.write_text(json_str)\n        return None\n\n    return json_str\n</code></pre>"},{"location":"api/export/#ragcrawl.export.JSONExporter.export_documents","title":"export_documents","text":"Python<pre><code>export_documents(\n    documents: list[Document], path: Path\n) -&gt; None\n</code></pre> <p>Export documents as JSON array.</p> Source code in <code>src/ragcrawl/export/json_exporter.py</code> Python<pre><code>def export_documents(self, documents: list[Document], path: Path) -&gt; None:\n    \"\"\"Export documents as JSON array.\"\"\"\n    data = [self._document_to_dict(doc) for doc in documents]\n    json_str = json.dumps(data, indent=self.indent, default=self._json_serializer)\n\n    path.parent.mkdir(parents=True, exist_ok=True)\n    path.write_text(json_str)\n</code></pre>"},{"location":"api/export/#ragcrawl.export.JSONExporter.export_chunk","title":"export_chunk","text":"Python<pre><code>export_chunk(\n    chunk: Chunk, path: Path | None = None\n) -&gt; str | None\n</code></pre> <p>Export a chunk as JSON.</p> Source code in <code>src/ragcrawl/export/json_exporter.py</code> Python<pre><code>def export_chunk(self, chunk: Chunk, path: Path | None = None) -&gt; str | None:\n    \"\"\"Export a chunk as JSON.\"\"\"\n    data = self._chunk_to_dict(chunk)\n    json_str = json.dumps(data, indent=self.indent, default=self._json_serializer)\n\n    if path:\n        path.parent.mkdir(parents=True, exist_ok=True)\n        path.write_text(json_str)\n        return None\n\n    return json_str\n</code></pre>"},{"location":"api/export/#ragcrawl.export.JSONExporter.export_chunks","title":"export_chunks","text":"Python<pre><code>export_chunks(chunks: list[Chunk], path: Path) -&gt; None\n</code></pre> <p>Export chunks as JSON array.</p> Source code in <code>src/ragcrawl/export/json_exporter.py</code> Python<pre><code>def export_chunks(self, chunks: list[Chunk], path: Path) -&gt; None:\n    \"\"\"Export chunks as JSON array.\"\"\"\n    data = [self._chunk_to_dict(chunk) for chunk in chunks]\n    json_str = json.dumps(data, indent=self.indent, default=self._json_serializer)\n\n    path.parent.mkdir(parents=True, exist_ok=True)\n    path.write_text(json_str)\n</code></pre>"},{"location":"api/export/#ragcrawl.export.JSONLExporter","title":"JSONLExporter","text":"Python<pre><code>JSONLExporter(\n    include_html: bool = False,\n    include_diagnostics: bool = True,\n)\n</code></pre> <p>               Bases: <code>Exporter</code></p> <p>Exports documents and chunks as JSONL (one JSON object per line).</p> <p>JSONL is better for streaming and large datasets.</p> <p>Initialize JSONL exporter.</p> PARAMETER DESCRIPTION <code>include_html</code> <p>Include HTML content.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>include_diagnostics</code> <p>Include diagnostics.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Source code in <code>src/ragcrawl/export/json_exporter.py</code> Python<pre><code>def __init__(\n    self,\n    include_html: bool = False,\n    include_diagnostics: bool = True,\n) -&gt; None:\n    \"\"\"\n    Initialize JSONL exporter.\n\n    Args:\n        include_html: Include HTML content.\n        include_diagnostics: Include diagnostics.\n    \"\"\"\n    self.include_html = include_html\n    self.include_diagnostics = include_diagnostics\n    self._json_exporter = JSONExporter(\n        indent=None,\n        include_html=include_html,\n        include_diagnostics=include_diagnostics,\n    )\n</code></pre>"},{"location":"api/export/#ragcrawl.export.JSONLExporter.export_document","title":"export_document","text":"Python<pre><code>export_document(\n    document: Document, path: Path | None = None\n) -&gt; str | None\n</code></pre> <p>Export a document as JSONL line.</p> Source code in <code>src/ragcrawl/export/json_exporter.py</code> Python<pre><code>def export_document(\n    self, document: Document, path: Path | None = None\n) -&gt; str | None:\n    \"\"\"Export a document as JSONL line.\"\"\"\n    return self._json_exporter.export_document(document, path)\n</code></pre>"},{"location":"api/export/#ragcrawl.export.JSONLExporter.export_documents","title":"export_documents","text":"Python<pre><code>export_documents(\n    documents: list[Document], path: Path\n) -&gt; None\n</code></pre> <p>Export documents as JSONL file.</p> Source code in <code>src/ragcrawl/export/json_exporter.py</code> Python<pre><code>def export_documents(self, documents: list[Document], path: Path) -&gt; None:\n    \"\"\"Export documents as JSONL file.\"\"\"\n    path.parent.mkdir(parents=True, exist_ok=True)\n\n    with path.open(\"w\") as f:\n        for doc in documents:\n            line = self._json_exporter.export_document(doc)\n            f.write(line + \"\\n\")\n</code></pre>"},{"location":"api/export/#ragcrawl.export.JSONLExporter.export_chunk","title":"export_chunk","text":"Python<pre><code>export_chunk(\n    chunk: Chunk, path: Path | None = None\n) -&gt; str | None\n</code></pre> <p>Export a chunk as JSONL line.</p> Source code in <code>src/ragcrawl/export/json_exporter.py</code> Python<pre><code>def export_chunk(self, chunk: Chunk, path: Path | None = None) -&gt; str | None:\n    \"\"\"Export a chunk as JSONL line.\"\"\"\n    return self._json_exporter.export_chunk(chunk, path)\n</code></pre>"},{"location":"api/export/#ragcrawl.export.JSONLExporter.export_chunks","title":"export_chunks","text":"Python<pre><code>export_chunks(chunks: list[Chunk], path: Path) -&gt; None\n</code></pre> <p>Export chunks as JSONL file.</p> Source code in <code>src/ragcrawl/export/json_exporter.py</code> Python<pre><code>def export_chunks(self, chunks: list[Chunk], path: Path) -&gt; None:\n    \"\"\"Export chunks as JSONL file.\"\"\"\n    path.parent.mkdir(parents=True, exist_ok=True)\n\n    with path.open(\"w\") as f:\n        for chunk in chunks:\n            line = self._json_exporter.export_chunk(chunk)\n            f.write(line + \"\\n\")\n</code></pre>"},{"location":"api/export/#ragcrawl.output.SinglePagePublisher","title":"SinglePagePublisher","text":"Python<pre><code>SinglePagePublisher(config: OutputConfig)\n</code></pre> <p>               Bases: <code>MarkdownPublisher</code></p> <p>Publishes all documents to a single markdown file.</p> <p>Features: - Auto-generated table of contents - Per-page anchors for navigation - Configurable page separators</p> Source code in <code>src/ragcrawl/output/publisher.py</code> Python<pre><code>def __init__(self, config: OutputConfig) -&gt; None:\n    \"\"\"\n    Initialize publisher.\n\n    Args:\n        config: Output configuration.\n    \"\"\"\n    self.config = config\n    self.output_path = Path(config.root_dir)\n</code></pre>"},{"location":"api/export/#ragcrawl.output.SinglePagePublisher.publish","title":"publish","text":"Python<pre><code>publish(documents: list[Document]) -&gt; list[Path]\n</code></pre> <p>Publish all documents to a single file.</p> PARAMETER DESCRIPTION <code>documents</code> <p>Documents to publish.</p> <p> TYPE: <code>list[Document]</code> </p> RETURNS DESCRIPTION <code>list[Path]</code> <p>List containing the single output file path.</p> Source code in <code>src/ragcrawl/output/single_page.py</code> Python<pre><code>def publish(self, documents: list[Document]) -&gt; list[Path]:\n    \"\"\"\n    Publish all documents to a single file.\n\n    Args:\n        documents: Documents to publish.\n\n    Returns:\n        List containing the single output file path.\n    \"\"\"\n    if not documents:\n        return []\n\n    self.ensure_output_dir()\n\n    # Sort documents by depth, then URL\n    sorted_docs = sorted(documents, key=lambda d: (d.depth, d.normalized_url))\n\n    # Build content\n    content_parts = []\n\n    # Generate TOC if enabled\n    if self.config.generate_toc:\n        toc = self._generate_toc(sorted_docs)\n        content_parts.append(toc)\n        content_parts.append(self.config.page_separator)\n\n    # Add each document\n    for doc in sorted_docs:\n        page_content = self._format_document(doc)\n        content_parts.append(page_content)\n        content_parts.append(self.config.page_separator)\n\n    # Write file\n    output_file = self.output_path / self.config.single_file_name\n    output_file.write_text(\"\".join(content_parts))\n\n    return [output_file]\n</code></pre>"},{"location":"api/export/#ragcrawl.output.SinglePagePublisher.publish_single","title":"publish_single","text":"Python<pre><code>publish_single(document: Document) -&gt; Path | None\n</code></pre> <p>Single page mode doesn't support individual publishing.</p> Source code in <code>src/ragcrawl/output/single_page.py</code> Python<pre><code>def publish_single(self, document: Document) -&gt; Path | None:\n    \"\"\"Single page mode doesn't support individual publishing.\"\"\"\n    return None\n</code></pre>"},{"location":"api/export/#ragcrawl.output.MultiPagePublisher","title":"MultiPagePublisher","text":"Python<pre><code>MultiPagePublisher(config: OutputConfig)\n</code></pre> <p>               Bases: <code>MarkdownPublisher</code></p> <p>Publishes documents as individual markdown files.</p> <p>Features: - Preserves site folder structure - Rewrites internal links to local markdown files - Generates navigation aids (index, breadcrumbs, prev/next) - Handles deleted pages via tombstones or redirects</p> <p>Initialize multi-page publisher.</p> Source code in <code>src/ragcrawl/output/multi_page.py</code> Python<pre><code>def __init__(self, config: OutputConfig) -&gt; None:\n    \"\"\"Initialize multi-page publisher.\"\"\"\n    super().__init__(config)\n    self.link_rewriter = LinkRewriter(config)\n    self.nav_generator = NavigationGenerator(config)\n</code></pre>"},{"location":"api/export/#ragcrawl.output.MultiPagePublisher.publish","title":"publish","text":"Python<pre><code>publish(documents: list[Document]) -&gt; list[Path]\n</code></pre> <p>Publish documents as individual files.</p> PARAMETER DESCRIPTION <code>documents</code> <p>Documents to publish.</p> <p> TYPE: <code>list[Document]</code> </p> RETURNS DESCRIPTION <code>list[Path]</code> <p>List of created file paths.</p> Source code in <code>src/ragcrawl/output/multi_page.py</code> Python<pre><code>def publish(self, documents: list[Document]) -&gt; list[Path]:\n    \"\"\"\n    Publish documents as individual files.\n\n    Args:\n        documents: Documents to publish.\n\n    Returns:\n        List of created file paths.\n    \"\"\"\n    if not documents:\n        return []\n\n    self.ensure_output_dir()\n\n    # Build URL to path mapping for link rewriting\n    url_to_path = {}\n    for doc in documents:\n        output_path = self._url_to_path(doc.normalized_url)\n        url_to_path[doc.normalized_url] = output_path\n\n    self.link_rewriter.set_url_mapping(url_to_path)\n\n    # Sort by depth for proper ordering\n    sorted_docs = sorted(documents, key=lambda d: (d.depth, d.normalized_url))\n\n    created_files = []\n\n    # Publish each document\n    for i, doc in enumerate(sorted_docs):\n        # Get prev/next for navigation\n        prev_doc = sorted_docs[i - 1] if i &gt; 0 else None\n        next_doc = sorted_docs[i + 1] if i &lt; len(sorted_docs) - 1 else None\n\n        file_path = self._publish_document(doc, prev_doc, next_doc)\n        if file_path:\n            created_files.append(file_path)\n\n    # Generate index if enabled\n    if self.config.generate_index:\n        index_path = self._generate_index(sorted_docs)\n        created_files.append(index_path)\n\n    return created_files\n</code></pre>"},{"location":"api/export/#ragcrawl.output.MultiPagePublisher.publish_single","title":"publish_single","text":"Python<pre><code>publish_single(document: Document) -&gt; Path | None\n</code></pre> <p>Publish a single document.</p> PARAMETER DESCRIPTION <code>document</code> <p>Document to publish.</p> <p> TYPE: <code>Document</code> </p> RETURNS DESCRIPTION <code>Path | None</code> <p>Created file path.</p> Source code in <code>src/ragcrawl/output/multi_page.py</code> Python<pre><code>def publish_single(self, document: Document) -&gt; Path | None:\n    \"\"\"\n    Publish a single document.\n\n    Args:\n        document: Document to publish.\n\n    Returns:\n        Created file path.\n    \"\"\"\n    self.ensure_output_dir()\n    return self._publish_document(document, None, None)\n</code></pre>"},{"location":"api/filters/","title":"Filters API","text":"<p>ragcrawl provides URL filtering utilities to control which pages are crawled.</p>"},{"location":"api/filters/#overview","title":"Overview","text":"Filter Description <code>LinkFilter</code> Complete URL filtering with domains, patterns, and deduplication <code>PatternMatcher</code> Glob/regex pattern matching <code>URLNormalizer</code> URL normalization and hashing <code>ExtensionFilter</code> File extension filtering"},{"location":"api/filters/#linkfilter","title":"LinkFilter","text":"<p>The main filter class combining all filtering logic.</p> Python<pre><code>from ragcrawl.filters import LinkFilter, FilterReason\n\nlink_filter = LinkFilter(\n    allowed_domains=[\"docs.example.com\"],\n    allow_subdomains=True,\n    include_patterns=[\"/docs/*\", \"/api/*\"],\n    exclude_patterns=[\"/admin/*\", \"*secret*\"],\n    blocked_extensions=[\".pdf\", \".zip\", \".png\"],\n)\n\n# Check if URL should be crawled\nresult = link_filter.filter(\"https://docs.example.com/api/users\")\n\nif result.allowed:\n    crawl_url(url)\nelse:\n    print(f\"Filtered: {result.reason}\")\n    # FilterReason.DOMAIN_NOT_ALLOWED\n    # FilterReason.EXCLUDED_PATTERN\n    # FilterReason.NO_INCLUDE_MATCH\n    # FilterReason.BLOCKED_EXTENSION\n    # FilterReason.ALREADY_SEEN\n</code></pre>"},{"location":"api/filters/#deduplication","title":"Deduplication","text":"Python<pre><code># Track seen URLs\nlink_filter.mark_seen(\"https://example.com/page1\")\n\n# Check with deduplication\nresult = link_filter.filter(\"https://example.com/page1\", check_seen=True)\nif not result.allowed:\n    print(f\"Already seen: {result.reason == FilterReason.ALREADY_SEEN}\")\n</code></pre>"},{"location":"api/filters/#configuration","title":"Configuration","text":"Option Type Description <code>allowed_domains</code> list[str] Domains to allow <code>allow_subdomains</code> bool Include subdomains <code>include_patterns</code> list[str] URL patterns to include <code>exclude_patterns</code> list[str] URL patterns to exclude <code>blocked_extensions</code> list[str] File extensions to skip"},{"location":"api/filters/#patternmatcher","title":"PatternMatcher","text":"<p>Match URLs against glob or regex patterns.</p> Python<pre><code>from ragcrawl.filters import PatternMatcher\n\nmatcher = PatternMatcher(\n    include_patterns=[\"/docs/*\", \"/api/v1/*\"],\n    exclude_patterns=[\"*internal*\", \"*private*\"],\n    case_sensitive=False,\n)\n\n# Check if URL path should be included\nif matcher.should_include(\"/docs/getting-started\"):\n    print(\"URL matches include pattern\")\n\nif not matcher.should_include(\"/admin/settings\"):\n    print(\"URL doesn't match any include pattern\")\n</code></pre>"},{"location":"api/filters/#pattern-syntax","title":"Pattern Syntax","text":"Pattern Matches <code>/docs/*</code> <code>/docs/anything</code> <code>**/api/*</code> <code>any/path/api/anything</code> <code>*.pdf</code> Files ending in .pdf <code>/api/v[12]/*</code> <code>/api/v1/</code> or <code>/api/v2/</code> <p>Patterns support both glob syntax (<code>*</code>, <code>**</code>, <code>?</code>) and regex (when containing <code>|</code>, <code>^</code>, <code>$</code>, etc.).</p>"},{"location":"api/filters/#urlnormalizer","title":"URLNormalizer","text":"<p>Normalize URLs for consistent comparison and hashing.</p> Python<pre><code>from ragcrawl.filters import URLNormalizer\n\nnormalizer = URLNormalizer(\n    remove_fragments=True,\n    remove_tracking_params=True,\n    sort_query_params=True,\n)\n\n# Normalize URL\nnormalized = normalizer.normalize(\n    \"HTTPS://Example.COM/Page?utm_source=google&amp;id=1#section\"\n)\n# Result: \"https://example.com/page?id=1\"\n\n# Get domain\ndomain = normalizer.get_domain(\"https://docs.example.com/page\")\n# Result: \"docs.example.com\"\n\n# Get registered domain\nbase = normalizer.get_registered_domain(\"https://docs.example.com/page\")\n# Result: \"example.com\"\n\n# Check same domain\nsame = normalizer.is_same_domain(\n    \"https://docs.example.com\",\n    \"https://api.example.com\",\n    include_subdomains=True,\n)\n# Result: True\n</code></pre>"},{"location":"api/filters/#configuration_1","title":"Configuration","text":"Option Type Default Description <code>remove_fragments</code> bool True Remove URL fragments (#) <code>remove_tracking_params</code> bool True Remove UTM params etc. <code>sort_query_params</code> bool True Sort query parameters <code>lowercase_path</code> bool False Lowercase URL path"},{"location":"api/filters/#extensionfilter","title":"ExtensionFilter","text":"<p>Filter URLs by file extension.</p> Python<pre><code>from ragcrawl.filters import ExtensionFilter\n\next_filter = ExtensionFilter(\n    blocked_extensions=[\".pdf\", \".png\", \".jpg\", \".zip\", \".exe\"]\n)\n\n# Check if URL is blocked\nif ext_filter.is_blocked(\"https://example.com/report.pdf\"):\n    print(\"PDF files are blocked\")\n\n# Get extension\next = ext_filter.get_extension(\"https://example.com/file.tar.gz\")\n# Result: \".gz\"\n</code></pre>"},{"location":"api/filters/#integration-example","title":"Integration Example","text":"Python<pre><code>from ragcrawl.filters import LinkFilter\n\n# Create comprehensive filter\nlink_filter = LinkFilter(\n    allowed_domains=[\"docs.python.org\"],\n    allow_subdomains=False,\n    include_patterns=[\n        \"/3/*\",          # Python 3 docs only\n    ],\n    exclude_patterns=[\n        \"*/whatsnew/*\",  # Skip what's new pages\n        \"*/_sources/*\",  # Skip source files\n    ],\n    blocked_extensions=[\n        \".pdf\", \".zip\", \".tar.gz\",\n        \".png\", \".jpg\", \".gif\", \".svg\",\n    ],\n)\n\n# Use in crawling\nfor url in discovered_urls:\n    result = link_filter.filter(url, check_seen=True)\n    if result.allowed:\n        link_filter.mark_seen(url)\n        queue.add(url)\n</code></pre>"},{"location":"api/filters/#module-reference","title":"Module Reference","text":"<p>URL filtering and normalization for ragcrawl.</p>"},{"location":"api/filters/#ragcrawl.filters.LinkFilter","title":"LinkFilter","text":"Python<pre><code>LinkFilter(\n    allowed_domains: list[str] | None = None,\n    allow_subdomains: bool = True,\n    allowed_schemes: list[str] | None = None,\n    allowed_path_prefixes: list[str] | None = None,\n    blocked_extensions: list[str] | None = None,\n    include_patterns: list[str] | None = None,\n    exclude_patterns: list[str] | None = None,\n    blocked_query_params: list[str] | None = None,\n)\n</code></pre> <p>Filters URLs based on domain, path, extension, and pattern constraints.</p> <p>This is the main filter used by the crawler to determine which URLs to include in the frontier.</p> <p>Initialize the link filter.</p> PARAMETER DESCRIPTION <code>allowed_domains</code> <p>Domains to allow (empty = all).</p> <p> TYPE: <code>list[str] | None</code> DEFAULT: <code>None</code> </p> <code>allow_subdomains</code> <p>Whether to allow subdomains of allowed_domains.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>allowed_schemes</code> <p>URL schemes to allow (default: http, https).</p> <p> TYPE: <code>list[str] | None</code> DEFAULT: <code>None</code> </p> <code>allowed_path_prefixes</code> <p>Path prefixes to allow (empty = all).</p> <p> TYPE: <code>list[str] | None</code> DEFAULT: <code>None</code> </p> <code>blocked_extensions</code> <p>File extensions to block.</p> <p> TYPE: <code>list[str] | None</code> DEFAULT: <code>None</code> </p> <code>include_patterns</code> <p>Regex/glob patterns for URLs to include.</p> <p> TYPE: <code>list[str] | None</code> DEFAULT: <code>None</code> </p> <code>exclude_patterns</code> <p>Regex/glob patterns for URLs to exclude.</p> <p> TYPE: <code>list[str] | None</code> DEFAULT: <code>None</code> </p> <code>blocked_query_params</code> <p>Query parameters to strip.</p> <p> TYPE: <code>list[str] | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/ragcrawl/filters/link_filter.py</code> Python<pre><code>def __init__(\n    self,\n    allowed_domains: list[str] | None = None,\n    allow_subdomains: bool = True,\n    allowed_schemes: list[str] | None = None,\n    allowed_path_prefixes: list[str] | None = None,\n    blocked_extensions: list[str] | None = None,\n    include_patterns: list[str] | None = None,\n    exclude_patterns: list[str] | None = None,\n    blocked_query_params: list[str] | None = None,\n) -&gt; None:\n    \"\"\"\n    Initialize the link filter.\n\n    Args:\n        allowed_domains: Domains to allow (empty = all).\n        allow_subdomains: Whether to allow subdomains of allowed_domains.\n        allowed_schemes: URL schemes to allow (default: http, https).\n        allowed_path_prefixes: Path prefixes to allow (empty = all).\n        blocked_extensions: File extensions to block.\n        include_patterns: Regex/glob patterns for URLs to include.\n        exclude_patterns: Regex/glob patterns for URLs to exclude.\n        blocked_query_params: Query parameters to strip.\n    \"\"\"\n    self.allowed_domains = set(d.lower() for d in (allowed_domains or []))\n    self.allow_subdomains = allow_subdomains\n    self.allowed_schemes = set(s.lower() for s in (allowed_schemes or [\"http\", \"https\"]))\n    self.allowed_path_prefixes = list(allowed_path_prefixes or [])\n\n    self.normalizer = URLNormalizer(remove_query_params=blocked_query_params)\n    self.pattern_matcher = PatternMatcher(include_patterns, exclude_patterns)\n    self.extension_filter = ExtensionFilter(blocked_extensions)\n\n    # Track seen URLs for deduplication\n    self._seen_urls: set[str] = set()\n</code></pre>"},{"location":"api/filters/#ragcrawl.filters.LinkFilter.seen_count","title":"seen_count  <code>property</code>","text":"Python<pre><code>seen_count: int\n</code></pre> <p>Get the count of seen URLs.</p>"},{"location":"api/filters/#ragcrawl.filters.LinkFilter.filter","title":"filter","text":"Python<pre><code>filter(\n    url: str,\n    check_seen: bool = True,\n    current_depth: int = 0,\n    max_depth: int | None = None,\n) -&gt; FilterResult\n</code></pre> <p>Filter a URL and return the result.</p> PARAMETER DESCRIPTION <code>url</code> <p>The URL to filter.</p> <p> TYPE: <code>str</code> </p> <code>check_seen</code> <p>Whether to check if URL was already seen.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>current_depth</code> <p>Current crawl depth.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>max_depth</code> <p>Maximum allowed depth.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>FilterResult</code> <p>FilterResult with allowed status and reason.</p> Source code in <code>src/ragcrawl/filters/link_filter.py</code> Python<pre><code>def filter(\n    self,\n    url: str,\n    check_seen: bool = True,\n    current_depth: int = 0,\n    max_depth: int | None = None,\n) -&gt; FilterResult:\n    \"\"\"\n    Filter a URL and return the result.\n\n    Args:\n        url: The URL to filter.\n        check_seen: Whether to check if URL was already seen.\n        current_depth: Current crawl depth.\n        max_depth: Maximum allowed depth.\n\n    Returns:\n        FilterResult with allowed status and reason.\n    \"\"\"\n    # Parse and validate URL\n    try:\n        parsed = urlparse(url)\n    except Exception:\n        return FilterResult(\n            allowed=False,\n            reason=FilterReason.INVALID_URL,\n            details=\"Failed to parse URL\",\n        )\n\n    if not parsed.scheme or not parsed.netloc:\n        return FilterResult(\n            allowed=False,\n            reason=FilterReason.INVALID_URL,\n            details=\"Missing scheme or netloc\",\n        )\n\n    # Scheme check\n    if parsed.scheme.lower() not in self.allowed_schemes:\n        return FilterResult(\n            allowed=False,\n            reason=FilterReason.INVALID_SCHEME,\n            details=f\"Scheme '{parsed.scheme}' not allowed\",\n        )\n\n    # Normalize URL\n    normalized = self.normalizer.normalize(url)\n\n    # Deduplication check\n    if check_seen and normalized in self._seen_urls:\n        return FilterResult(\n            allowed=False,\n            reason=FilterReason.ALREADY_SEEN,\n            normalized_url=normalized,\n        )\n\n    # Depth check\n    if max_depth is not None and current_depth &gt; max_depth:\n        return FilterResult(\n            allowed=False,\n            reason=FilterReason.MAX_DEPTH_EXCEEDED,\n            normalized_url=normalized,\n            details=f\"Depth {current_depth} exceeds max {max_depth}\",\n        )\n\n    # Domain check\n    if self.allowed_domains:\n        hostname = parsed.netloc.lower()\n        # Remove port if present\n        if \":\" in hostname:\n            hostname = hostname.split(\":\")[0]\n\n        if not self._is_domain_allowed(hostname):\n            return FilterResult(\n                allowed=False,\n                reason=FilterReason.DOMAIN_NOT_ALLOWED,\n                normalized_url=normalized,\n                details=f\"Domain '{hostname}' not in allowed list\",\n            )\n\n    # Path prefix check\n    if self.allowed_path_prefixes:\n        path = parsed.path\n        if not any(path.startswith(prefix) for prefix in self.allowed_path_prefixes):\n            return FilterResult(\n                allowed=False,\n                reason=FilterReason.PATH_NOT_ALLOWED,\n                normalized_url=normalized,\n                details=f\"Path '{path}' doesn't match allowed prefixes\",\n            )\n\n    # Extension check\n    if self.extension_filter.is_blocked(url):\n        ext = self.extension_filter.get_extension(url)\n        return FilterResult(\n            allowed=False,\n            reason=FilterReason.BLOCKED_EXTENSION,\n            normalized_url=normalized,\n            details=f\"Extension '{ext}' is blocked\",\n        )\n\n    # Pattern check\n    if not self.pattern_matcher.should_include(url):\n        reason = self.pattern_matcher.get_match_reason(url)\n        if self.pattern_matcher.matches_exclude(url):\n            return FilterResult(\n                allowed=False,\n                reason=FilterReason.EXCLUDED_PATTERN,\n                normalized_url=normalized,\n                details=reason,\n            )\n        else:\n            return FilterResult(\n                allowed=False,\n                reason=FilterReason.NO_INCLUDE_MATCH,\n                normalized_url=normalized,\n                details=reason,\n            )\n\n    # URL is allowed\n    return FilterResult(\n        allowed=True,\n        reason=FilterReason.ALLOWED,\n        normalized_url=normalized,\n    )\n</code></pre>"},{"location":"api/filters/#ragcrawl.filters.LinkFilter.mark_seen","title":"mark_seen","text":"Python<pre><code>mark_seen(url: str) -&gt; str\n</code></pre> <p>Mark a URL as seen and return normalized form.</p> PARAMETER DESCRIPTION <code>url</code> <p>The URL to mark.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>str</code> <p>The normalized URL.</p> Source code in <code>src/ragcrawl/filters/link_filter.py</code> Python<pre><code>def mark_seen(self, url: str) -&gt; str:\n    \"\"\"\n    Mark a URL as seen and return normalized form.\n\n    Args:\n        url: The URL to mark.\n\n    Returns:\n        The normalized URL.\n    \"\"\"\n    normalized = self.normalizer.normalize(url)\n    self._seen_urls.add(normalized)\n    return normalized\n</code></pre>"},{"location":"api/filters/#ragcrawl.filters.LinkFilter.is_seen","title":"is_seen","text":"Python<pre><code>is_seen(url: str) -&gt; bool\n</code></pre> <p>Check if URL has been seen.</p> PARAMETER DESCRIPTION <code>url</code> <p>The URL to check.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>True if URL has been seen.</p> Source code in <code>src/ragcrawl/filters/link_filter.py</code> Python<pre><code>def is_seen(self, url: str) -&gt; bool:\n    \"\"\"\n    Check if URL has been seen.\n\n    Args:\n        url: The URL to check.\n\n    Returns:\n        True if URL has been seen.\n    \"\"\"\n    normalized = self.normalizer.normalize(url)\n    return normalized in self._seen_urls\n</code></pre>"},{"location":"api/filters/#ragcrawl.filters.LinkFilter.clear_seen","title":"clear_seen","text":"Python<pre><code>clear_seen() -&gt; None\n</code></pre> <p>Clear the set of seen URLs.</p> Source code in <code>src/ragcrawl/filters/link_filter.py</code> Python<pre><code>def clear_seen(self) -&gt; None:\n    \"\"\"Clear the set of seen URLs.\"\"\"\n    self._seen_urls.clear()\n</code></pre>"},{"location":"api/filters/#ragcrawl.filters.PatternMatcher","title":"PatternMatcher","text":"Python<pre><code>PatternMatcher(\n    include_patterns: list[str] | None = None,\n    exclude_patterns: list[str] | None = None,\n    case_sensitive: bool = False,\n)\n</code></pre> <p>Matches URLs against include/exclude patterns.</p> <p>Supports both regex and glob patterns.</p> <p>Initialize the pattern matcher.</p> PARAMETER DESCRIPTION <code>include_patterns</code> <p>Patterns for URLs to include (regex or glob).</p> <p> TYPE: <code>list[str] | None</code> DEFAULT: <code>None</code> </p> <code>exclude_patterns</code> <p>Patterns for URLs to exclude (regex or glob).</p> <p> TYPE: <code>list[str] | None</code> DEFAULT: <code>None</code> </p> <code>case_sensitive</code> <p>Whether pattern matching is case-sensitive.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>src/ragcrawl/filters/patterns.py</code> Python<pre><code>def __init__(\n    self,\n    include_patterns: list[str] | None = None,\n    exclude_patterns: list[str] | None = None,\n    case_sensitive: bool = False,\n) -&gt; None:\n    \"\"\"\n    Initialize the pattern matcher.\n\n    Args:\n        include_patterns: Patterns for URLs to include (regex or glob).\n        exclude_patterns: Patterns for URLs to exclude (regex or glob).\n        case_sensitive: Whether pattern matching is case-sensitive.\n    \"\"\"\n    self.case_sensitive = case_sensitive\n    self._include_patterns = self._compile_patterns(include_patterns or [])\n    self._exclude_patterns = self._compile_patterns(exclude_patterns or [])\n</code></pre>"},{"location":"api/filters/#ragcrawl.filters.PatternMatcher.matches_include","title":"matches_include","text":"Python<pre><code>matches_include(url: str) -&gt; bool\n</code></pre> <p>Check if URL matches any include pattern.</p> PARAMETER DESCRIPTION <code>url</code> <p>The URL to check.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>True if URL matches an include pattern or no include patterns defined.</p> Source code in <code>src/ragcrawl/filters/patterns.py</code> Python<pre><code>def matches_include(self, url: str) -&gt; bool:\n    \"\"\"\n    Check if URL matches any include pattern.\n\n    Args:\n        url: The URL to check.\n\n    Returns:\n        True if URL matches an include pattern or no include patterns defined.\n    \"\"\"\n    if not self._include_patterns:\n        return True\n\n    return any(p.search(url) for p in self._include_patterns)\n</code></pre>"},{"location":"api/filters/#ragcrawl.filters.PatternMatcher.matches_exclude","title":"matches_exclude","text":"Python<pre><code>matches_exclude(url: str) -&gt; bool\n</code></pre> <p>Check if URL matches any exclude pattern.</p> PARAMETER DESCRIPTION <code>url</code> <p>The URL to check.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>True if URL matches an exclude pattern.</p> Source code in <code>src/ragcrawl/filters/patterns.py</code> Python<pre><code>def matches_exclude(self, url: str) -&gt; bool:\n    \"\"\"\n    Check if URL matches any exclude pattern.\n\n    Args:\n        url: The URL to check.\n\n    Returns:\n        True if URL matches an exclude pattern.\n    \"\"\"\n    if not self._exclude_patterns:\n        return False\n\n    return any(p.search(url) for p in self._exclude_patterns)\n</code></pre>"},{"location":"api/filters/#ragcrawl.filters.PatternMatcher.should_include","title":"should_include","text":"Python<pre><code>should_include(url: str) -&gt; bool\n</code></pre> <p>Determine if URL should be included based on patterns.</p> <p>Exclude patterns take precedence over include patterns.</p> PARAMETER DESCRIPTION <code>url</code> <p>The URL to check.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>True if URL should be included.</p> Source code in <code>src/ragcrawl/filters/patterns.py</code> Python<pre><code>def should_include(self, url: str) -&gt; bool:\n    \"\"\"\n    Determine if URL should be included based on patterns.\n\n    Exclude patterns take precedence over include patterns.\n\n    Args:\n        url: The URL to check.\n\n    Returns:\n        True if URL should be included.\n    \"\"\"\n    # Exclude takes precedence\n    if self.matches_exclude(url):\n        return False\n\n    return self.matches_include(url)\n</code></pre>"},{"location":"api/filters/#ragcrawl.filters.PatternMatcher.get_match_reason","title":"get_match_reason","text":"Python<pre><code>get_match_reason(url: str) -&gt; str | None\n</code></pre> <p>Get the reason for inclusion/exclusion.</p> PARAMETER DESCRIPTION <code>url</code> <p>The URL to check.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>str | None</code> <p>A string describing the match, or None if included by default.</p> Source code in <code>src/ragcrawl/filters/patterns.py</code> Python<pre><code>def get_match_reason(self, url: str) -&gt; str | None:\n    \"\"\"\n    Get the reason for inclusion/exclusion.\n\n    Args:\n        url: The URL to check.\n\n    Returns:\n        A string describing the match, or None if included by default.\n    \"\"\"\n    for pattern in self._exclude_patterns:\n        if pattern.search(url):\n            return f\"excluded by pattern: {pattern.pattern}\"\n\n    if self._include_patterns:\n        for pattern in self._include_patterns:\n            if pattern.search(url):\n                return f\"included by pattern: {pattern.pattern}\"\n\n        return \"no include pattern matched\"\n\n    return None\n</code></pre>"},{"location":"api/filters/#ragcrawl.filters.URLNormalizer","title":"URLNormalizer","text":"Python<pre><code>URLNormalizer(\n    remove_fragments: bool = True,\n    normalize_trailing_slash: bool = True,\n    sort_query_params: bool = True,\n    remove_query_params: list[str] | None = None,\n    lowercase_hostname: bool = True,\n    remove_default_ports: bool = True,\n    remove_www: bool = False,\n)\n</code></pre> <p>Normalizes URLs for deterministic deduplication.</p> <p>Handles: - Fragment removal - Trailing slash normalization - Query parameter sorting and filtering - Scheme normalization - Case normalization for hostname - Path normalization</p> <p>Initialize the URL normalizer.</p> PARAMETER DESCRIPTION <code>remove_fragments</code> <p>Remove URL fragments (#...).</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>normalize_trailing_slash</code> <p>Ensure consistent trailing slash handling.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>sort_query_params</code> <p>Sort query parameters alphabetically.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>remove_query_params</code> <p>List of query params to remove (e.g., tracking params).</p> <p> TYPE: <code>list[str] | None</code> DEFAULT: <code>None</code> </p> <code>lowercase_hostname</code> <p>Lowercase the hostname.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>remove_default_ports</code> <p>Remove default ports (80, 443).</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>remove_www</code> <p>Remove www. prefix from hostname.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>src/ragcrawl/filters/url_normalizer.py</code> Python<pre><code>def __init__(\n    self,\n    remove_fragments: bool = True,\n    normalize_trailing_slash: bool = True,\n    sort_query_params: bool = True,\n    remove_query_params: list[str] | None = None,\n    lowercase_hostname: bool = True,\n    remove_default_ports: bool = True,\n    remove_www: bool = False,\n) -&gt; None:\n    \"\"\"\n    Initialize the URL normalizer.\n\n    Args:\n        remove_fragments: Remove URL fragments (#...).\n        normalize_trailing_slash: Ensure consistent trailing slash handling.\n        sort_query_params: Sort query parameters alphabetically.\n        remove_query_params: List of query params to remove (e.g., tracking params).\n        lowercase_hostname: Lowercase the hostname.\n        remove_default_ports: Remove default ports (80, 443).\n        remove_www: Remove www. prefix from hostname.\n    \"\"\"\n    self.remove_fragments = remove_fragments\n    self.normalize_trailing_slash = normalize_trailing_slash\n    self.sort_query_params = sort_query_params\n    self.remove_query_params = set(remove_query_params or [])\n    self.lowercase_hostname = lowercase_hostname\n    self.remove_default_ports = remove_default_ports\n    self.remove_www = remove_www\n\n    # Default tracking params to remove\n    self.default_tracking_params = {\n        \"utm_source\",\n        \"utm_medium\",\n        \"utm_campaign\",\n        \"utm_term\",\n        \"utm_content\",\n        \"fbclid\",\n        \"gclid\",\n        \"ref\",\n        \"source\",\n    }\n</code></pre>"},{"location":"api/filters/#ragcrawl.filters.URLNormalizer.normalize","title":"normalize","text":"Python<pre><code>normalize(url: str) -&gt; str\n</code></pre> <p>Normalize a URL for deduplication.</p> PARAMETER DESCRIPTION <code>url</code> <p>The URL to normalize.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>str</code> <p>The normalized URL string.</p> Source code in <code>src/ragcrawl/filters/url_normalizer.py</code> Python<pre><code>def normalize(self, url: str) -&gt; str:\n    \"\"\"\n    Normalize a URL for deduplication.\n\n    Args:\n        url: The URL to normalize.\n\n    Returns:\n        The normalized URL string.\n    \"\"\"\n    try:\n        parsed = urlparse(url)\n    except Exception:\n        return url\n\n    # Scheme normalization (lowercase)\n    scheme = parsed.scheme.lower()\n\n    # Hostname normalization\n    hostname = parsed.netloc\n    if self.lowercase_hostname:\n        hostname = hostname.lower()\n\n    # Remove default ports\n    if self.remove_default_ports:\n        if scheme == \"http\" and hostname.endswith(\":80\"):\n            hostname = hostname[:-3]\n        elif scheme == \"https\" and hostname.endswith(\":443\"):\n            hostname = hostname[:-4]\n\n    # Remove www prefix\n    if self.remove_www and hostname.startswith(\"www.\"):\n        hostname = hostname[4:]\n\n    # Path normalization\n    path = parsed.path\n\n    # Remove duplicate slashes\n    path = re.sub(r\"/+\", \"/\", path)\n\n    # Normalize path encoding\n    # Decode safe characters that don't need encoding\n    path = path.replace(\"%7E\", \"~\")\n\n    # Handle trailing slash\n    if self.normalize_trailing_slash:\n        # Keep trailing slash only for directories (no extension)\n        if path and not path.endswith(\"/\"):\n            # Check if it looks like a file (has extension)\n            last_segment = path.split(\"/\")[-1]\n            if \".\" not in last_segment and path != \"/\":\n                # It's a directory-like path, could add trailing slash\n                # But for consistency, we'll remove trailing slashes\n                pass\n        # Remove trailing slash except for root\n        if path != \"/\" and path.endswith(\"/\"):\n            path = path.rstrip(\"/\")\n\n    # Empty path becomes /\n    if not path:\n        path = \"/\"\n\n    # Query parameter normalization\n    query = parsed.query\n    if query:\n        params = parse_qs(query, keep_blank_values=True)\n\n        # Remove tracking and specified params\n        params_to_remove = self.remove_query_params | self.default_tracking_params\n        params = {k: v for k, v in params.items() if k not in params_to_remove}\n\n        # Sort and rebuild query string\n        if self.sort_query_params:\n            sorted_params = sorted(params.items())\n            # Flatten multi-value params\n            flat_params = []\n            for k, values in sorted_params:\n                for v in sorted(values):\n                    flat_params.append((k, v))\n            query = urlencode(flat_params)\n        else:\n            query = urlencode(params, doseq=True)\n    else:\n        query = \"\"\n\n    # Fragment handling\n    fragment = \"\" if self.remove_fragments else parsed.fragment\n\n    # Rebuild URL\n    normalized = urlunparse((scheme, hostname, path, \"\", query, fragment))\n\n    return normalized\n</code></pre>"},{"location":"api/filters/#ragcrawl.filters.URLNormalizer.get_domain","title":"get_domain","text":"Python<pre><code>get_domain(url: str) -&gt; str\n</code></pre> <p>Extract the domain from a URL.</p> PARAMETER DESCRIPTION <code>url</code> <p>The URL.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>str</code> <p>The domain (e.g., 'example.com').</p> Source code in <code>src/ragcrawl/filters/url_normalizer.py</code> Python<pre><code>def get_domain(self, url: str) -&gt; str:\n    \"\"\"\n    Extract the domain from a URL.\n\n    Args:\n        url: The URL.\n\n    Returns:\n        The domain (e.g., 'example.com').\n    \"\"\"\n    try:\n        parsed = urlparse(url)\n        hostname = parsed.netloc.lower()\n\n        # Remove port\n        if \":\" in hostname:\n            hostname = hostname.split(\":\")[0]\n\n        return hostname\n    except Exception:\n        return \"\"\n</code></pre>"},{"location":"api/filters/#ragcrawl.filters.URLNormalizer.get_registered_domain","title":"get_registered_domain","text":"Python<pre><code>get_registered_domain(url: str) -&gt; str\n</code></pre> <p>Extract the registered domain (eTLD+1) from a URL.</p> PARAMETER DESCRIPTION <code>url</code> <p>The URL.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>str</code> <p>The registered domain (e.g., 'example.com' for 'sub.example.com').</p> Source code in <code>src/ragcrawl/filters/url_normalizer.py</code> Python<pre><code>def get_registered_domain(self, url: str) -&gt; str:\n    \"\"\"\n    Extract the registered domain (eTLD+1) from a URL.\n\n    Args:\n        url: The URL.\n\n    Returns:\n        The registered domain (e.g., 'example.com' for 'sub.example.com').\n    \"\"\"\n    try:\n        extracted = tldextract.extract(url)\n        if extracted.domain and extracted.suffix:\n            return f\"{extracted.domain}.{extracted.suffix}\"\n        return extracted.domain or \"\"\n    except Exception:\n        return \"\"\n</code></pre>"},{"location":"api/filters/#ragcrawl.filters.URLNormalizer.is_same_domain","title":"is_same_domain","text":"Python<pre><code>is_same_domain(url1: str, url2: str) -&gt; bool\n</code></pre> <p>Check if two URLs are on the same domain.</p> PARAMETER DESCRIPTION <code>url1</code> <p>First URL.</p> <p> TYPE: <code>str</code> </p> <code>url2</code> <p>Second URL.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>True if same domain.</p> Source code in <code>src/ragcrawl/filters/url_normalizer.py</code> Python<pre><code>def is_same_domain(self, url1: str, url2: str) -&gt; bool:\n    \"\"\"\n    Check if two URLs are on the same domain.\n\n    Args:\n        url1: First URL.\n        url2: Second URL.\n\n    Returns:\n        True if same domain.\n    \"\"\"\n    return self.get_domain(url1) == self.get_domain(url2)\n</code></pre>"},{"location":"api/filters/#ragcrawl.filters.URLNormalizer.is_same_registered_domain","title":"is_same_registered_domain","text":"Python<pre><code>is_same_registered_domain(url1: str, url2: str) -&gt; bool\n</code></pre> <p>Check if two URLs are on the same registered domain.</p> <p>This considers subdomains as the same domain.</p> PARAMETER DESCRIPTION <code>url1</code> <p>First URL.</p> <p> TYPE: <code>str</code> </p> <code>url2</code> <p>Second URL.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>True if same registered domain.</p> Source code in <code>src/ragcrawl/filters/url_normalizer.py</code> Python<pre><code>def is_same_registered_domain(self, url1: str, url2: str) -&gt; bool:\n    \"\"\"\n    Check if two URLs are on the same registered domain.\n\n    This considers subdomains as the same domain.\n\n    Args:\n        url1: First URL.\n        url2: Second URL.\n\n    Returns:\n        True if same registered domain.\n    \"\"\"\n    return self.get_registered_domain(url1) == self.get_registered_domain(url2)\n</code></pre>"},{"location":"api/models/","title":"Models","text":"<p>Data models used throughout ragcrawl.</p>"},{"location":"api/models/#overview","title":"Overview","text":"<p>ragcrawl uses Pydantic models for type-safe data handling:</p> Model Description Document Crawled page content for output Site Crawl configuration and metadata Page Page state and freshness tracking Chunk Content chunks for embeddings <code>PageVersion</code> Versioned content snapshots <code>CrawlRun</code> Crawl execution records <code>FrontierItem</code> URL queue items"},{"location":"api/models/#model-relationships","title":"Model Relationships","text":"<pre><code>erDiagram\n    Site ||--o{ CrawlRun : has\n    Site ||--o{ Page : contains\n    Page ||--o{ PageVersion : versions\n    CrawlRun ||--o{ PageVersion : creates\n    CrawlRun ||--o{ FrontierItem : manages\n    PageVersion ||--|| Document : exports_to\n    Document ||--o{ Chunk : chunks_into</code></pre>"},{"location":"api/models/#quick-reference","title":"Quick Reference","text":""},{"location":"api/models/#document","title":"Document","text":"<p>The output model for crawled content:</p> Python<pre><code>from ragcrawl.models import Document\n\ndoc = Document(\n    doc_id=\"abc123\",\n    page_id=\"abc123\",\n    source_url=\"https://example.com/page\",\n    normalized_url=\"https://example.com/page\",\n    markdown=\"# Page Title\\n\\nContent here...\",\n    title=\"Page Title\",\n    status_code=200,\n    content_type=\"text/html\",\n    depth=1,\n    run_id=\"run_xyz\",\n    site_id=\"site_abc\",\n    first_seen=datetime.now(),\n    last_seen=datetime.now(),\n    last_crawled=datetime.now(),\n)\n</code></pre>"},{"location":"api/models/#site","title":"Site","text":"<p>Configuration snapshot for a crawled site:</p> Python<pre><code>from ragcrawl.models import Site\n\nsite = Site(\n    site_id=\"site_abc123\",\n    name=\"Example Docs\",\n    seeds=[\"https://docs.example.com\"],\n    allowed_domains=[\"docs.example.com\"],\n    created_at=datetime.now(),\n    updated_at=datetime.now(),\n)\n</code></pre>"},{"location":"api/models/#chunk","title":"Chunk","text":"<p>Content chunk for embeddings:</p> Python<pre><code>from ragcrawl.models import Chunk\n\nchunk = Chunk(\n    chunk_id=\"chunk_001\",\n    doc_id=\"abc123\",\n    content=\"This is the chunk content...\",\n    chunk_index=0,\n    total_chunks=5,\n    section_path=[\"Introduction\", \"Overview\"],\n    heading=\"Overview\",\n    token_estimate=150,\n)\n</code></pre>"},{"location":"api/models/#module-reference","title":"Module Reference","text":"<p>Data models for ragcrawl.</p>"},{"location":"api/models/#ragcrawl.models.Document","title":"Document","text":"<p>               Bases: <code>BaseModel</code></p> <p>A crawled document with rich metadata for LLM/RAG consumption.</p> <p>This is the primary output model containing all extracted content and metadata from a crawled page.</p>"},{"location":"api/models/#ragcrawl.models.Site","title":"Site","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a website/crawl target with its configuration.</p> <p>Stores the configuration snapshot and metadata for a crawl target.</p>"},{"location":"api/models/#ragcrawl.models.Page","title":"Page","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents the current state of a URL in the crawl database.</p> <p>This model tracks freshness information and points to the current version of the page content. It's used for incremental sync to determine what needs re-crawling.</p>"},{"location":"api/models/#ragcrawl.models.Page.needs_recrawl","title":"needs_recrawl","text":"Python<pre><code>needs_recrawl(\n    max_age_hours: float | None = None, force: bool = False\n) -&gt; bool\n</code></pre> <p>Determine if this page needs to be re-crawled.</p> PARAMETER DESCRIPTION <code>max_age_hours</code> <p>Maximum age in hours before recrawl. None means always recrawl.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>force</code> <p>If True, always return True.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>True if the page should be re-crawled.</p> Source code in <code>src/ragcrawl/models/page.py</code> Python<pre><code>def needs_recrawl(\n    self,\n    max_age_hours: float | None = None,\n    force: bool = False,\n) -&gt; bool:\n    \"\"\"\n    Determine if this page needs to be re-crawled.\n\n    Args:\n        max_age_hours: Maximum age in hours before recrawl. None means always recrawl.\n        force: If True, always return True.\n\n    Returns:\n        True if the page should be re-crawled.\n    \"\"\"\n    if force:\n        return True\n\n    if self.is_tombstone:\n        return False\n\n    if self.last_crawled is None:\n        return True\n\n    if max_age_hours is None:\n        return True\n\n    age = datetime.now() - self.last_crawled\n    return age.total_seconds() / 3600 &gt; max_age_hours\n</code></pre>"},{"location":"api/models/#ragcrawl.models.PageVersion","title":"PageVersion","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a specific version of a page's content.</p> <p>Each time content changes, a new PageVersion is created. This enables version history and change tracking for KB updates.</p>"},{"location":"api/models/#ragcrawl.models.Chunk","title":"Chunk","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a chunk of content for RAG/embedding pipelines.</p> <p>Chunks are segments of a document optimized for vector embedding and retrieval, with metadata for context reconstruction.</p>"},{"location":"api/models/#ragcrawl.models.Chunk.is_first","title":"is_first  <code>property</code>","text":"Python<pre><code>is_first: bool\n</code></pre> <p>Check if this is the first chunk.</p>"},{"location":"api/models/#ragcrawl.models.Chunk.is_last","title":"is_last  <code>property</code>","text":"Python<pre><code>is_last: bool\n</code></pre> <p>Check if this is the last chunk.</p>"},{"location":"api/models/#ragcrawl.models.CrawlRun","title":"CrawlRun","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a single crawl or sync execution.</p> <p>Tracks the status, configuration snapshot, and statistics for a crawl run.</p>"},{"location":"api/models/#ragcrawl.models.CrawlRun.duration_seconds","title":"duration_seconds  <code>property</code>","text":"Python<pre><code>duration_seconds: float | None\n</code></pre> <p>Get run duration in seconds.</p>"},{"location":"api/models/#ragcrawl.models.CrawlRun.mark_started","title":"mark_started","text":"Python<pre><code>mark_started() -&gt; None\n</code></pre> <p>Mark the run as started.</p> Source code in <code>src/ragcrawl/models/crawl_run.py</code> Python<pre><code>def mark_started(self) -&gt; None:\n    \"\"\"Mark the run as started.\"\"\"\n    self.status = RunStatus.RUNNING\n    self.started_at = datetime.now()\n</code></pre>"},{"location":"api/models/#ragcrawl.models.CrawlRun.mark_completed","title":"mark_completed","text":"Python<pre><code>mark_completed(partial: bool = False) -&gt; None\n</code></pre> <p>Mark the run as completed.</p> Source code in <code>src/ragcrawl/models/crawl_run.py</code> Python<pre><code>def mark_completed(self, partial: bool = False) -&gt; None:\n    \"\"\"Mark the run as completed.\"\"\"\n    self.status = RunStatus.PARTIAL if partial else RunStatus.COMPLETED\n    self.completed_at = datetime.now()\n</code></pre>"},{"location":"api/models/#ragcrawl.models.CrawlRun.mark_failed","title":"mark_failed","text":"Python<pre><code>mark_failed(error: str) -&gt; None\n</code></pre> <p>Mark the run as failed.</p> Source code in <code>src/ragcrawl/models/crawl_run.py</code> Python<pre><code>def mark_failed(self, error: str) -&gt; None:\n    \"\"\"Mark the run as failed.\"\"\"\n    self.status = RunStatus.FAILED\n    self.error_message = error\n    self.completed_at = datetime.now()\n</code></pre>"},{"location":"api/models/#ragcrawl.models.CrawlRun.mark_cancelled","title":"mark_cancelled","text":"Python<pre><code>mark_cancelled() -&gt; None\n</code></pre> <p>Mark the run as cancelled.</p> Source code in <code>src/ragcrawl/models/crawl_run.py</code> Python<pre><code>def mark_cancelled(self) -&gt; None:\n    \"\"\"Mark the run as cancelled.\"\"\"\n    self.status = RunStatus.CANCELLED\n    self.completed_at = datetime.now()\n</code></pre>"},{"location":"api/models/#ragcrawl.models.FrontierItem","title":"FrontierItem","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a URL in the crawl frontier queue.</p> <p>Used for pause/resume functionality and tracking crawl progress.</p>"},{"location":"api/models/#ragcrawl.models.FrontierItem.mark_in_progress","title":"mark_in_progress","text":"Python<pre><code>mark_in_progress() -&gt; None\n</code></pre> <p>Mark item as being crawled.</p> Source code in <code>src/ragcrawl/models/frontier_item.py</code> Python<pre><code>def mark_in_progress(self) -&gt; None:\n    \"\"\"Mark item as being crawled.\"\"\"\n    self.status = FrontierStatus.IN_PROGRESS\n    self.started_at = datetime.now()\n</code></pre>"},{"location":"api/models/#ragcrawl.models.FrontierItem.mark_completed","title":"mark_completed","text":"Python<pre><code>mark_completed() -&gt; None\n</code></pre> <p>Mark item as successfully crawled.</p> Source code in <code>src/ragcrawl/models/frontier_item.py</code> Python<pre><code>def mark_completed(self) -&gt; None:\n    \"\"\"Mark item as successfully crawled.\"\"\"\n    self.status = FrontierStatus.COMPLETED\n    self.completed_at = datetime.now()\n</code></pre>"},{"location":"api/models/#ragcrawl.models.FrontierItem.mark_failed","title":"mark_failed","text":"Python<pre><code>mark_failed(error: str) -&gt; None\n</code></pre> <p>Mark item as failed.</p> Source code in <code>src/ragcrawl/models/frontier_item.py</code> Python<pre><code>def mark_failed(self, error: str) -&gt; None:\n    \"\"\"Mark item as failed.\"\"\"\n    self.status = FrontierStatus.FAILED\n    self.last_error = error\n    self.retry_count += 1\n    self.completed_at = datetime.now()\n</code></pre>"},{"location":"api/models/#ragcrawl.models.FrontierItem.mark_skipped","title":"mark_skipped","text":"Python<pre><code>mark_skipped(reason: str) -&gt; None\n</code></pre> <p>Mark item as skipped.</p> Source code in <code>src/ragcrawl/models/frontier_item.py</code> Python<pre><code>def mark_skipped(self, reason: str) -&gt; None:\n    \"\"\"Mark item as skipped.\"\"\"\n    self.status = FrontierStatus.SKIPPED\n    self.last_error = reason\n    self.completed_at = datetime.now()\n</code></pre>"},{"location":"api/models/chunk/","title":"Chunk","text":"<p>The <code>Chunk</code> model represents a content chunk ready for embedding.</p>"},{"location":"api/models/chunk/#overview","title":"Overview","text":"<p><code>Chunk</code> is produced by chunkers and contains:</p> <ul> <li>The chunk content text</li> <li>Position information (index, total)</li> <li>Section context (heading path)</li> <li>Token estimates</li> </ul>"},{"location":"api/models/chunk/#usage","title":"Usage","text":""},{"location":"api/models/chunk/#working-with-chunks","title":"Working with Chunks","text":"<p>Chunks are created by chunkers:</p> Python<pre><code>from ragcrawl.chunking import HeadingChunker\nfrom ragcrawl.models import Document\n\n# Create a chunker\nchunker = HeadingChunker(max_tokens=500)\n\n# Chunk a document\nchunks = chunker.chunk(document)\n\nfor chunk in chunks:\n    print(f\"Chunk {chunk.chunk_index + 1}/{chunk.total_chunks}\")\n    print(f\"Section: {' &gt; '.join(chunk.section_path)}\")\n    print(f\"Tokens: ~{chunk.token_estimate}\")\n    print(f\"Content: {chunk.content[:100]}...\")\n    print()\n</code></pre>"},{"location":"api/models/chunk/#chunk-metadata","title":"Chunk Metadata","text":"Python<pre><code># Access chunk context\nchunk = chunks[0]\n\n# Document reference\nprint(f\"From document: {chunk.doc_id}\")\n\n# Section path (breadcrumb)\nprint(f\"Section: {chunk.section_path}\")\n# e.g., [\"Getting Started\", \"Installation\", \"Requirements\"]\n\n# Current heading\nprint(f\"Heading: {chunk.heading}\")\n# e.g., \"Requirements\"\n\n# Position\nprint(f\"Position: {chunk.chunk_index + 1} of {chunk.total_chunks}\")\n</code></pre>"},{"location":"api/models/chunk/#preparing-for-embeddings","title":"Preparing for Embeddings","text":"Python<pre><code># Prepare chunks for embedding API\nfor chunk in chunks:\n    text = chunk.content\n\n    # Add context prefix\n    if chunk.section_path:\n        context = \" &gt; \".join(chunk.section_path)\n        text = f\"[{context}]\\n\\n{text}\"\n\n    # Send to embedding API\n    embedding = get_embedding(text)\n\n    # Store with metadata\n    store_embedding(\n        embedding=embedding,\n        metadata={\n            \"doc_id\": chunk.doc_id,\n            \"chunk_id\": chunk.chunk_id,\n            \"chunk_index\": chunk.chunk_index,\n            \"section\": chunk.section_path,\n            \"heading\": chunk.heading,\n        }\n    )\n</code></pre>"},{"location":"api/models/chunk/#fields","title":"Fields","text":"Field Type Description <code>chunk_id</code> str Unique chunk identifier <code>doc_id</code> str Source document ID <code>content</code> str Chunk text content <code>chunk_index</code> int Zero-based position <code>total_chunks</code> int Total chunks in document <code>section_path</code> list[str] Heading hierarchy <code>heading</code> str Current section heading <code>start_char</code> int Start character offset <code>end_char</code> int End character offset <code>char_count</code> int Character count <code>token_estimate</code> int Estimated token count"},{"location":"api/models/chunk/#api-reference","title":"API Reference","text":""},{"location":"api/models/chunk/#ragcrawl.models.chunk.Chunk","title":"Chunk","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a chunk of content for RAG/embedding pipelines.</p> <p>Chunks are segments of a document optimized for vector embedding and retrieval, with metadata for context reconstruction.</p>"},{"location":"api/models/chunk/#ragcrawl.models.chunk.Chunk.is_first","title":"is_first  <code>property</code>","text":"Python<pre><code>is_first: bool\n</code></pre> <p>Check if this is the first chunk.</p>"},{"location":"api/models/chunk/#ragcrawl.models.chunk.Chunk.is_last","title":"is_last  <code>property</code>","text":"Python<pre><code>is_last: bool\n</code></pre> <p>Check if this is the last chunk.</p>"},{"location":"api/models/document/","title":"Document","text":"<p>The <code>Document</code> model represents crawled page content ready for output and processing.</p>"},{"location":"api/models/document/#overview","title":"Overview","text":"<p><code>Document</code> is the primary output model that contains:</p> <ul> <li>Cleaned markdown content</li> <li>Page metadata (title, description)</li> <li>Source information (URL, status code)</li> <li>Crawl context (run ID, site ID, timestamps)</li> </ul>"},{"location":"api/models/document/#usage","title":"Usage","text":""},{"location":"api/models/document/#creating-documents","title":"Creating Documents","text":"<p>Documents are typically created by the crawl process, but you can create them manually:</p> Python<pre><code>from datetime import datetime, timezone\nfrom ragcrawl.models import Document\n\ndoc = Document(\n    doc_id=\"abc123\",\n    page_id=\"abc123\",\n    source_url=\"https://example.com/guide\",\n    normalized_url=\"https://example.com/guide\",\n    markdown=\"# User Guide\\n\\nWelcome to the guide...\",\n    title=\"User Guide\",\n    description=\"Complete user guide for the product\",\n    status_code=200,\n    content_type=\"text/html\",\n    depth=1,\n    run_id=\"run_xyz789\",\n    site_id=\"site_abc123\",\n    first_seen=datetime.now(timezone.utc),\n    last_seen=datetime.now(timezone.utc),\n    last_crawled=datetime.now(timezone.utc),\n)\n</code></pre>"},{"location":"api/models/document/#accessing-content","title":"Accessing Content","text":"Python<pre><code># Get the markdown content\nprint(doc.markdown)\n\n# Get metadata\nprint(f\"Title: {doc.title}\")\nprint(f\"Description: {doc.description}\")\nprint(f\"Word count: {doc.word_count}\")\n\n# Get source info\nprint(f\"URL: {doc.source_url}\")\nprint(f\"Status: {doc.status_code}\")\n</code></pre>"},{"location":"api/models/document/#document-properties","title":"Document Properties","text":"Python<pre><code># Check if document is valid\nif doc.status_code == 200 and not doc.is_tombstone:\n    process_document(doc)\n\n# Check content type\nif doc.content_type.startswith(\"text/html\"):\n    # Process as HTML-derived content\n    pass\n</code></pre>"},{"location":"api/models/document/#fields","title":"Fields","text":"Field Type Description <code>doc_id</code> str Unique document identifier <code>page_id</code> str Associated page ID <code>source_url</code> str Original URL <code>normalized_url</code> str Normalized URL for deduplication <code>markdown</code> str Cleaned markdown content <code>title</code> str Page title <code>description</code> str Page description/meta <code>status_code</code> int HTTP status code <code>content_type</code> str Content MIME type <code>language</code> str Detected language <code>depth</code> int Crawl depth from seed <code>word_count</code> int Word count <code>char_count</code> int Character count <code>outlinks</code> list[str] Outbound links <code>run_id</code> str Crawl run ID <code>site_id</code> str Site ID <code>first_seen</code> datetime First crawl time <code>last_seen</code> datetime Last seen time <code>last_crawled</code> datetime Last crawl time <code>is_tombstone</code> bool Deleted page marker"},{"location":"api/models/document/#api-reference","title":"API Reference","text":""},{"location":"api/models/document/#ragcrawl.models.document.Document","title":"Document","text":"<p>               Bases: <code>BaseModel</code></p> <p>A crawled document with rich metadata for LLM/RAG consumption.</p> <p>This is the primary output model containing all extracted content and metadata from a crawled page.</p>"},{"location":"api/models/document/#ragcrawl.models.document.Document.doc_id","title":"doc_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"Python<pre><code>doc_id: str = Field(\n    description=\"Stable ID: hash(normalized_url)\"\n)\n</code></pre>"},{"location":"api/models/document/#ragcrawl.models.document.Document.page_id","title":"page_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"Python<pre><code>page_id: str = Field(\n    description=\"Alias for doc_id for compatibility\"\n)\n</code></pre>"},{"location":"api/models/document/#ragcrawl.models.document.Document.source_url","title":"source_url  <code>class-attribute</code> <code>instance-attribute</code>","text":"Python<pre><code>source_url: str = Field(\n    description=\"Original URL as discovered\"\n)\n</code></pre>"},{"location":"api/models/document/#ragcrawl.models.document.Document.normalized_url","title":"normalized_url  <code>class-attribute</code> <code>instance-attribute</code>","text":"Python<pre><code>normalized_url: str = Field(\n    description=\"Normalized/canonical URL\"\n)\n</code></pre>"},{"location":"api/models/document/#ragcrawl.models.document.Document.markdown","title":"markdown  <code>class-attribute</code> <code>instance-attribute</code>","text":"Python<pre><code>markdown: str = Field(\n    description=\"Extracted clean Markdown content\"\n)\n</code></pre>"},{"location":"api/models/document/#ragcrawl.models.document.Document.title","title":"title  <code>class-attribute</code> <code>instance-attribute</code>","text":"Python<pre><code>title: str | None = Field(\n    default=None, description=\"Page title\"\n)\n</code></pre>"},{"location":"api/models/document/#ragcrawl.models.document.Document.description","title":"description  <code>class-attribute</code> <code>instance-attribute</code>","text":"Python<pre><code>description: str | None = Field(\n    default=None, description=\"Meta description\"\n)\n</code></pre>"},{"location":"api/models/document/#ragcrawl.models.document.Document.status_code","title":"status_code  <code>class-attribute</code> <code>instance-attribute</code>","text":"Python<pre><code>status_code: int = Field(description='HTTP status code')\n</code></pre>"},{"location":"api/models/document/#ragcrawl.models.document.Document.content_type","title":"content_type  <code>class-attribute</code> <code>instance-attribute</code>","text":"Python<pre><code>content_type: str | None = Field(\n    default=None, description=\"HTTP Content-Type\"\n)\n</code></pre>"},{"location":"api/models/page/","title":"Page","text":"<p>The <code>Page</code> model tracks the state and freshness of crawled pages.</p>"},{"location":"api/models/page/#overview","title":"Overview","text":"<p><code>Page</code> is an internal model that tracks:</p> <ul> <li>Page URL and identification</li> <li>Current content version</li> <li>Freshness information (ETags, Last-Modified)</li> <li>Crawl metadata (depth, status, errors)</li> </ul>"},{"location":"api/models/page/#usage","title":"Usage","text":""},{"location":"api/models/page/#accessing-pages","title":"Accessing Pages","text":"Python<pre><code>from ragcrawl.storage import create_storage_backend\n\nbackend = create_storage_backend(storage_config)\nbackend.initialize()\n\n# Get a specific page\npage = backend.get_page(page_id)\nprint(f\"URL: {page.url}\")\nprint(f\"Last crawled: {page.last_crawled}\")\nprint(f\"Status: {page.status_code}\")\n\n# Get page by URL\npage = backend.get_page_by_url(site_id, url)\n\n# List all pages for a site\npages = backend.list_pages(site_id)\nfor page in pages:\n    print(f\"{page.url}: {page.status_code}\")\n</code></pre>"},{"location":"api/models/page/#page-freshness","title":"Page Freshness","text":"Python<pre><code># Check if page needs re-crawling\nfrom datetime import datetime, timezone, timedelta\n\nmax_age = timedelta(hours=24)\nif page.last_crawled &lt; datetime.now(timezone.utc) - max_age:\n    print(f\"Page {page.url} needs refresh\")\n\n# Check using conditional request headers\nif page.etag:\n    headers = {\"If-None-Match\": page.etag}\nif page.last_modified:\n    headers = {\"If-Modified-Since\": page.last_modified}\n</code></pre>"},{"location":"api/models/page/#tombstone-pages","title":"Tombstone Pages","text":"Python<pre><code># Check if page was deleted\nif page.is_tombstone:\n    print(f\"Page {page.url} was removed from site\")\n\n# Get pages including tombstones\npages = backend.list_pages(site_id, include_tombstones=True)\n</code></pre>"},{"location":"api/models/page/#fields","title":"Fields","text":"Field Type Description <code>page_id</code> str Unique page identifier <code>site_id</code> str Parent site ID <code>url</code> str Page URL <code>canonical_url</code> str Canonical URL if different <code>current_version_id</code> str Current content version <code>content_hash</code> str Content hash for change detection <code>etag</code> str HTTP ETag header <code>last_modified</code> str HTTP Last-Modified header <code>first_seen</code> datetime First crawl time <code>last_seen</code> datetime Last seen in crawl <code>last_crawled</code> datetime Last successful crawl <code>last_changed</code> datetime Last content change <code>depth</code> int Link depth from seed <code>referrer_url</code> str Referring page URL <code>status_code</code> int Last HTTP status <code>is_tombstone</code> bool Page was deleted <code>error_count</code> int Consecutive errors <code>last_error</code> str Last error message <code>version_count</code> int Total versions"},{"location":"api/models/page/#api-reference","title":"API Reference","text":""},{"location":"api/models/page/#ragcrawl.models.page.Page","title":"Page","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents the current state of a URL in the crawl database.</p> <p>This model tracks freshness information and points to the current version of the page content. It's used for incremental sync to determine what needs re-crawling.</p>"},{"location":"api/models/page/#ragcrawl.models.page.Page.needs_recrawl","title":"needs_recrawl","text":"Python<pre><code>needs_recrawl(\n    max_age_hours: float | None = None, force: bool = False\n) -&gt; bool\n</code></pre> <p>Determine if this page needs to be re-crawled.</p> PARAMETER DESCRIPTION <code>max_age_hours</code> <p>Maximum age in hours before recrawl. None means always recrawl.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>force</code> <p>If True, always return True.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>True if the page should be re-crawled.</p> Source code in <code>src/ragcrawl/models/page.py</code> Python<pre><code>def needs_recrawl(\n    self,\n    max_age_hours: float | None = None,\n    force: bool = False,\n) -&gt; bool:\n    \"\"\"\n    Determine if this page needs to be re-crawled.\n\n    Args:\n        max_age_hours: Maximum age in hours before recrawl. None means always recrawl.\n        force: If True, always return True.\n\n    Returns:\n        True if the page should be re-crawled.\n    \"\"\"\n    if force:\n        return True\n\n    if self.is_tombstone:\n        return False\n\n    if self.last_crawled is None:\n        return True\n\n    if max_age_hours is None:\n        return True\n\n    age = datetime.now() - self.last_crawled\n    return age.total_seconds() / 3600 &gt; max_age_hours\n</code></pre>"},{"location":"api/models/site/","title":"Site","text":"<p>The <code>Site</code> model represents a crawled website and its configuration.</p>"},{"location":"api/models/site/#overview","title":"Overview","text":"<p><code>Site</code> stores:</p> <ul> <li>Site identification (ID, name)</li> <li>Seed URLs for crawling</li> <li>Domain restrictions</li> <li>Crawl statistics</li> <li>Timestamps</li> </ul>"},{"location":"api/models/site/#usage","title":"Usage","text":""},{"location":"api/models/site/#creating-a-site","title":"Creating a Site","text":"<p>Sites are typically created automatically during crawling:</p> Python<pre><code>from datetime import datetime, timezone\nfrom ragcrawl.models import Site\n\nsite = Site(\n    site_id=\"site_abc123\",\n    name=\"Example Documentation\",\n    seeds=[\"https://docs.example.com\"],\n    allowed_domains=[\"docs.example.com\"],\n    allowed_subdomains=True,\n    created_at=datetime.now(timezone.utc),\n    updated_at=datetime.now(timezone.utc),\n)\n</code></pre>"},{"location":"api/models/site/#accessing-site-data","title":"Accessing Site Data","text":"Python<pre><code># Get basic info\nprint(f\"Site: {site.name}\")\nprint(f\"ID: {site.site_id}\")\nprint(f\"Seeds: {site.seeds}\")\n\n# Get statistics\nprint(f\"Total pages: {site.total_pages}\")\nprint(f\"Total runs: {site.total_runs}\")\n\n# Check timestamps\nprint(f\"Created: {site.created_at}\")\nprint(f\"Last crawl: {site.last_crawl_at}\")\n</code></pre>"},{"location":"api/models/site/#listing-sites","title":"Listing Sites","text":"Python<pre><code>from ragcrawl.storage import create_storage_backend\n\nbackend = create_storage_backend(storage_config)\nbackend.initialize()\n\nsites = backend.list_sites()\nfor site in sites:\n    print(f\"{site.site_id}: {site.name} ({site.total_pages} pages)\")\n</code></pre>"},{"location":"api/models/site/#fields","title":"Fields","text":"Field Type Description <code>site_id</code> str Unique site identifier <code>name</code> str Human-readable name <code>seeds</code> list[str] Starting URLs <code>allowed_domains</code> list[str] Domains to crawl <code>allowed_subdomains</code> bool Allow subdomains <code>config</code> dict Configuration snapshot <code>created_at</code> datetime Creation time <code>updated_at</code> datetime Last update time <code>last_crawl_at</code> datetime Last crawl time <code>last_sync_at</code> datetime Last sync time <code>total_pages</code> int Total pages crawled <code>total_runs</code> int Total crawl runs <code>is_active</code> bool Site is active"},{"location":"api/models/site/#api-reference","title":"API Reference","text":""},{"location":"api/models/site/#ragcrawl.models.site.Site","title":"Site","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a website/crawl target with its configuration.</p> <p>Stores the configuration snapshot and metadata for a crawl target.</p>"},{"location":"api/storage/","title":"Storage API","text":"<p>ragcrawl supports pluggable storage backends for persisting crawl data.</p>"},{"location":"api/storage/#overview","title":"Overview","text":"Backend Description Use Case DuckDB Local file-based SQL database Default, local development DynamoDB AWS managed NoSQL database Cloud deployments, scalability"},{"location":"api/storage/#storage-backend-interface","title":"Storage Backend Interface","text":"<p>All backends implement the <code>StorageBackend</code> protocol:</p> Python<pre><code>from ragcrawl.storage import StorageBackend\n\nclass StorageBackend(Protocol):\n    # Lifecycle\n    def initialize(self) -&gt; None: ...\n    def close(self) -&gt; None: ...\n    def health_check(self) -&gt; bool: ...\n\n    # Sites\n    def save_site(self, site: Site) -&gt; None: ...\n    def get_site(self, site_id: str) -&gt; Site | None: ...\n    def list_sites(self) -&gt; list[Site]: ...\n\n    # Crawl Runs\n    def save_run(self, run: CrawlRun) -&gt; None: ...\n    def get_run(self, run_id: str) -&gt; CrawlRun | None: ...\n    def list_runs(self, site_id: str) -&gt; list[CrawlRun]: ...\n\n    # Pages\n    def save_page(self, page: Page) -&gt; None: ...\n    def get_page(self, page_id: str) -&gt; Page | None: ...\n    def get_page_by_url(self, site_id: str, url: str) -&gt; Page | None: ...\n    def list_pages(self, site_id: str) -&gt; list[Page]: ...\n\n    # Versions\n    def save_version(self, version: PageVersion) -&gt; None: ...\n    def get_version(self, version_id: str) -&gt; PageVersion | None: ...\n    def list_versions(self, page_id: str) -&gt; list[PageVersion]: ...\n\n    # Frontier\n    def save_frontier_item(self, item: FrontierItem) -&gt; None: ...\n    def get_frontier_items(self, run_id: str) -&gt; list[FrontierItem]: ...\n</code></pre>"},{"location":"api/storage/#quick-start","title":"Quick Start","text":""},{"location":"api/storage/#using-duckdb-default","title":"Using DuckDB (Default)","text":"Python<pre><code>from ragcrawl.config.storage_config import StorageConfig, DuckDBConfig\nfrom ragcrawl.storage import create_storage_backend\n\nconfig = StorageConfig(\n    backend=DuckDBConfig(path=\"./crawler.duckdb\")\n)\n\nbackend = create_storage_backend(config)\nbackend.initialize()\n\n# Use the backend\nsites = backend.list_sites()\n</code></pre>"},{"location":"api/storage/#using-dynamodb","title":"Using DynamoDB","text":"Python<pre><code>from ragcrawl.config.storage_config import StorageConfig, DynamoDBConfig\nfrom ragcrawl.storage import create_storage_backend\n\nconfig = StorageConfig(\n    backend=DynamoDBConfig(\n        table_prefix=\"ragcrawl_\",\n        region=\"us-west-2\",\n    )\n)\n\nbackend = create_storage_backend(config)\nbackend.initialize()\n</code></pre>"},{"location":"api/storage/#factory-function","title":"Factory Function","text":"<p>Use <code>create_storage_backend()</code> to create backends:</p> Python<pre><code>from ragcrawl.storage import create_storage_backend\n\n# Automatically selects backend based on config\nbackend = create_storage_backend(storage_config)\n</code></pre>"},{"location":"api/storage/#context-manager","title":"Context Manager","text":"<p>Backends support context manager protocol:</p> Python<pre><code>with create_storage_backend(config) as backend:\n    backend.initialize()\n    sites = backend.list_sites()\n# Automatically closed\n</code></pre>"},{"location":"api/storage/#module-reference","title":"Module Reference","text":"<p>Storage backend protocol and factory.</p>"},{"location":"api/storage/#ragcrawl.storage.backend.StorageBackend","title":"StorageBackend","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for storage backends.</p> <p>All backends must implement this interface to ensure feature parity.</p>"},{"location":"api/storage/#ragcrawl.storage.backend.StorageBackend.save_site","title":"save_site  <code>abstractmethod</code>","text":"Python<pre><code>save_site(site: Site) -&gt; None\n</code></pre> <p>Save or update a site.</p> Source code in <code>src/ragcrawl/storage/backend.py</code> Python<pre><code>@abstractmethod\ndef save_site(self, site: Site) -&gt; None:\n    \"\"\"Save or update a site.\"\"\"\n    ...\n</code></pre>"},{"location":"api/storage/#ragcrawl.storage.backend.StorageBackend.get_site","title":"get_site  <code>abstractmethod</code>","text":"Python<pre><code>get_site(site_id: str) -&gt; Site | None\n</code></pre> <p>Get a site by ID.</p> Source code in <code>src/ragcrawl/storage/backend.py</code> Python<pre><code>@abstractmethod\ndef get_site(self, site_id: str) -&gt; Site | None:\n    \"\"\"Get a site by ID.\"\"\"\n    ...\n</code></pre>"},{"location":"api/storage/#ragcrawl.storage.backend.StorageBackend.list_sites","title":"list_sites  <code>abstractmethod</code>","text":"Python<pre><code>list_sites() -&gt; list[Site]\n</code></pre> <p>List all sites.</p> Source code in <code>src/ragcrawl/storage/backend.py</code> Python<pre><code>@abstractmethod\ndef list_sites(self) -&gt; list[Site]:\n    \"\"\"List all sites.\"\"\"\n    ...\n</code></pre>"},{"location":"api/storage/#ragcrawl.storage.backend.StorageBackend.delete_site","title":"delete_site  <code>abstractmethod</code>","text":"Python<pre><code>delete_site(site_id: str) -&gt; bool\n</code></pre> <p>Delete a site and all associated data.</p> Source code in <code>src/ragcrawl/storage/backend.py</code> Python<pre><code>@abstractmethod\ndef delete_site(self, site_id: str) -&gt; bool:\n    \"\"\"Delete a site and all associated data.\"\"\"\n    ...\n</code></pre>"},{"location":"api/storage/#ragcrawl.storage.backend.StorageBackend.save_run","title":"save_run  <code>abstractmethod</code>","text":"Python<pre><code>save_run(run: CrawlRun) -&gt; None\n</code></pre> <p>Save or update a crawl run.</p> Source code in <code>src/ragcrawl/storage/backend.py</code> Python<pre><code>@abstractmethod\ndef save_run(self, run: CrawlRun) -&gt; None:\n    \"\"\"Save or update a crawl run.\"\"\"\n    ...\n</code></pre>"},{"location":"api/storage/#ragcrawl.storage.backend.StorageBackend.get_run","title":"get_run  <code>abstractmethod</code>","text":"Python<pre><code>get_run(run_id: str) -&gt; CrawlRun | None\n</code></pre> <p>Get a crawl run by ID.</p> Source code in <code>src/ragcrawl/storage/backend.py</code> Python<pre><code>@abstractmethod\ndef get_run(self, run_id: str) -&gt; CrawlRun | None:\n    \"\"\"Get a crawl run by ID.\"\"\"\n    ...\n</code></pre>"},{"location":"api/storage/#ragcrawl.storage.backend.StorageBackend.list_runs","title":"list_runs  <code>abstractmethod</code>","text":"Python<pre><code>list_runs(\n    site_id: str, limit: int = 100, offset: int = 0\n) -&gt; list[CrawlRun]\n</code></pre> <p>List crawl runs for a site.</p> Source code in <code>src/ragcrawl/storage/backend.py</code> Python<pre><code>@abstractmethod\ndef list_runs(\n    self,\n    site_id: str,\n    limit: int = 100,\n    offset: int = 0,\n) -&gt; list[CrawlRun]:\n    \"\"\"List crawl runs for a site.\"\"\"\n    ...\n</code></pre>"},{"location":"api/storage/#ragcrawl.storage.backend.StorageBackend.get_latest_run","title":"get_latest_run  <code>abstractmethod</code>","text":"Python<pre><code>get_latest_run(site_id: str) -&gt; CrawlRun | None\n</code></pre> <p>Get the latest crawl run for a site.</p> Source code in <code>src/ragcrawl/storage/backend.py</code> Python<pre><code>@abstractmethod\ndef get_latest_run(self, site_id: str) -&gt; CrawlRun | None:\n    \"\"\"Get the latest crawl run for a site.\"\"\"\n    ...\n</code></pre>"},{"location":"api/storage/#ragcrawl.storage.backend.StorageBackend.save_page","title":"save_page  <code>abstractmethod</code>","text":"Python<pre><code>save_page(page: Page) -&gt; None\n</code></pre> <p>Save or update a page.</p> Source code in <code>src/ragcrawl/storage/backend.py</code> Python<pre><code>@abstractmethod\ndef save_page(self, page: Page) -&gt; None:\n    \"\"\"Save or update a page.\"\"\"\n    ...\n</code></pre>"},{"location":"api/storage/#ragcrawl.storage.backend.StorageBackend.get_page","title":"get_page  <code>abstractmethod</code>","text":"Python<pre><code>get_page(page_id: str) -&gt; Page | None\n</code></pre> <p>Get a page by ID.</p> Source code in <code>src/ragcrawl/storage/backend.py</code> Python<pre><code>@abstractmethod\ndef get_page(self, page_id: str) -&gt; Page | None:\n    \"\"\"Get a page by ID.\"\"\"\n    ...\n</code></pre>"},{"location":"api/storage/#ragcrawl.storage.backend.StorageBackend.get_page_by_url","title":"get_page_by_url  <code>abstractmethod</code>","text":"Python<pre><code>get_page_by_url(site_id: str, url: str) -&gt; Page | None\n</code></pre> <p>Get a page by normalized URL.</p> Source code in <code>src/ragcrawl/storage/backend.py</code> Python<pre><code>@abstractmethod\ndef get_page_by_url(self, site_id: str, url: str) -&gt; Page | None:\n    \"\"\"Get a page by normalized URL.\"\"\"\n    ...\n</code></pre>"},{"location":"api/storage/#ragcrawl.storage.backend.StorageBackend.list_pages","title":"list_pages  <code>abstractmethod</code>","text":"Python<pre><code>list_pages(\n    site_id: str,\n    limit: int = 1000,\n    offset: int = 0,\n    include_tombstones: bool = False,\n) -&gt; list[Page]\n</code></pre> <p>List pages for a site.</p> Source code in <code>src/ragcrawl/storage/backend.py</code> Python<pre><code>@abstractmethod\ndef list_pages(\n    self,\n    site_id: str,\n    limit: int = 1000,\n    offset: int = 0,\n    include_tombstones: bool = False,\n) -&gt; list[Page]:\n    \"\"\"List pages for a site.\"\"\"\n    ...\n</code></pre>"},{"location":"api/storage/#ragcrawl.storage.backend.StorageBackend.get_pages_needing_recrawl","title":"get_pages_needing_recrawl  <code>abstractmethod</code>","text":"Python<pre><code>get_pages_needing_recrawl(\n    site_id: str,\n    max_age_hours: float | None = None,\n    limit: int = 1000,\n) -&gt; list[Page]\n</code></pre> <p>Get pages that need to be re-crawled.</p> Source code in <code>src/ragcrawl/storage/backend.py</code> Python<pre><code>@abstractmethod\ndef get_pages_needing_recrawl(\n    self,\n    site_id: str,\n    max_age_hours: float | None = None,\n    limit: int = 1000,\n) -&gt; list[Page]:\n    \"\"\"Get pages that need to be re-crawled.\"\"\"\n    ...\n</code></pre>"},{"location":"api/storage/#ragcrawl.storage.backend.StorageBackend.count_pages","title":"count_pages  <code>abstractmethod</code>","text":"Python<pre><code>count_pages(\n    site_id: str, include_tombstones: bool = False\n) -&gt; int\n</code></pre> <p>Count pages for a site.</p> Source code in <code>src/ragcrawl/storage/backend.py</code> Python<pre><code>@abstractmethod\ndef count_pages(self, site_id: str, include_tombstones: bool = False) -&gt; int:\n    \"\"\"Count pages for a site.\"\"\"\n    ...\n</code></pre>"},{"location":"api/storage/#ragcrawl.storage.backend.StorageBackend.save_version","title":"save_version  <code>abstractmethod</code>","text":"Python<pre><code>save_version(version: PageVersion) -&gt; None\n</code></pre> <p>Save a page version.</p> Source code in <code>src/ragcrawl/storage/backend.py</code> Python<pre><code>@abstractmethod\ndef save_version(self, version: PageVersion) -&gt; None:\n    \"\"\"Save a page version.\"\"\"\n    ...\n</code></pre>"},{"location":"api/storage/#ragcrawl.storage.backend.StorageBackend.get_version","title":"get_version  <code>abstractmethod</code>","text":"Python<pre><code>get_version(version_id: str) -&gt; PageVersion | None\n</code></pre> <p>Get a page version by ID.</p> Source code in <code>src/ragcrawl/storage/backend.py</code> Python<pre><code>@abstractmethod\ndef get_version(self, version_id: str) -&gt; PageVersion | None:\n    \"\"\"Get a page version by ID.\"\"\"\n    ...\n</code></pre>"},{"location":"api/storage/#ragcrawl.storage.backend.StorageBackend.get_current_version","title":"get_current_version  <code>abstractmethod</code>","text":"Python<pre><code>get_current_version(page_id: str) -&gt; PageVersion | None\n</code></pre> <p>Get the current version for a page.</p> Source code in <code>src/ragcrawl/storage/backend.py</code> Python<pre><code>@abstractmethod\ndef get_current_version(self, page_id: str) -&gt; PageVersion | None:\n    \"\"\"Get the current version for a page.\"\"\"\n    ...\n</code></pre>"},{"location":"api/storage/#ragcrawl.storage.backend.StorageBackend.list_versions","title":"list_versions  <code>abstractmethod</code>","text":"Python<pre><code>list_versions(\n    page_id: str, limit: int = 100\n) -&gt; list[PageVersion]\n</code></pre> <p>List versions for a page.</p> Source code in <code>src/ragcrawl/storage/backend.py</code> Python<pre><code>@abstractmethod\ndef list_versions(\n    self,\n    page_id: str,\n    limit: int = 100,\n) -&gt; list[PageVersion]:\n    \"\"\"List versions for a page.\"\"\"\n    ...\n</code></pre>"},{"location":"api/storage/#ragcrawl.storage.backend.StorageBackend.save_frontier_item","title":"save_frontier_item  <code>abstractmethod</code>","text":"Python<pre><code>save_frontier_item(item: FrontierItem) -&gt; None\n</code></pre> <p>Save a frontier item.</p> Source code in <code>src/ragcrawl/storage/backend.py</code> Python<pre><code>@abstractmethod\ndef save_frontier_item(self, item: FrontierItem) -&gt; None:\n    \"\"\"Save a frontier item.\"\"\"\n    ...\n</code></pre>"},{"location":"api/storage/#ragcrawl.storage.backend.StorageBackend.get_frontier_items","title":"get_frontier_items  <code>abstractmethod</code>","text":"Python<pre><code>get_frontier_items(\n    run_id: str,\n    status: str | None = None,\n    limit: int = 1000,\n) -&gt; list[FrontierItem]\n</code></pre> <p>Get frontier items for a run.</p> Source code in <code>src/ragcrawl/storage/backend.py</code> Python<pre><code>@abstractmethod\ndef get_frontier_items(\n    self,\n    run_id: str,\n    status: str | None = None,\n    limit: int = 1000,\n) -&gt; list[FrontierItem]:\n    \"\"\"Get frontier items for a run.\"\"\"\n    ...\n</code></pre>"},{"location":"api/storage/#ragcrawl.storage.backend.StorageBackend.update_frontier_status","title":"update_frontier_status  <code>abstractmethod</code>","text":"Python<pre><code>update_frontier_status(\n    item_id: str, status: str, error: str | None = None\n) -&gt; None\n</code></pre> <p>Update frontier item status.</p> Source code in <code>src/ragcrawl/storage/backend.py</code> Python<pre><code>@abstractmethod\ndef update_frontier_status(\n    self,\n    item_id: str,\n    status: str,\n    error: str | None = None,\n) -&gt; None:\n    \"\"\"Update frontier item status.\"\"\"\n    ...\n</code></pre>"},{"location":"api/storage/#ragcrawl.storage.backend.StorageBackend.clear_frontier","title":"clear_frontier  <code>abstractmethod</code>","text":"Python<pre><code>clear_frontier(run_id: str) -&gt; int\n</code></pre> <p>Clear all frontier items for a run. Returns count deleted.</p> Source code in <code>src/ragcrawl/storage/backend.py</code> Python<pre><code>@abstractmethod\ndef clear_frontier(self, run_id: str) -&gt; int:\n    \"\"\"Clear all frontier items for a run. Returns count deleted.\"\"\"\n    ...\n</code></pre>"},{"location":"api/storage/#ragcrawl.storage.backend.StorageBackend.save_pages_bulk","title":"save_pages_bulk  <code>abstractmethod</code>","text":"Python<pre><code>save_pages_bulk(pages: list[Page]) -&gt; int\n</code></pre> <p>Bulk save pages. Returns count saved.</p> Source code in <code>src/ragcrawl/storage/backend.py</code> Python<pre><code>@abstractmethod\ndef save_pages_bulk(self, pages: list[Page]) -&gt; int:\n    \"\"\"Bulk save pages. Returns count saved.\"\"\"\n    ...\n</code></pre>"},{"location":"api/storage/#ragcrawl.storage.backend.StorageBackend.save_versions_bulk","title":"save_versions_bulk  <code>abstractmethod</code>","text":"Python<pre><code>save_versions_bulk(versions: list[PageVersion]) -&gt; int\n</code></pre> <p>Bulk save versions. Returns count saved.</p> Source code in <code>src/ragcrawl/storage/backend.py</code> Python<pre><code>@abstractmethod\ndef save_versions_bulk(self, versions: list[PageVersion]) -&gt; int:\n    \"\"\"Bulk save versions. Returns count saved.\"\"\"\n    ...\n</code></pre>"},{"location":"api/storage/#ragcrawl.storage.backend.StorageBackend.initialize","title":"initialize  <code>abstractmethod</code>","text":"Python<pre><code>initialize() -&gt; None\n</code></pre> <p>Initialize the storage backend (create tables, etc.).</p> Source code in <code>src/ragcrawl/storage/backend.py</code> Python<pre><code>@abstractmethod\ndef initialize(self) -&gt; None:\n    \"\"\"Initialize the storage backend (create tables, etc.).\"\"\"\n    ...\n</code></pre>"},{"location":"api/storage/#ragcrawl.storage.backend.StorageBackend.close","title":"close  <code>abstractmethod</code>","text":"Python<pre><code>close() -&gt; None\n</code></pre> <p>Close any connections.</p> Source code in <code>src/ragcrawl/storage/backend.py</code> Python<pre><code>@abstractmethod\ndef close(self) -&gt; None:\n    \"\"\"Close any connections.\"\"\"\n    ...\n</code></pre>"},{"location":"api/storage/#ragcrawl.storage.backend.StorageBackend.health_check","title":"health_check  <code>abstractmethod</code>","text":"Python<pre><code>health_check() -&gt; bool\n</code></pre> <p>Check if the backend is healthy/available.</p> Source code in <code>src/ragcrawl/storage/backend.py</code> Python<pre><code>@abstractmethod\ndef health_check(self) -&gt; bool:\n    \"\"\"Check if the backend is healthy/available.\"\"\"\n    ...\n</code></pre>"},{"location":"api/storage/#ragcrawl.storage.backend.create_storage_backend","title":"create_storage_backend","text":"Python<pre><code>create_storage_backend(\n    config: StorageConfig,\n) -&gt; StorageBackend\n</code></pre> <p>Create a storage backend from configuration.</p> <p>Falls back to DuckDB if the configured backend is unavailable and fail_if_unavailable is False.</p> PARAMETER DESCRIPTION <code>config</code> <p>Storage configuration.</p> <p> TYPE: <code>StorageConfig</code> </p> RETURNS DESCRIPTION <code>StorageBackend</code> <p>A StorageBackend instance.</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If backend unavailable and fail_if_unavailable is True.</p> Source code in <code>src/ragcrawl/storage/backend.py</code> Python<pre><code>def create_storage_backend(config: StorageConfig) -&gt; StorageBackend:\n    \"\"\"\n    Create a storage backend from configuration.\n\n    Falls back to DuckDB if the configured backend is unavailable\n    and fail_if_unavailable is False.\n\n    Args:\n        config: Storage configuration.\n\n    Returns:\n        A StorageBackend instance.\n\n    Raises:\n        RuntimeError: If backend unavailable and fail_if_unavailable is True.\n    \"\"\"\n    if config.storage_type == StorageType.DYNAMODB:\n        try:\n            from ragcrawl.storage.dynamodb.backend import DynamoDBBackend\n\n            assert isinstance(config.backend, DynamoDBConfig)\n            backend = DynamoDBBackend(config.backend)\n\n            if backend.health_check():\n                logger.info(\"Using DynamoDB storage backend\")\n                return backend\n            else:\n                raise RuntimeError(\"DynamoDB health check failed\")\n\n        except Exception as e:\n            if config.fail_if_unavailable:\n                raise RuntimeError(f\"DynamoDB unavailable: {e}\") from e\n\n            logger.warning(\n                \"DynamoDB unavailable, falling back to DuckDB\",\n                error=str(e),\n            )\n\n    # Default to DuckDB\n    from ragcrawl.storage.duckdb.backend import DuckDBBackend\n\n    if isinstance(config.backend, DuckDBConfig):\n        db_config = config.backend\n    else:\n        # Fallback config\n        db_config = DuckDBConfig()\n\n    logger.info(\"Using DuckDB storage backend\", path=str(db_config.path))\n    return DuckDBBackend(db_config)\n</code></pre>"},{"location":"api/storage/duckdb/","title":"DuckDB Backend","text":"<p>The DuckDB backend provides local file-based storage using the DuckDB embedded database.</p>"},{"location":"api/storage/duckdb/#overview","title":"Overview","text":"<p>DuckDB is the default storage backend, ideal for:</p> <ul> <li>Local development and testing</li> <li>Single-machine deployments</li> <li>Small to medium crawls (up to millions of pages)</li> <li>Fast SQL queries on crawled data</li> </ul>"},{"location":"api/storage/duckdb/#configuration","title":"Configuration","text":"Python<pre><code>from ragcrawl.config.storage_config import StorageConfig, DuckDBConfig\n\nconfig = StorageConfig(\n    backend=DuckDBConfig(\n        path=\"./crawler.duckdb\",  # Database file path\n        read_only=False,          # Read-only mode\n    )\n)\n</code></pre>"},{"location":"api/storage/duckdb/#configuration-options","title":"Configuration Options","text":"Option Type Default Description <code>path</code> str <code>\"./crawler.duckdb\"</code> Database file path <code>read_only</code> bool <code>False</code> Open in read-only mode"},{"location":"api/storage/duckdb/#usage","title":"Usage","text":""},{"location":"api/storage/duckdb/#basic-usage","title":"Basic Usage","text":"Python<pre><code>from ragcrawl.storage import create_storage_backend\n\nbackend = create_storage_backend(config)\nbackend.initialize()\n\n# Use the backend\nsites = backend.list_sites()\n\n# Close when done\nbackend.close()\n</code></pre>"},{"location":"api/storage/duckdb/#with-context-manager","title":"With Context Manager","text":"Python<pre><code>with create_storage_backend(config) as backend:\n    backend.initialize()\n    sites = backend.list_sites()\n</code></pre>"},{"location":"api/storage/duckdb/#direct-sql-queries","title":"Direct SQL Queries","text":"<p>You can access the underlying DuckDB connection for custom queries:</p> Python<pre><code>from ragcrawl.storage.duckdb import DuckDBBackend\n\nbackend = DuckDBBackend(duckdb_config)\nbackend.initialize()\n\n# Run custom SQL\nresult = backend.conn.execute(\"\"\"\n    SELECT url, status_code, last_crawled\n    FROM pages\n    WHERE site_id = ?\n    ORDER BY last_crawled DESC\n    LIMIT 10\n\"\"\", [site_id]).fetchall()\n</code></pre>"},{"location":"api/storage/duckdb/#database-schema","title":"Database Schema","text":""},{"location":"api/storage/duckdb/#tables","title":"Tables","text":"Table Description <code>sites</code> Site configurations <code>crawl_runs</code> Crawl execution records <code>pages</code> Page state and metadata <code>page_versions</code> Content version history <code>frontier_items</code> URL queue items"},{"location":"api/storage/duckdb/#example-queries","title":"Example Queries","text":"<p>Find recently changed pages: SQL<pre><code>SELECT url, last_changed\nFROM pages\nWHERE site_id = 'site_abc123'\n  AND last_changed &gt; CURRENT_TIMESTAMP - INTERVAL 7 DAY\nORDER BY last_changed DESC;\n</code></pre></p> <p>Get crawl statistics: SQL<pre><code>SELECT\n    COUNT(*) as total_pages,\n    COUNT(CASE WHEN status_code = 200 THEN 1 END) as successful,\n    COUNT(CASE WHEN is_tombstone THEN 1 END) as deleted\nFROM pages\nWHERE site_id = 'site_abc123';\n</code></pre></p> <p>Find pages with errors: SQL<pre><code>SELECT url, status_code, last_error\nFROM pages\nWHERE site_id = 'site_abc123'\n  AND error_count &gt; 0\nORDER BY error_count DESC;\n</code></pre></p>"},{"location":"api/storage/duckdb/#performance-tips","title":"Performance Tips","text":""},{"location":"api/storage/duckdb/#indexing","title":"Indexing","text":"<p>The schema includes indexes for common queries. For custom queries, consider adding indexes:</p> SQL<pre><code>CREATE INDEX idx_pages_status ON pages(site_id, status_code);\n</code></pre>"},{"location":"api/storage/duckdb/#vacuuming","title":"Vacuuming","text":"<p>Periodically vacuum the database:</p> Python<pre><code>backend.conn.execute(\"VACUUM\")\n</code></pre>"},{"location":"api/storage/duckdb/#memory-settings","title":"Memory Settings","text":"<p>For large crawls, increase memory limit:</p> Python<pre><code>backend.conn.execute(\"SET memory_limit='4GB'\")\n</code></pre>"},{"location":"api/storage/duckdb/#api-reference","title":"API Reference","text":""},{"location":"api/storage/duckdb/#ragcrawl.storage.duckdb.backend.DuckDBBackend","title":"DuckDBBackend","text":"Python<pre><code>DuckDBBackend(config: DuckDBConfig)\n</code></pre> <p>               Bases: <code>StorageBackend</code></p> <p>DuckDB storage backend implementation.</p> <p>Provides local file-based storage with SQL capabilities.</p> <p>Initialize DuckDB backend.</p> PARAMETER DESCRIPTION <code>config</code> <p>DuckDB configuration.</p> <p> TYPE: <code>DuckDBConfig</code> </p> Source code in <code>src/ragcrawl/storage/duckdb/backend.py</code> Python<pre><code>def __init__(self, config: DuckDBConfig) -&gt; None:\n    \"\"\"\n    Initialize DuckDB backend.\n\n    Args:\n        config: DuckDB configuration.\n    \"\"\"\n    self.config = config\n    self.db_path = Path(config.path)\n    self._conn: duckdb.DuckDBPyConnection | None = None\n</code></pre>"},{"location":"api/storage/duckdb/#ragcrawl.storage.duckdb.backend.DuckDBBackend.initialize","title":"initialize","text":"Python<pre><code>initialize() -&gt; None\n</code></pre> <p>Initialize the database schema.</p> Source code in <code>src/ragcrawl/storage/duckdb/backend.py</code> Python<pre><code>def initialize(self) -&gt; None:\n    \"\"\"Initialize the database schema.\"\"\"\n    for schema_sql in get_all_schemas():\n        self.conn.execute(schema_sql)\n</code></pre>"},{"location":"api/storage/duckdb/#ragcrawl.storage.duckdb.backend.DuckDBBackend.close","title":"close","text":"Python<pre><code>close() -&gt; None\n</code></pre> <p>Close the database connection.</p> Source code in <code>src/ragcrawl/storage/duckdb/backend.py</code> Python<pre><code>def close(self) -&gt; None:\n    \"\"\"Close the database connection.\"\"\"\n    if self._conn is not None:\n        self._conn.close()\n        self._conn = None\n</code></pre>"},{"location":"api/storage/duckdb/#ragcrawl.storage.duckdb.backend.DuckDBBackend.health_check","title":"health_check","text":"Python<pre><code>health_check() -&gt; bool\n</code></pre> <p>Check if the database is accessible.</p> Source code in <code>src/ragcrawl/storage/duckdb/backend.py</code> Python<pre><code>def health_check(self) -&gt; bool:\n    \"\"\"Check if the database is accessible.\"\"\"\n    try:\n        self.conn.execute(\"SELECT 1\").fetchone()\n        return True\n    except Exception:\n        return False\n</code></pre>"},{"location":"api/storage/dynamodb/","title":"DynamoDB Backend","text":"<p>The DynamoDB backend provides scalable cloud storage using AWS DynamoDB.</p>"},{"location":"api/storage/dynamodb/#overview","title":"Overview","text":"<p>DynamoDB is ideal for:</p> <ul> <li>Cloud-native deployments</li> <li>Distributed crawling systems</li> <li>High availability requirements</li> <li>Serverless architectures</li> </ul>"},{"location":"api/storage/dynamodb/#installation","title":"Installation","text":"<p>Install with DynamoDB support:</p> Bash<pre><code>pip install ragcrawl[dynamodb]\n</code></pre>"},{"location":"api/storage/dynamodb/#configuration","title":"Configuration","text":"Python<pre><code>from ragcrawl.config.storage_config import StorageConfig, DynamoDBConfig\n\nconfig = StorageConfig(\n    backend=DynamoDBConfig(\n        table_prefix=\"ragcrawl_\",\n        region=\"us-west-2\",\n        endpoint_url=None,  # Use for local DynamoDB\n    )\n)\n</code></pre>"},{"location":"api/storage/dynamodb/#configuration-options","title":"Configuration Options","text":"Option Type Default Description <code>table_prefix</code> str <code>\"ragcrawl_\"</code> Prefix for table names <code>region</code> str <code>\"us-east-1\"</code> AWS region <code>endpoint_url</code> str <code>None</code> Custom endpoint (for local)"},{"location":"api/storage/dynamodb/#aws-credentials","title":"AWS Credentials","text":"<p>The backend uses standard AWS credential resolution:</p> <ol> <li>Environment variables (<code>AWS_ACCESS_KEY_ID</code>, <code>AWS_SECRET_ACCESS_KEY</code>)</li> <li>AWS credentials file (<code>~/.aws/credentials</code>)</li> <li>IAM role (for EC2/Lambda)</li> </ol>"},{"location":"api/storage/dynamodb/#usage","title":"Usage","text":""},{"location":"api/storage/dynamodb/#basic-usage","title":"Basic Usage","text":"Python<pre><code>from ragcrawl.storage import create_storage_backend\n\nbackend = create_storage_backend(config)\nbackend.initialize()  # Creates tables if needed\n\nsites = backend.list_sites()\nbackend.close()\n</code></pre>"},{"location":"api/storage/dynamodb/#local-development","title":"Local Development","text":"<p>Use DynamoDB Local for development:</p> Bash<pre><code># Start DynamoDB Local\ndocker run -p 8000:8000 amazon/dynamodb-local\n</code></pre> Python<pre><code>config = StorageConfig(\n    backend=DynamoDBConfig(\n        table_prefix=\"dev_\",\n        region=\"us-east-1\",\n        endpoint_url=\"http://localhost:8000\",\n    )\n)\n</code></pre>"},{"location":"api/storage/dynamodb/#table-structure","title":"Table Structure","text":""},{"location":"api/storage/dynamodb/#tables-created","title":"Tables Created","text":"Table Partition Key Sort Key Description <code>{prefix}sites</code> <code>site_id</code> - Site records <code>{prefix}runs</code> <code>site_id</code> <code>run_id</code> Crawl runs <code>{prefix}pages</code> <code>site_id</code> <code>page_id</code> Pages <code>{prefix}versions</code> <code>page_id</code> <code>version_id</code> Content versions <code>{prefix}frontier</code> <code>run_id</code> <code>item_id</code> Queue items"},{"location":"api/storage/dynamodb/#global-secondary-indexes","title":"Global Secondary Indexes","text":"<ul> <li><code>runs</code>: GSI on <code>run_id</code> for direct lookup</li> <li><code>pages</code>: GSI on <code>url</code> for URL lookups</li> </ul>"},{"location":"api/storage/dynamodb/#iam-permissions","title":"IAM Permissions","text":"<p>Required IAM permissions:</p> JSON<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"dynamodb:CreateTable\",\n                \"dynamodb:DescribeTable\",\n                \"dynamodb:GetItem\",\n                \"dynamodb:PutItem\",\n                \"dynamodb:UpdateItem\",\n                \"dynamodb:DeleteItem\",\n                \"dynamodb:Query\",\n                \"dynamodb:Scan\"\n            ],\n            \"Resource\": \"arn:aws:dynamodb:*:*:table/ragcrawl_*\"\n        }\n    ]\n}\n</code></pre>"},{"location":"api/storage/dynamodb/#cost-optimization","title":"Cost Optimization","text":""},{"location":"api/storage/dynamodb/#on-demand-vs-provisioned","title":"On-Demand vs Provisioned","text":"<p>By default, tables use on-demand capacity. For predictable workloads, consider provisioned capacity:</p> Python<pre><code># Set via AWS Console or CLI after table creation\naws dynamodb update-table \\\n    --table-name ragcrawl_pages \\\n    --billing-mode PROVISIONED \\\n    --provisioned-throughput ReadCapacityUnits=100,WriteCapacityUnits=50\n</code></pre>"},{"location":"api/storage/dynamodb/#ttl-for-frontier-items","title":"TTL for Frontier Items","text":"<p>Enable TTL on frontier table to auto-delete old items:</p> Python<pre><code># Frontier items can be cleaned up after run completion\n</code></pre>"},{"location":"api/storage/dynamodb/#api-reference","title":"API Reference","text":""},{"location":"api/storage/dynamodb/#ragcrawl.storage.dynamodb.backend.DynamoDBBackend","title":"DynamoDBBackend","text":"Python<pre><code>DynamoDBBackend(config: DynamoDBConfig)\n</code></pre> <p>               Bases: <code>StorageBackend</code></p> <p>DynamoDB storage backend implementation using PynamoDB.</p> <p>Initialize DynamoDB backend.</p> PARAMETER DESCRIPTION <code>config</code> <p>DynamoDB configuration.</p> <p> TYPE: <code>DynamoDBConfig</code> </p> Source code in <code>src/ragcrawl/storage/dynamodb/backend.py</code> Python<pre><code>def __init__(self, config: DynamoDBConfig) -&gt; None:\n    \"\"\"\n    Initialize DynamoDB backend.\n\n    Args:\n        config: DynamoDB configuration.\n    \"\"\"\n    self.config = config\n    self._configure_models()\n</code></pre>"},{"location":"api/storage/dynamodb/#ragcrawl.storage.dynamodb.backend.DynamoDBBackend.initialize","title":"initialize","text":"Python<pre><code>initialize() -&gt; None\n</code></pre> <p>Create tables if they don't exist.</p> Source code in <code>src/ragcrawl/storage/dynamodb/backend.py</code> Python<pre><code>def initialize(self) -&gt; None:\n    \"\"\"Create tables if they don't exist.\"\"\"\n    for model_class in [\n        SiteModel,\n        CrawlRunModel,\n        PageModel,\n        PageVersionModel,\n        FrontierItemModel,\n    ]:\n        if not model_class.exists():\n            model_class.create_table(\n                read_capacity_units=self.config.read_capacity_units,\n                write_capacity_units=self.config.write_capacity_units,\n                wait=True,\n            )\n            logger.info(f\"Created table: {model_class.Meta.table_name}\")\n</code></pre>"},{"location":"api/storage/dynamodb/#ragcrawl.storage.dynamodb.backend.DynamoDBBackend.close","title":"close","text":"Python<pre><code>close() -&gt; None\n</code></pre> <p>Close connections (no-op for DynamoDB).</p> Source code in <code>src/ragcrawl/storage/dynamodb/backend.py</code> Python<pre><code>def close(self) -&gt; None:\n    \"\"\"Close connections (no-op for DynamoDB).\"\"\"\n    pass\n</code></pre>"},{"location":"api/storage/dynamodb/#ragcrawl.storage.dynamodb.backend.DynamoDBBackend.health_check","title":"health_check","text":"Python<pre><code>health_check() -&gt; bool\n</code></pre> <p>Check if DynamoDB is accessible.</p> Source code in <code>src/ragcrawl/storage/dynamodb/backend.py</code> Python<pre><code>def health_check(self) -&gt; bool:\n    \"\"\"Check if DynamoDB is accessible.\"\"\"\n    try:\n        # Try to describe one table\n        SiteModel.exists()\n        return True\n    except Exception as e:\n        logger.error(\"DynamoDB health check failed\", error=str(e))\n        return False\n</code></pre>"},{"location":"cli/","title":"CLI Reference","text":"<p>ragcrawl provides a powerful command-line interface for all crawling operations.</p>"},{"location":"cli/#installation","title":"Installation","text":"<p>The CLI is included with the ragcrawl package:</p> Bash<pre><code>pip install ragcrawl\nragcrawl --help\n</code></pre>"},{"location":"cli/#commands-overview","title":"Commands Overview","text":"Command Description <code>crawl</code> Crawl a website from seed URLs <code>sync</code> Incrementally sync an existing site <code>sites</code> List all crawled sites <code>runs</code> List crawl runs for a site <code>list</code> List pages for a site <code>config</code> Manage configuration"},{"location":"cli/#crawl","title":"crawl","text":"<p>Start a new crawl from one or more seed URLs.</p>"},{"location":"cli/#usage","title":"Usage","text":"Bash<pre><code>ragcrawl crawl [OPTIONS] SEEDS...\n</code></pre>"},{"location":"cli/#arguments","title":"Arguments","text":"Argument Description <code>SEEDS</code> One or more seed URLs to start crawling"},{"location":"cli/#options","title":"Options","text":"Option Type Default Description <code>--max-pages</code> int 100 Maximum pages to crawl <code>--max-depth</code> int 10 Maximum link depth <code>--delay</code> float 1.0 Delay between requests (seconds) <code>--include</code> str - URL pattern to include (can repeat) <code>--exclude</code> str - URL pattern to exclude (can repeat) <code>--output</code> path ./output Output directory <code>--format</code> choice multi Output format: single, multi <code>--db</code> path crawler.duckdb Database file path <code>--site-name</code> str auto Custom name for the site <code>--user-agent</code> str ragcrawl Custom user agent <code>--robots</code> choice strict Robots mode: strict, off <code>--browser</code> flag false Enable browser rendering <code>--verbose</code> flag false Verbose output"},{"location":"cli/#examples","title":"Examples","text":"<p>Basic crawl: Bash<pre><code>ragcrawl crawl https://docs.example.com\n</code></pre></p> <p>With limits and filters: Bash<pre><code>ragcrawl crawl https://docs.example.com \\\n    --max-pages 500 \\\n    --max-depth 5 \\\n    --include \"/docs/*\" \\\n    --exclude \"/api/internal/*\"\n</code></pre></p> <p>Custom output: Bash<pre><code>ragcrawl crawl https://docs.example.com \\\n    --output ./knowledge-base \\\n    --format single\n</code></pre></p> <p>Browser rendering for JavaScript sites: Bash<pre><code>ragcrawl crawl https://spa.example.com --browser\n</code></pre></p>"},{"location":"cli/#sync","title":"sync","text":"<p>Incrementally update a previously crawled site.</p>"},{"location":"cli/#usage_1","title":"Usage","text":"Bash<pre><code>ragcrawl sync [OPTIONS] SITE_ID\n</code></pre>"},{"location":"cli/#arguments_1","title":"Arguments","text":"Argument Description <code>SITE_ID</code> Site ID to sync (from <code>ragcrawl sites</code>)"},{"location":"cli/#options_1","title":"Options","text":"Option Type Default Description <code>--max-pages</code> int unlimited Maximum pages to sync <code>--max-age</code> float - Only sync pages older than N hours <code>--sitemap</code> flag true Use sitemap for discovery <code>--conditional</code> flag true Use conditional requests <code>--output</code> path ./output Output directory <code>--db</code> path crawler.duckdb Database file path"},{"location":"cli/#examples_1","title":"Examples","text":"<p>Basic sync: Bash<pre><code>ragcrawl sync site_abc123\n</code></pre></p> <p>Sync with limits: Bash<pre><code>ragcrawl sync site_abc123 --max-pages 100 --max-age 24\n</code></pre></p>"},{"location":"cli/#sites","title":"sites","text":"<p>List all crawled sites in the database.</p>"},{"location":"cli/#usage_2","title":"Usage","text":"Bash<pre><code>ragcrawl sites [OPTIONS]\n</code></pre>"},{"location":"cli/#options_2","title":"Options","text":"Option Type Default Description <code>--db</code> path crawler.duckdb Database file path <code>--json</code> flag false Output as JSON"},{"location":"cli/#example-output","title":"Example Output","text":"Text Only<pre><code>ID              Name                 Seeds                    Pages  Last Crawl\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nsite_abc123     Example Docs         https://docs.example.com    150  2024-01-15\nsite_def456     API Reference        https://api.example.com      75  2024-01-14\n</code></pre>"},{"location":"cli/#runs","title":"runs","text":"<p>List crawl runs for a specific site.</p>"},{"location":"cli/#usage_3","title":"Usage","text":"Bash<pre><code>ragcrawl runs [OPTIONS] SITE_ID\n</code></pre>"},{"location":"cli/#arguments_2","title":"Arguments","text":"Argument Description <code>SITE_ID</code> Site ID to list runs for"},{"location":"cli/#options_3","title":"Options","text":"Option Type Default Description <code>--limit</code> int 10 Number of runs to show <code>--db</code> path crawler.duckdb Database file path <code>--json</code> flag false Output as JSON"},{"location":"cli/#example-output_1","title":"Example Output","text":"Text Only<pre><code>Run ID          Status     Started              Pages  Duration\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nrun_xyz789      completed  2024-01-15 10:30:00    150  5m 23s\nrun_xyz788      completed  2024-01-14 09:15:00    148  5m 10s\nrun_xyz787      failed     2024-01-13 08:00:00     45  2m 15s\n</code></pre>"},{"location":"cli/#list","title":"list","text":"<p>List pages for a specific site.</p>"},{"location":"cli/#usage_4","title":"Usage","text":"Bash<pre><code>ragcrawl list [OPTIONS] SITE_ID\n</code></pre>"},{"location":"cli/#arguments_3","title":"Arguments","text":"Argument Description <code>SITE_ID</code> Site ID to list pages for"},{"location":"cli/#options_4","title":"Options","text":"Option Type Default Description <code>--limit</code> int 100 Number of pages to show <code>--status</code> int - Filter by HTTP status <code>--db</code> path crawler.duckdb Database file path <code>--json</code> flag false Output as JSON"},{"location":"cli/#config","title":"config","text":"<p>Manage ragcrawl configuration.</p>"},{"location":"cli/#subcommands","title":"Subcommands","text":"Subcommand Description <code>show</code> Show current configuration <code>set</code> Set a configuration value <code>reset</code> Reset to defaults <code>path</code> Show config file path"},{"location":"cli/#examples_2","title":"Examples","text":"<p>Show config: Bash<pre><code>ragcrawl config show\n</code></pre></p> <p>Set default database: Bash<pre><code>ragcrawl config set db_path ./my-crawler.duckdb\n</code></pre></p>"},{"location":"cli/#global-options","title":"Global Options","text":"<p>These options are available for all commands:</p> Option Description <code>--help</code> Show help message <code>--version</code> Show version <code>--verbose</code> / <code>-v</code> Increase verbosity <code>--quiet</code> / <code>-q</code> Suppress output"},{"location":"cli/#exit-codes","title":"Exit Codes","text":"Code Meaning 0 Success 1 General error 2 Configuration error 3 Network error 4 Storage error"},{"location":"cli/#environment-variables","title":"Environment Variables","text":"Variable Description <code>RAGCRAWL_DB_PATH</code> Default database path <code>RAGCRAWL_OUTPUT_DIR</code> Default output directory <code>RAGCRAWL_LOG_LEVEL</code> Logging level (DEBUG, INFO, WARNING, ERROR)"},{"location":"cli/#see-also","title":"See Also","text":"<ul> <li>Configuration - Full configuration reference</li> <li>User Guide - Detailed usage guides</li> <li>API Reference - Python API documentation</li> </ul>"},{"location":"community/","title":"Community","text":"<p>Welcome to the ragcrawl community! We're excited to have you here.</p>"},{"location":"community/#get-involved","title":"Get Involved","text":"<p>There are many ways to contribute to ragcrawl:</p> <ul> <li>Report bugs - Help us improve by reporting issues</li> <li>Request features - Share your ideas for new functionality</li> <li>Submit code - Contribute bug fixes and new features</li> <li>Improve documentation - Help others learn and use ragcrawl</li> <li>Share your experience - Write blog posts, tutorials, or examples</li> </ul>"},{"location":"community/#community-resources","title":"Community Resources","text":"<ul> <li> <p> Contributing Guide</p> <p>Learn how to contribute to ragcrawl</p> </li> <li> <p> Code of Conduct</p> <p>Our community standards</p> </li> <li> <p> Support</p> <p>Get help and report issues</p> </li> <li> <p> Changelog</p> <p>Release history</p> </li> </ul>"},{"location":"community/#communication","title":"Communication","text":"<ul> <li>GitHub Issues - For bug reports and feature requests</li> <li>GitHub Discussions - For questions and community discussions (if enabled)</li> <li>Pull Requests - For code contributions</li> </ul>"},{"location":"community/#recognition","title":"Recognition","text":"<p>We appreciate all contributions, big and small. Contributors are recognized in our release notes and changelog.</p>"},{"location":"community/#license","title":"License","text":"<p>All contributions to ragcrawl are licensed under the Apache License 2.0.</p>"},{"location":"community/changelog/","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project follows Semantic Versioning.</p>"},{"location":"community/changelog/#unreleased","title":"Unreleased","text":""},{"location":"community/changelog/#added","title":"Added","text":"<ul> <li>Community documentation and governance files</li> <li>Comprehensive test suite for project metadata validation</li> <li>Enhanced documentation with community guidelines</li> </ul>"},{"location":"community/changelog/#001-2025-11-26","title":"0.0.1 - 2025-11-26","text":""},{"location":"community/changelog/#added_1","title":"Added","text":"<ul> <li>Project scaffolding and initial MVP baseline for ragcrawl</li> <li>Initial architecture for recursive crawling, KB/RAG-ready Markdown generation, chunking, and exporters</li> <li>Pluggable storage backends: DuckDB (default) + optional DynamoDB via PynamoDB</li> <li>Incremental sync using conditional requests (ETag/Last-Modified), sitemap prioritization, and content-hash diffs</li> <li>Markdown publishing formats: single-page output and multi-page output with folder structure and link rewriting</li> <li>Centralized DuckDB storage defaulting to <code>~/.ragcrawl/</code> and CLI commands for config/listing</li> <li>Apache License 2.0 with proper SPDX identifier</li> <li>Community governance files:</li> <li>CODE_OF_CONDUCT.md (Contributor Covenant)</li> <li>CONTRIBUTING.md (Development setup and guidelines)</li> <li>SUPPORT.md (Getting help and reporting issues)</li> <li>Issue templates for bug reports and feature requests</li> <li>Comprehensive documentation site with MkDocs Material theme</li> </ul>"},{"location":"community/changelog/#changed","title":"Changed","text":"<ul> <li>N/A (initial release)</li> </ul>"},{"location":"community/changelog/#fixed","title":"Fixed","text":"<ul> <li>N/A (initial release)</li> </ul>"},{"location":"community/changelog/#security","title":"Security","text":"<ul> <li>N/A (initial release)</li> </ul>"},{"location":"community/changelog/#release-notes","title":"Release Notes","text":""},{"location":"community/changelog/#version-001-initial-release","title":"Version 0.0.1 - Initial Release","text":"<p>This is the first public release of ragcrawl, a recursive website crawler designed to produce LLM-ready knowledge base artifacts.</p> <p>Key Features: - Recursive crawling with pattern-based filtering - Clean Markdown extraction optimized for RAG pipelines - Incremental sync with change detection - Pluggable storage (DuckDB and DynamoDB) - Built-in chunking strategies - CLI and Python API</p> <p>Getting Started: Bash<pre><code>pip install ragcrawl\nragcrawl crawl https://docs.example.com --max-pages 100\n</code></pre></p> <p>See the documentation for more details.</p>"},{"location":"community/code-of-conduct/","title":"Code of Conduct","text":"<p>This project follows the Contributor Covenant Code of Conduct.</p>"},{"location":"community/code-of-conduct/#our-pledge","title":"Our Pledge","text":"<p>We pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.</p>"},{"location":"community/code-of-conduct/#our-standards","title":"Our Standards","text":""},{"location":"community/code-of-conduct/#examples-of-behavior-that-contributes-to-a-positive-environment","title":"Examples of behavior that contributes to a positive environment:","text":"<ul> <li>Being respectful and considerate</li> <li>Welcoming different viewpoints and experiences</li> <li>Giving and gracefully accepting constructive feedback</li> <li>Focusing on what is best for the community</li> <li>Showing empathy towards other community members</li> </ul>"},{"location":"community/code-of-conduct/#examples-of-unacceptable-behavior","title":"Examples of unacceptable behavior:","text":"<ul> <li>Harassment, discrimination, or personal attacks</li> <li>Trolling, insults, or derogatory comments</li> <li>Publishing someone's private information without explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a professional setting</li> </ul>"},{"location":"community/code-of-conduct/#enforcement-responsibilities","title":"Enforcement Responsibilities","text":"<p>Project maintainers are responsible for clarifying and enforcing standards of acceptable behavior and may take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.</p>"},{"location":"community/code-of-conduct/#scope","title":"Scope","text":"<p>This Code of Conduct applies within all project spaces, including: - GitHub repositories - Issue trackers - Pull requests - Discussions - Social media channels</p> <p>It also applies when an individual is representing the project in public spaces.</p>"},{"location":"community/code-of-conduct/#enforcement","title":"Enforcement","text":"<p>If you experience or witness unacceptable behavior, please report it via the method described in our Support page. All complaints will be reviewed and investigated promptly and fairly.</p> <p>Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.</p>"},{"location":"community/code-of-conduct/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 2.1, available at: https://www.contributor-covenant.org/version/2/1/code_of_conduct.html</p> <p>For answers to common questions about this code of conduct, see: https://www.contributor-covenant.org/faq</p>"},{"location":"community/contributing/","title":"Contributing to ragcrawl","text":"<p>Thanks for taking the time to contribute! \ud83c\udf89</p>"},{"location":"community/contributing/#code-of-conduct","title":"Code of Conduct","text":"<p>By participating in this project, you agree to follow the rules in our Code of Conduct.</p>"},{"location":"community/contributing/#how-to-contribute","title":"How to Contribute","text":""},{"location":"community/contributing/#report-bugs","title":"Report Bugs","text":"<p>Open an Issue with: - Reproduction steps - Logs (redact any secrets) - Environment details (OS, Python version, ragcrawl version)</p>"},{"location":"community/contributing/#request-features","title":"Request Features","text":"<p>Open an Issue describing: - The use case and desired behavior - Any constraints (scale, compliance, auth, JS rendering, etc.)</p>"},{"location":"community/contributing/#submit-pull-requests","title":"Submit Pull Requests","text":"<ol> <li>Fork the repository</li> <li>Create a feature branch</li> <li>Make your changes</li> <li>Add/update tests</li> <li>Update documentation</li> <li>Open a Pull Request</li> </ol>"},{"location":"community/contributing/#development-setup","title":"Development Setup","text":""},{"location":"community/contributing/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10+ (3.11 or 3.12 recommended)</li> <li><code>uv</code> recommended (but <code>pip</code> works too)</li> </ul>"},{"location":"community/contributing/#setup-with-uv-recommended","title":"Setup with uv (recommended)","text":"Bash<pre><code>git clone https://github.com/vamshirapolu/ragcrawl.git\ncd ragcrawl\nuv venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\nuv pip install -e \".[dev]\"\n</code></pre>"},{"location":"community/contributing/#setup-with-pip","title":"Setup with pip","text":"Bash<pre><code>git clone https://github.com/vamshirapolu/ragcrawl.git\ncd ragcrawl\npython -m venv .venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\npip install -U pip\npip install -e \".[dev]\"\n</code></pre>"},{"location":"community/contributing/#running-the-project","title":"Running the Project","text":"Bash<pre><code># Show CLI help\nragcrawl --help\n\n# Run a simple crawl\nragcrawl crawl https://example.com --max-pages 10\n</code></pre>"},{"location":"community/contributing/#running-tests","title":"Running Tests","text":"Bash<pre><code># Run all tests\npytest\n\n# Run with coverage\npytest --cov=ragcrawl --cov-report=html\n\n# Run specific test file\npytest tests/unit/test_url_normalizer.py\n\n# Run with verbose output\npytest -v\n</code></pre>"},{"location":"community/contributing/#linting-and-formatting","title":"Linting and Formatting","text":"<p>We use Ruff for linting and formatting:</p> Bash<pre><code># Check code\nruff check .\n\n# Format code\nruff format .\n\n# Run both\nruff check . &amp;&amp; ruff format .\n</code></pre>"},{"location":"community/contributing/#type-checking","title":"Type Checking","text":"<p>We use mypy for type checking:</p> Bash<pre><code>mypy src/ragcrawl\n</code></pre>"},{"location":"community/contributing/#documentation","title":"Documentation","text":"<p>We use MkDocs with Material theme:</p> Bash<pre><code># Serve docs locally\nmkdocs serve\n\n# Build docs\nmkdocs build\n\n# Deploy docs (maintainers only)\nmkdocs gh-deploy\n</code></pre>"},{"location":"community/contributing/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<ul> <li>Keep PRs focused - One feature/fix per PR when possible</li> <li>Add/adjust tests - When behavior changes</li> <li>Update docs - If you add/modify public-facing behavior</li> <li>Backwards compatibility - Keep it for public APIs, or clearly call out breaking changes</li> <li>Write clear commit messages - See guidance below</li> </ul>"},{"location":"community/contributing/#commit-message-guidance","title":"Commit Message Guidance","text":"<p>We follow conventional commits (suggested):</p> <ul> <li><code>feat: ...</code> - New feature</li> <li><code>fix: ...</code> - Bug fix</li> <li><code>docs: ...</code> - Documentation only</li> <li><code>test: ...</code> - Adding or updating tests</li> <li><code>refactor: ...</code> - Code refactoring</li> <li><code>chore: ...</code> - Build/tooling changes</li> </ul> <p>Examples: Text Only<pre><code>feat: add support for custom user agents\nfix: handle 404 errors in sync job\ndocs: update installation instructions\ntest: add tests for URL normalization\n</code></pre></p>"},{"location":"community/contributing/#code-style","title":"Code Style","text":"<ul> <li>Follow PEP 8 (enforced by Ruff)</li> <li>Use type hints for all functions</li> <li>Write docstrings in Google style</li> <li>Keep functions focused and testable</li> <li>Prefer composition over inheritance</li> </ul>"},{"location":"community/contributing/#testing-guidelines","title":"Testing Guidelines","text":"<ul> <li>Write unit tests for new functionality</li> <li>Use pytest fixtures for common setup</li> <li>Mock external dependencies (HTTP, file system, etc.)</li> <li>Aim for high coverage, but focus on critical paths</li> <li>Add integration tests for end-to-end scenarios</li> </ul>"},{"location":"community/contributing/#security","title":"Security","text":"<p>Please do not open public issues for security vulnerabilities. See Support for the preferred reporting path.</p>"},{"location":"community/contributing/#questions","title":"Questions?","text":"<p>If you have questions about contributing, feel free to: - Open a GitHub Discussion (if enabled) - Ask in an Issue - Check our Support page</p> <p>Thank you for contributing to ragcrawl! \ud83d\ude80</p>"},{"location":"community/support/","title":"Support","text":""},{"location":"community/support/#getting-help","title":"Getting Help","text":""},{"location":"community/support/#questions-and-how-to","title":"Questions and How-To","text":"<ul> <li>GitHub Issues - Open an issue for questions</li> <li>GitHub Discussions - Use discussions for community Q&amp;A (if enabled)</li> <li>Documentation - Check our comprehensive documentation</li> </ul>"},{"location":"community/support/#bug-reports","title":"Bug Reports","text":"<p>When reporting a bug, please include:</p> <ul> <li>OS and Python version - e.g., \"macOS 14.0, Python 3.11.5\"</li> <li>ragcrawl version - Run <code>ragcrawl --version</code></li> <li>Installation method - pip, uv, or from source</li> <li>Command/config used - The exact command or configuration</li> <li>Logs - Include relevant logs (redact any secrets/tokens)</li> <li>Minimal reproduction steps - Steps to reproduce the issue</li> </ul> <p>Example: Text Only<pre><code>OS: Ubuntu 22.04\nPython: 3.11.5\nragcrawl: 0.0.1\nInstall: pip install ragcrawl\n\nCommand:\nragcrawl crawl https://example.com --max-pages 100\n\nError:\n[paste error message here]\n</code></pre></p>"},{"location":"community/support/#feature-requests","title":"Feature Requests","text":"<p>When requesting a feature, please describe:</p> <ul> <li>User problem / use case - What are you trying to accomplish?</li> <li>Expected behavior - What should happen?</li> <li>Constraints - Any specific requirements (scale, compliance, auth, JS rendering, etc.)</li> <li>Alternatives considered - What workarounds have you tried?</li> </ul>"},{"location":"community/support/#security-issues","title":"Security Issues","text":"<p>Please do not open a public issue for security vulnerabilities.</p> <p>Instead: 1. Use GitHub's private vulnerability reporting (if enabled) 2. Or contact the maintainer privately via their GitHub profile</p> <p>We will respond as quickly as possible and work with you to address the issue.</p>"},{"location":"community/support/#compatibility-notes","title":"Compatibility Notes","text":"<p>Crawling behavior depends heavily on target websites:</p> <ul> <li>Respect robots.txt - Honor site crawling policies</li> <li>Respect site terms - Follow the target site's terms of service</li> <li>Be mindful of rate limits - Don't overwhelm servers</li> <li>Handle authentication properly - Use appropriate auth methods</li> <li>Prefer efficient strategies - Use ETag/Last-Modified, sitemaps, conditional GET</li> </ul>"},{"location":"community/support/#common-issues","title":"Common Issues","text":""},{"location":"community/support/#installation-problems","title":"Installation Problems","text":"<p>Issue: <code>pip install ragcrawl</code> fails</p> <p>Solution:  - Ensure Python 3.10+ is installed - Try upgrading pip: <code>pip install --upgrade pip</code> - Use a virtual environment</p>"},{"location":"community/support/#browser-mode-issues","title":"Browser Mode Issues","text":"<p>Issue: Browser mode not working</p> <p>Solution: - Install browser dependencies: <code>pip install ragcrawl[browser]</code> - Install Playwright browsers: <code>playwright install</code></p>"},{"location":"community/support/#duckdb-permissions","title":"DuckDB Permissions","text":"<p>Issue: Permission denied for DuckDB file</p> <p>Solution: - Check file permissions on <code>~/.ragcrawl/</code> - Specify a custom path: <code>--storage /path/to/crawler.duckdb</code></p>"},{"location":"community/support/#rate-limiting","title":"Rate Limiting","text":"<p>Issue: Getting rate limited or blocked</p> <p>Solution: - Reduce concurrency: <code>--max-concurrency 2</code> - Add delays between requests - Check and respect robots.txt - Use a custom user agent</p>"},{"location":"community/support/#response-times","title":"Response Times","text":"<ul> <li>Bug reports: We aim to respond within 2-3 business days</li> <li>Feature requests: We review and prioritize quarterly</li> <li>Security issues: We respond within 24 hours</li> </ul>"},{"location":"community/support/#contributing","title":"Contributing","text":"<p>If you'd like to help improve ragcrawl, see our Contributing Guide.</p>"},{"location":"community/support/#community","title":"Community","text":"<p>Join our community: - Star the GitHub repository - Follow development updates - Share your use cases and feedback</p>"},{"location":"configuration/","title":"Configuration","text":"<p>ragcrawl is highly configurable to handle various crawling scenarios. This section covers all configuration options.</p>"},{"location":"configuration/#configuration-overview","title":"Configuration Overview","text":"<p>ragcrawl uses four main configuration classes:</p> Config Class Purpose Documentation <code>CrawlerConfig</code> Controls crawl behavior Crawler Options <code>StorageConfig</code> Database backend settings Storage Backends <code>OutputConfig</code> Output format and files Output Settings <code>MarkdownConfig</code> Markdown extraction and filtering Markdown Extraction"},{"location":"configuration/#quick-configuration-example","title":"Quick Configuration Example","text":"Python<pre><code>from ragcrawl.config import CrawlerConfig, StorageConfig, OutputConfig\nfrom ragcrawl.config.storage_config import DuckDBConfig\n\nconfig = CrawlerConfig(\n    # What to crawl\n    seeds=[\"https://docs.example.com\"],\n    allowed_domains=[\"docs.example.com\"],\n\n    # Crawl limits\n    max_pages=100,\n    max_depth=5,\n\n    # URL filtering\n    include_patterns=[\"/docs/*\", \"/api/*\"],\n    exclude_patterns=[\"/blog/*\", \"*/print/*\"],\n\n    # Politeness\n    delay_seconds=1.0,\n    robots_mode=\"strict\",\n\n    # Storage\n    storage=StorageConfig(\n        backend=DuckDBConfig(path=\"./crawler.duckdb\")\n    ),\n\n    # Output\n    output=OutputConfig(\n        mode=\"multi\",\n        root_dir=\"./output\",\n        include_metadata=True,\n    ),\n)\n</code></pre>"},{"location":"configuration/#cli-configuration","title":"CLI Configuration","text":"<p>You can also configure ragcrawl via command line:</p> Bash<pre><code>ragcrawl crawl https://docs.example.com \\\n    --max-pages 100 \\\n    --max-depth 5 \\\n    --include \"/docs/*\" \\\n    --exclude \"/blog/*\" \\\n    --delay 1.0 \\\n    --output ./output\n</code></pre>"},{"location":"configuration/#configuration-files","title":"Configuration Files","text":"<p>ragcrawl supports YAML configuration files:</p> YAML<pre><code># ragcrawl.yaml\nseeds:\n  - https://docs.example.com\n\ncrawl:\n  max_pages: 100\n  max_depth: 5\n  delay_seconds: 1.0\n\nfilters:\n  include_patterns:\n    - \"/docs/*\"\n  exclude_patterns:\n    - \"/blog/*\"\n\nstorage:\n  backend: duckdb\n  path: ./crawler.duckdb\n\noutput:\n  mode: multi\n  root_dir: ./output\n</code></pre> <p>Load with:</p> Bash<pre><code>ragcrawl crawl --config ragcrawl.yaml\n</code></pre>"},{"location":"configuration/#environment-variables","title":"Environment Variables","text":"<p>Some settings can be configured via environment variables:</p> Variable Description Default <code>RAGCRAWL_DB_PATH</code> DuckDB database path <code>./crawler.duckdb</code> <code>RAGCRAWL_OUTPUT_DIR</code> Default output directory <code>./output</code> <code>RAGCRAWL_LOG_LEVEL</code> Logging level <code>INFO</code> <code>RAGCRAWL_USER_AGENT</code> Custom user agent <code>ragcrawl/0.1</code>"},{"location":"configuration/#configuration-sections","title":"Configuration Sections","text":"<ul> <li> <p> Crawler Options</p> <p>Configure crawl behavior, limits, URL filtering, and politeness settings.</p> </li> <li> <p> Storage Backends</p> <p>Choose between DuckDB and DynamoDB for data persistence.</p> </li> <li> <p> Output Settings</p> <p>Configure output formats, file structure, and export options.</p> </li> <li> <p> Markdown Extraction</p> <p>Tune Crawl4AI filters and Markdown generator options for cleaner output.</p> </li> </ul>"},{"location":"configuration/#validation","title":"Validation","text":"<p>Configuration is validated at runtime:</p> Python<pre><code>from ragcrawl.config import CrawlerConfig\nfrom pydantic import ValidationError\n\ntry:\n    config = CrawlerConfig(\n        seeds=[],  # Error: empty seeds\n        max_pages=-1,  # Error: negative value\n    )\nexcept ValidationError as e:\n    print(e.errors())\n</code></pre>"},{"location":"configuration/#next-steps","title":"Next Steps","text":"<ul> <li>Crawler Options - Detailed crawler settings</li> <li>Storage Backends - Database configuration</li> <li>Output Settings - Output format options</li> </ul>"},{"location":"configuration/crawler/","title":"Crawler Configuration","text":"<p>Complete reference for <code>CrawlerConfig</code>.</p>"},{"location":"configuration/crawler/#basic-configuration","title":"Basic Configuration","text":"Python<pre><code>from ragcrawl.config.crawler_config import CrawlerConfig\n\nconfig = CrawlerConfig(\n    seeds=[\"https://docs.example.com\"],\n    max_pages=100,\n    max_depth=5,\n)\n</code></pre>"},{"location":"configuration/crawler/#all-options","title":"All Options","text":""},{"location":"configuration/crawler/#seeds-and-scope","title":"Seeds and Scope","text":"Option Type Default Description <code>seeds</code> <code>list[str]</code> Required Starting URLs for crawl <code>allowed_domains</code> <code>list[str]</code> Auto Domains to crawl (defaults to seed domains) <code>allow_subdomains</code> <code>bool</code> <code>True</code> Also crawl subdomains of allowed domains"},{"location":"configuration/crawler/#limits","title":"Limits","text":"Option Type Default Description <code>max_pages</code> <code>int</code> <code>100</code> Maximum total pages to crawl <code>max_depth</code> <code>int</code> <code>5</code> Maximum depth from seed URLs <code>max_pages_per_domain</code> <code>int</code> <code>None</code> Maximum pages per domain"},{"location":"configuration/crawler/#url-filtering","title":"URL Filtering","text":"Option Type Default Description <code>include_patterns</code> <code>list[str]</code> <code>[]</code> Regex patterns URLs must match <code>exclude_patterns</code> <code>list[str]</code> <code>[]</code> Regex patterns to skip <code>skip_extensions</code> <code>list[str]</code> Binary File extensions to skip"},{"location":"configuration/crawler/#fetching","title":"Fetching","text":"Option Type Default Description <code>fetch_mode</code> <code>FetchMode</code> <code>HTTP</code> HTTP, BROWSER, or HYBRID <code>timeout</code> <code>float</code> <code>30.0</code> Request timeout in seconds <code>user_agent</code> <code>str</code> Default Custom user agent string"},{"location":"configuration/crawler/#rate-limiting","title":"Rate Limiting","text":"Option Type Default Description <code>requests_per_second</code> <code>float</code> <code>2.0</code> Max requests per second per domain <code>concurrent_requests</code> <code>int</code> <code>10</code> Max concurrent requests <code>delay_range</code> <code>tuple[float, float]</code> <code>None</code> Random delay range (min, max)"},{"location":"configuration/crawler/#robotstxt","title":"Robots.txt","text":"Option Type Default Description <code>robots_mode</code> <code>RobotsMode</code> <code>STRICT</code> STRICT, OFF, or ALLOWLIST <code>robots_allowlist</code> <code>list[str]</code> <code>[]</code> Paths to allow despite robots.txt"},{"location":"configuration/crawler/#error-handling","title":"Error Handling","text":"Option Type Default Description <code>max_retries</code> <code>int</code> <code>3</code> Maximum retry attempts <code>retry_delay</code> <code>float</code> <code>1.0</code> Delay between retries <code>circuit_breaker_threshold</code> <code>int</code> <code>10</code> Failures before circuit break <code>circuit_breaker_reset</code> <code>float</code> <code>300.0</code> Seconds before circuit reset"},{"location":"configuration/crawler/#markdown-extraction","title":"Markdown Extraction","text":"Option Type Default Description <code>markdown</code> <code>MarkdownConfig</code> Tuned defaults Controls Crawl4AI content filtering and Markdown generator options. See Markdown Extraction."},{"location":"configuration/crawler/#fetchmode-enum","title":"FetchMode Enum","text":"Python<pre><code>from ragcrawl.config.crawler_config import FetchMode\n\nFetchMode.HTTP     # HTTP requests only (fastest)\nFetchMode.BROWSER  # Headless browser (for JS sites)\nFetchMode.HYBRID   # Try HTTP first, fall back to browser\n</code></pre>"},{"location":"configuration/crawler/#robotsmode-enum","title":"RobotsMode Enum","text":"Python<pre><code>from ragcrawl.config.crawler_config import RobotsMode\n\nRobotsMode.STRICT     # Respect all robots.txt rules\nRobotsMode.OFF        # Ignore robots.txt\nRobotsMode.ALLOWLIST  # Respect except for allowlisted paths\n</code></pre>"},{"location":"configuration/crawler/#complete-example","title":"Complete Example","text":"Python<pre><code>from ragcrawl.config.crawler_config import (\n    CrawlerConfig,\n    FetchMode,\n    RobotsMode,\n)\nfrom ragcrawl.config.storage_config import DuckDBConfig, StorageConfig\nfrom ragcrawl.config.output_config import OutputConfig, OutputMode\n\nconfig = CrawlerConfig(\n    # Seeds and scope\n    seeds=[\"https://docs.example.com\", \"https://api.example.com\"],\n    allowed_domains=[\"docs.example.com\", \"api.example.com\"],\n    allow_subdomains=True,\n\n    # Limits\n    max_pages=1000,\n    max_depth=10,\n    max_pages_per_domain=500,\n\n    # URL filtering\n    include_patterns=[\n        r\"/docs/.*\",\n        r\"/api/v\\d+/.*\",\n    ],\n    exclude_patterns=[\n        r\"/admin/.*\",\n        r\"/internal/.*\",\n        r\".*\\.(pdf|zip|exe)$\",\n    ],\n\n    # Fetching\n    fetch_mode=FetchMode.HTTP,\n    timeout=30.0,\n    user_agent=\"MyBot/1.0\",\n\n    # Rate limiting\n    requests_per_second=2.0,\n    concurrent_requests=5,\n    delay_range=(0.5, 1.5),\n\n    # Robots\n    robots_mode=RobotsMode.STRICT,\n\n    # Error handling\n    max_retries=3,\n    retry_delay=2.0,\n    circuit_breaker_threshold=10,\n\n    # Storage\n    storage=StorageConfig(\n        backend=DuckDBConfig(path=\"./crawler.duckdb\")\n    ),\n\n    # Output\n    output=OutputConfig(\n        mode=OutputMode.MULTI,\n        root_dir=\"./output\",\n    ),\n)\n</code></pre>"},{"location":"configuration/crawler/#environment-variables","title":"Environment Variables","text":"<p>Some options can be set via environment variables:</p> Bash<pre><code>export RAGCRAWL_USER_AGENT=\"MyBot/1.0\"\nexport RAGCRAWL_TIMEOUT=60\nexport RAGCRAWL_MAX_RETRIES=5\n</code></pre>"},{"location":"configuration/markdown/","title":"Markdown Extraction Settings","text":"<p><code>MarkdownConfig</code> controls how Crawl4AI extracts and filters Markdown for LLM-ready output. It is used by both <code>CrawlerConfig</code> and <code>SyncConfig</code> and tuned for documentation-style sites by default.</p>"},{"location":"configuration/markdown/#quick-example","title":"Quick Example","text":"Python<pre><code>from ragcrawl.config import CrawlerConfig\nfrom ragcrawl.config.markdown_config import MarkdownConfig, ContentFilterType\n\nconfig = CrawlerConfig(\n    seeds=[\"https://docs.example.com\"],\n    markdown=MarkdownConfig(\n        content_filter=ContentFilterType.PRUNING,\n        pruning_threshold=0.55,\n        excluded_tags=[\"nav\", \"footer\", \"form\"],\n        ignore_images=True,\n        include_citations=True,\n    ),\n)\n</code></pre> <p>Tip: <code>content_filter=\"bm25\"</code> requires <code>user_query</code>; otherwise ragcrawl falls back to no filter and logs a warning.</p>"},{"location":"configuration/markdown/#cli-usage","title":"CLI Usage","text":"<p>You can pass a TOML/JSON file with Markdown settings to the CLI:</p> TOML<pre><code># markdown.config.toml\ncontent_filter = \"pruning\"\npruning_threshold = 0.55\nignore_images = true\ninclude_citations = true\nexcluded_tags = [\"nav\", \"footer\", \"form\"]\n</code></pre> Bash<pre><code>ragcrawl crawl https://docs.example.com --markdown-config ./markdown.config.toml\n</code></pre> <p>JSON works too:</p> JSON<pre><code>{\n  \"content_filter\": \"bm25\",\n  \"user_query\": \"authentication guide\",\n  \"bm25_threshold\": 1.2\n}\n</code></pre>"},{"location":"configuration/markdown/#content-filters","title":"Content Filters","text":"Option Type / Values Default Description <code>content_filter</code> <code>none \\| pruning \\| bm25</code> <code>pruning</code> Select the Crawl4AI content filter. <code>word_count_threshold</code> <code>int</code> <code>15</code> Minimum words per text block to keep. <code>remove_overlay_elements</code> <code>bool</code> <code>true</code> Drop popups and modals. <code>process_iframes</code> <code>bool</code> <code>true</code> Include iframe content. <code>remove_forms</code> <code>bool</code> <code>true</code> Strip form elements from output."},{"location":"configuration/markdown/#pruning-filter-default","title":"Pruning Filter (default)","text":"Option Type Default Description <code>pruning_threshold</code> <code>float</code> <code>0.55</code> Higher = more aggressive boilerplate removal. <code>pruning_threshold_type</code> <code>str</code> <code>\"fixed\"</code> <code>\"fixed\"</code> or <code>\"dynamic\"</code> scoring strategy. <code>pruning_min_word_threshold</code> <code>int</code> <code>15</code> Minimum words per block to keep."},{"location":"configuration/markdown/#bm25-filter-query-focused","title":"BM25 Filter (query-focused)","text":"Option Type Default Description <code>bm25_threshold</code> <code>float</code> <code>1.0</code> Relevance cutoff; higher is stricter. <code>user_query</code> <code>str \\| None</code> <code>None</code> Required when <code>content_filter=\"bm25\"</code>."},{"location":"configuration/markdown/#html-selection","title":"HTML Selection","text":"Option Type Default Description <code>excluded_tags</code> <code>list[str]</code> <code>[\"nav\",\"footer\",\"header\",\"aside\",\"noscript\"]</code> Tags to drop entirely. <code>excluded_selector</code> <code>str \\| None</code> <code>None</code> CSS selector to exclude (e.g., <code>.sidebar, .ads</code>). <code>css_selector</code> <code>str \\| None</code> <code>None</code> CSS selector to target (e.g., <code>article, main</code>). <code>target_elements</code> <code>list[str] \\| None</code> <code>None</code> Flexible element targets for extraction."},{"location":"configuration/markdown/#link-filtering","title":"Link Filtering","text":"Option Type Default Description <code>exclude_external_links</code> <code>bool</code> <code>false</code> Drop hyperlinks to other domains. <code>exclude_social_media_links</code> <code>bool</code> <code>true</code> Remove common social links. <code>exclude_external_images</code> <code>bool</code> <code>false</code> Remove images hosted off-domain. <code>exclude_domains</code> <code>list[str]</code> <code>[]</code> Specific domains to strip from links."},{"location":"configuration/markdown/#markdown-generator-options","title":"Markdown Generator Options","text":"Option Type Default Description <code>ignore_links</code> <code>bool</code> <code>false</code> Remove all links from Markdown. <code>ignore_images</code> <code>bool</code> <code>false</code> Remove all images from Markdown. <code>escape_html</code> <code>bool</code> <code>true</code> Convert HTML entities to text. <code>body_width</code> <code>int</code> <code>0</code> Wrap text at width; <code>0</code> = no wrapping. <code>skip_internal_links</code> <code>bool</code> <code>false</code> Drop same-page anchor links. <code>include_sup_sub</code> <code>bool</code> <code>true</code> Preserve sup/sub formatting."},{"location":"configuration/markdown/#output-selection","title":"Output Selection","text":"Option Type Default Description <code>use_fit_markdown</code> <code>bool</code> <code>true</code> Prefer filtered <code>fit_markdown</code> when available, otherwise <code>raw_markdown</code>. <code>include_citations</code> <code>bool</code> <code>false</code> Use <code>markdown_with_citations</code> (reference-style links) when present."},{"location":"configuration/output/","title":"Output Configuration","text":"<p>Configure how crawled content is published.</p>"},{"location":"configuration/output/#overview","title":"Overview","text":"<p>ragcrawl can output content in two modes:</p> <ul> <li>Multi-page: Each page becomes a separate Markdown file</li> <li>Single-page: All content combined into one file</li> </ul>"},{"location":"configuration/output/#outputconfig","title":"OutputConfig","text":"Python<pre><code>from ragcrawl.config.output_config import OutputConfig, OutputMode\n\nconfig = OutputConfig(\n    mode=OutputMode.MULTI,\n    root_dir=\"./output\",\n)\n</code></pre>"},{"location":"configuration/output/#options","title":"Options","text":"Option Type Default Description <code>mode</code> <code>OutputMode</code> <code>MULTI</code> SINGLE or MULTI <code>root_dir</code> <code>str</code> <code>\"./output\"</code> Output directory <code>include_metadata</code> <code>bool</code> <code>True</code> Include source URLs in output <code>include_toc</code> <code>bool</code> <code>True</code> Include table of contents (SINGLE mode) <code>rewrite_links</code> <code>bool</code> <code>True</code> Rewrite internal links (MULTI mode) <code>single_file_name</code> <code>str</code> <code>\"knowledge_base.md\"</code> Output filename (SINGLE mode) <code>generate_index</code> <code>bool</code> <code>True</code> Generate index.md (MULTI mode)"},{"location":"configuration/output/#multi-page-output","title":"Multi-Page Output","text":"<p>Each crawled page becomes a separate Markdown file:</p> Python<pre><code>config = OutputConfig(\n    mode=OutputMode.MULTI,\n    root_dir=\"./docs-output\",\n    include_metadata=True,\n    rewrite_links=True,\n    generate_index=True,\n)\n</code></pre>"},{"location":"configuration/output/#output-structure","title":"Output Structure","text":"Text Only<pre><code>docs-output/\n\u251c\u2500\u2500 index.md                 # Generated index\n\u251c\u2500\u2500 example.com/\n\u2502   \u251c\u2500\u2500 index.md\n\u2502   \u251c\u2500\u2500 docs/\n\u2502   \u2502   \u251c\u2500\u2500 getting-started.md\n\u2502   \u2502   \u251c\u2500\u2500 installation.md\n\u2502   \u2502   \u2514\u2500\u2500 api/\n\u2502   \u2502       \u251c\u2500\u2500 overview.md\n\u2502   \u2502       \u2514\u2500\u2500 reference.md\n\u2502   \u2514\u2500\u2500 blog/\n\u2502       \u251c\u2500\u2500 post-1.md\n\u2502       \u2514\u2500\u2500 post-2.md\n</code></pre>"},{"location":"configuration/output/#link-rewriting","title":"Link Rewriting","text":"<p>Internal links are automatically converted:</p> <p>Original HTML: HTML<pre><code>&lt;a href=\"/docs/api/overview\"&gt;API Overview&lt;/a&gt;\n</code></pre></p> <p>Output Markdown: Markdown<pre><code>[API Overview](./api/overview.md)\n</code></pre></p>"},{"location":"configuration/output/#metadata-headers","title":"Metadata Headers","text":"<p>Each file includes source information:</p> Markdown<pre><code>&lt;!-- Source: https://example.com/docs/getting-started --&gt;\n&lt;!-- Crawled: 2024-01-15T10:30:00Z --&gt;\n\n# Getting Started\n\nContent here...\n</code></pre>"},{"location":"configuration/output/#single-page-output","title":"Single-Page Output","text":"<p>All content combined into one file:</p> Python<pre><code>config = OutputConfig(\n    mode=OutputMode.SINGLE,\n    root_dir=\"./output\",\n    single_file_name=\"knowledge_base.md\",\n    include_toc=True,\n    include_metadata=True,\n)\n</code></pre>"},{"location":"configuration/output/#output-structure_1","title":"Output Structure","text":"Text Only<pre><code>output/\n\u2514\u2500\u2500 knowledge_base.md\n</code></pre>"},{"location":"configuration/output/#file-format","title":"File Format","text":"Markdown<pre><code># Knowledge Base\n\nGenerated from https://docs.example.com\n\n## Table of Contents\n\n- [Getting Started](#getting-started)\n- [Installation](#installation)\n- [Configuration](#configuration)\n\n---\n\n## Getting Started\n\n&lt;!-- Source: https://example.com/docs/getting-started --&gt;\n\nContent here...\n\n---\n\n## Installation\n\n&lt;!-- Source: https://example.com/docs/installation --&gt;\n\nContent here...\n</code></pre>"},{"location":"configuration/output/#custom-publishers","title":"Custom Publishers","text":"<p>Create custom output formats:</p> Python<pre><code>from ragcrawl.output.publisher import MarkdownPublisher\nfrom ragcrawl.config.output_config import OutputConfig\nfrom pathlib import Path\n\nclass MyCustomPublisher(MarkdownPublisher):\n    def publish(self, documents: list) -&gt; list[Path]:\n        output_files = []\n\n        for doc in documents:\n            # Custom processing\n            content = self.format_document(doc)\n\n            # Custom filename\n            filename = f\"{doc.doc_id}.md\"\n            path = Path(self.config.root_dir) / filename\n\n            path.parent.mkdir(parents=True, exist_ok=True)\n            path.write_text(content)\n            output_files.append(path)\n\n        return output_files\n\n    def format_document(self, doc):\n        return f\"\"\"---\ntitle: {doc.title}\nurl: {doc.url}\ndate: {doc.fetched_at.isoformat()}\n---\n\n{doc.content}\n\"\"\"\n</code></pre>"},{"location":"configuration/output/#cli-options","title":"CLI Options","text":"Bash<pre><code># Multi-page output (default)\nragcrawl crawl https://example.com --output ./output --output-mode multi\n\n# Single-page output\nragcrawl crawl https://example.com --output ./output --output-mode single\n</code></pre>"},{"location":"configuration/output/#best-practices","title":"Best Practices","text":"<ol> <li>Multi-page for large sites: Easier to navigate and search</li> <li>Single-page for LLM context: One file for full-text RAG</li> <li>Enable link rewriting: Keeps navigation working offline</li> <li>Include metadata: Helps trace content back to sources</li> <li>Use consistent structure: Match the site's URL hierarchy</li> </ol>"},{"location":"configuration/storage/","title":"Storage Configuration","text":"<p>Configure where crawl data is stored.</p>"},{"location":"configuration/storage/#overview","title":"Overview","text":"<p>ragcrawl supports two storage backends:</p> <ul> <li>DuckDB (default): Local file-based storage, great for development and single-machine deployments</li> <li>DynamoDB: AWS cloud storage, suitable for distributed and serverless deployments</li> </ul>"},{"location":"configuration/storage/#duckdb-configuration","title":"DuckDB Configuration","text":""},{"location":"configuration/storage/#basic-setup","title":"Basic Setup","text":"Python<pre><code>from ragcrawl.config.storage_config import DuckDBConfig, StorageConfig\n\nstorage = StorageConfig(\n    backend=DuckDBConfig(path=\"./crawler.duckdb\")\n)\n</code></pre>"},{"location":"configuration/storage/#options","title":"Options","text":"Option Type Default Description <code>path</code> <code>str</code> <code>\"./crawler.duckdb\"</code> Path to database file <code>read_only</code> <code>bool</code> <code>False</code> Open in read-only mode"},{"location":"configuration/storage/#cli-usage","title":"CLI Usage","text":"Bash<pre><code># Default storage\nragcrawl crawl https://example.com\n\n# Custom storage path\nragcrawl crawl https://example.com --storage ./data/my-crawl.duckdb\n</code></pre>"},{"location":"configuration/storage/#multiple-databases","title":"Multiple Databases","text":"Python<pre><code># Separate databases for different projects\nproject_a = StorageConfig(backend=DuckDBConfig(path=\"./project-a.duckdb\"))\nproject_b = StorageConfig(backend=DuckDBConfig(path=\"./project-b.duckdb\"))\n</code></pre>"},{"location":"configuration/storage/#dynamodb-configuration","title":"DynamoDB Configuration","text":""},{"location":"configuration/storage/#prerequisites","title":"Prerequisites","text":"<ol> <li> <p>Install DynamoDB support:    Bash<pre><code>pip install ragcrawl[dynamodb]\n</code></pre></p> </li> <li> <p>Configure AWS credentials:    Bash<pre><code>export AWS_ACCESS_KEY_ID=your_key\nexport AWS_SECRET_ACCESS_KEY=your_secret\nexport AWS_DEFAULT_REGION=us-east-1\n</code></pre></p> </li> </ol>"},{"location":"configuration/storage/#basic-setup_1","title":"Basic Setup","text":"Python<pre><code>from ragcrawl.config.storage_config import DynamoDBConfig, StorageConfig\n\nstorage = StorageConfig(\n    backend=DynamoDBConfig(\n        table_prefix=\"myapp\",\n        region=\"us-east-1\",\n    )\n)\n</code></pre>"},{"location":"configuration/storage/#options_1","title":"Options","text":"Option Type Default Description <code>table_prefix</code> <code>str</code> <code>\"ragcrawl\"</code> Prefix for table names <code>region</code> <code>str</code> <code>\"us-east-1\"</code> AWS region <code>endpoint_url</code> <code>str</code> <code>None</code> Custom endpoint (for local testing) <code>create_tables</code> <code>bool</code> <code>True</code> Auto-create tables if missing"},{"location":"configuration/storage/#table-structure","title":"Table Structure","text":"<p>The following tables are created:</p> <ul> <li><code>{prefix}-sites</code>: Site configurations</li> <li><code>{prefix}-runs</code>: Crawl run records</li> <li><code>{prefix}-pages</code>: Page metadata</li> <li><code>{prefix}-versions</code>: Page versions with content</li> <li><code>{prefix}-frontier</code>: Crawl queue items</li> </ul>"},{"location":"configuration/storage/#local-development-with-dynamodb-local","title":"Local Development with DynamoDB Local","text":"Python<pre><code>storage = StorageConfig(\n    backend=DynamoDBConfig(\n        table_prefix=\"dev\",\n        region=\"us-east-1\",\n        endpoint_url=\"http://localhost:8000\",  # DynamoDB Local\n    )\n)\n</code></pre> <p>Run DynamoDB Local: Bash<pre><code>docker run -p 8000:8000 amazon/dynamodb-local\n</code></pre></p>"},{"location":"configuration/storage/#fallback-configuration","title":"Fallback Configuration","text":"<p>Configure fallback when DynamoDB is unavailable:</p> Python<pre><code>storage = StorageConfig(\n    backend=DynamoDBConfig(\n        table_prefix=\"prod\",\n        region=\"us-east-1\",\n    ),\n    fallback=DuckDBConfig(path=\"./fallback.duckdb\"),\n    fail_if_unavailable=False,  # Use fallback instead of failing\n)\n</code></pre>"},{"location":"configuration/storage/#accessing-storage-directly","title":"Accessing Storage Directly","text":"Python<pre><code>from ragcrawl.storage.backend import create_storage_backend\nfrom ragcrawl.config.storage_config import DuckDBConfig, StorageConfig\n\n# Create backend\nconfig = StorageConfig(backend=DuckDBConfig(path=\"./crawler.duckdb\"))\nbackend = create_storage_backend(config)\nbackend.initialize()\n\n# Query data\nsites = backend.list_sites()\nfor site in sites:\n    print(f\"Site: {site.name}\")\n    pages = backend.list_pages(site.site_id)\n    print(f\"  Pages: {len(pages)}\")\n\n# Clean up\nbackend.close()\n</code></pre>"},{"location":"configuration/storage/#data-schema","title":"Data Schema","text":""},{"location":"configuration/storage/#sites-table","title":"Sites Table","text":"Column Type Description site_id string Unique identifier name string Site name seeds json List of seed URLs allowed_domains json Allowed domains config json Crawler configuration created_at datetime Creation timestamp total_pages integer Total pages crawled total_runs integer Total crawl runs"},{"location":"configuration/storage/#pages-table","title":"Pages Table","text":"Column Type Description page_id string Unique identifier site_id string Parent site url string Page URL content_hash string Current content hash first_seen datetime First crawl time last_crawled datetime Last crawl time is_tombstone boolean Deleted page marker"},{"location":"configuration/storage/#versions-table","title":"Versions Table","text":"Column Type Description version_id string Unique identifier page_id string Parent page run_id string Crawl run markdown text Markdown content content_hash string Content hash title string Page title crawled_at datetime Crawl timestamp"},{"location":"configuration/storage/#best-practices","title":"Best Practices","text":"<ol> <li>Development: Use DuckDB for fast local iteration</li> <li>Production: Use DynamoDB for scalability and durability</li> <li>Testing: Use DynamoDB Local or in-memory DuckDB</li> <li>Backups: DuckDB files can be copied; DynamoDB has built-in backups</li> <li>Migration: Export from one backend, import to another via JSON export</li> </ol>"},{"location":"getting-started/","title":"Getting Started","text":"<p>Welcome to ragcrawl! This section will help you get up and running quickly.</p>"},{"location":"getting-started/#what-is-ragcrawl","title":"What is ragcrawl?","text":"<p>ragcrawl is a Python library for recursively crawling websites and producing LLM-ready knowledge base artifacts. It's designed specifically for:</p> <ul> <li>RAG Systems: Generate clean, structured content for Retrieval-Augmented Generation</li> <li>Documentation Crawling: Convert technical docs into searchable knowledge bases</li> <li>Content Migration: Extract and structure content from existing websites</li> <li>Knowledge Management: Build internal knowledge bases from company resources</li> </ul>"},{"location":"getting-started/#key-features","title":"Key Features","text":"<ul> <li> <p> Smart Crawling</p> <p>Respects robots.txt, handles rate limiting, and prevents duplicate content with intelligent URL normalization.</p> </li> <li> <p> Clean Markdown</p> <p>Converts web pages to clean, readable Markdown while preserving semantic structure.</p> </li> <li> <p> Incremental Sync</p> <p>Efficiently update your knowledge base with only changed content using sitemap, ETags, and content hashing.</p> </li> <li> <p> Flexible Chunking</p> <p>Heading-aware and token-based chunking optimized for embedding models.</p> </li> </ul>"},{"location":"getting-started/#quick-links","title":"Quick Links","text":"Topic Description Installation Install ragcrawl and its dependencies Quickstart Start crawling in 5 minutes CLI Reference Command-line interface guide Configuration Customize crawler behavior"},{"location":"getting-started/#system-requirements","title":"System Requirements","text":"<ul> <li>Python: 3.10 or higher</li> <li>OS: Linux, macOS, or Windows</li> <li>Memory: 512MB minimum (more for large sites)</li> <li>Storage: Varies based on crawled content</li> </ul>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<ol> <li>Install ragcrawl - Get the library installed</li> <li>Follow the Quickstart - Crawl your first site</li> <li>Explore the User Guide - Learn advanced features</li> </ol>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.10 or higher</li> <li>pip or uv package manager</li> </ul>"},{"location":"getting-started/installation/#installation-options","title":"Installation Options","text":""},{"location":"getting-started/installation/#basic-installation","title":"Basic Installation","text":"<p>The basic installation includes DuckDB storage and HTTP-only fetching:</p> Bash<pre><code>pip install ragcrawl\n</code></pre> <p>Or with uv:</p> Bash<pre><code>uv pip install ragcrawl\n</code></pre>"},{"location":"getting-started/installation/#browser-rendering-support","title":"Browser Rendering Support","text":"<p>For JavaScript-heavy sites, install with browser support:</p> Bash<pre><code>pip install ragcrawl[browser]\n</code></pre> <p>This installs Playwright for headless browser rendering.</p> <p>After installation, set up Playwright:</p> Bash<pre><code>playwright install chromium\n</code></pre>"},{"location":"getting-started/installation/#dynamodb-support","title":"DynamoDB Support","text":"<p>For cloud deployments with AWS DynamoDB:</p> Bash<pre><code>pip install ragcrawl[dynamodb]\n</code></pre>"},{"location":"getting-started/installation/#full-installation","title":"Full Installation","text":"<p>Install all optional dependencies:</p> Bash<pre><code>pip install ragcrawl[all]\n</code></pre>"},{"location":"getting-started/installation/#development-installation","title":"Development Installation","text":"<p>For contributing to the project:</p> Bash<pre><code>git clone https://github.com/your-org/ragcrawl.git\ncd ragcrawl\npip install -e \".[dev]\"\n</code></pre>"},{"location":"getting-started/installation/#verifying-installation","title":"Verifying Installation","text":"<p>Test your installation:</p> Bash<pre><code># Check CLI is available\nragcrawl --version\n\n# Run a simple crawl\nragcrawl crawl https://example.com --max-pages 5 --output ./test-output\n</code></pre>"},{"location":"getting-started/installation/#dependencies","title":"Dependencies","text":""},{"location":"getting-started/installation/#core-dependencies","title":"Core Dependencies","text":"Package Purpose crawl4ai Web fetching and HTML-to-Markdown conversion duckdb Default local storage backend httpx Async HTTP client pydantic Data validation and configuration structlog Structured logging xxhash Fast content hashing tiktoken Token counting for chunking click Command-line interface"},{"location":"getting-started/installation/#optional-dependencies","title":"Optional Dependencies","text":"Package Purpose Extra playwright Browser rendering <code>[browser]</code> pynamodb DynamoDB ORM <code>[dynamodb]</code>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/installation/#playwright-installation-issues","title":"Playwright Installation Issues","text":"<p>If you encounter issues with Playwright:</p> Bash<pre><code># Install system dependencies (Ubuntu/Debian)\nsudo apt-get install libnss3 libatk1.0-0 libatk-bridge2.0-0 libcups2 libdrm2 libxkbcommon0 libxcomposite1 libxdamage1 libxfixes3 libxrandr2 libgbm1 libasound2\n\n# Install browsers\nplaywright install chromium\n</code></pre>"},{"location":"getting-started/installation/#duckdb-permission-issues","title":"DuckDB Permission Issues","text":"<p>Ensure the storage directory is writable:</p> Bash<pre><code># Create with proper permissions\nmkdir -p ./data\nchmod 755 ./data\nragcrawl crawl https://example.com --storage ./data/crawler.duckdb\n</code></pre>"},{"location":"getting-started/installation/#aws-credentials-for-dynamodb","title":"AWS Credentials for DynamoDB","text":"<p>Set up AWS credentials for DynamoDB:</p> Bash<pre><code>export AWS_ACCESS_KEY_ID=your_key\nexport AWS_SECRET_ACCESS_KEY=your_secret\nexport AWS_DEFAULT_REGION=us-east-1\n</code></pre> <p>Or use AWS profiles:</p> Bash<pre><code>aws configure --profile crawler\nexport AWS_PROFILE=crawler\n</code></pre>"},{"location":"getting-started/quickstart/","title":"Quick Start Guide","text":"<p>This guide will help you crawl your first website in minutes.</p>"},{"location":"getting-started/quickstart/#your-first-crawl","title":"Your First Crawl","text":""},{"location":"getting-started/quickstart/#using-the-cli","title":"Using the CLI","text":"<p>The simplest way to crawl a website:</p> Bash<pre><code>ragcrawl crawl https://docs.example.com\n</code></pre> <p>This will: - Crawl up to 100 pages (default) - Save to <code>./output</code> directory - Store crawl data in <code>~/.ragcrawl/ragcrawl.duckdb</code></p>"},{"location":"getting-started/quickstart/#customizing-the-crawl","title":"Customizing the Crawl","text":"Bash<pre><code>ragcrawl crawl https://docs.example.com \\\n    --max-pages 500 \\\n    --max-depth 10 \\\n    --output ./knowledge-base \\\n    --output-mode single \\\n    --export-json ./export.json \\\n    --verbose\n</code></pre>"},{"location":"getting-started/quickstart/#cli-options-reference","title":"CLI Options Reference","text":"Option Short Description <code>--max-pages</code> <code>-m</code> Maximum pages to crawl <code>--max-depth</code> <code>-d</code> Maximum crawl depth <code>--output</code> <code>-o</code> Output directory <code>--output-mode</code> <code>single</code> or <code>multi</code> page output <code>--storage</code> <code>-s</code> DuckDB storage path <code>--include</code> <code>-i</code> Include URL patterns (regex) <code>--exclude</code> <code>-e</code> Exclude URL patterns (regex) <code>--robots/--no-robots</code> Respect robots.txt <code>--js/--no-js</code> Enable JavaScript rendering <code>--export-json</code> Export to JSON file <code>--export-jsonl</code> Export to JSONL file <code>--verbose</code> <code>-v</code> Verbose output"},{"location":"getting-started/quickstart/#using-the-python-api","title":"Using the Python API","text":"Python<pre><code>import asyncio\nfrom ragcrawl.config.crawler_config import CrawlerConfig\nfrom ragcrawl.config.output_config import OutputConfig, OutputMode\nfrom ragcrawl.core.crawl_job import CrawlJob\n\nasync def main():\n    config = CrawlerConfig(\n        seeds=[\"https://docs.example.com\"],\n        max_pages=100,\n        max_depth=5,\n        output=OutputConfig(\n            mode=OutputMode.MULTI,\n            root_dir=\"./output\",\n        ),\n    )\n\n    job = CrawlJob(config)\n    result = await job.run()\n\n    if result.success:\n        print(f\"\u2713 Crawled {result.stats.pages_crawled} pages\")\n        print(f\"\u2713 Duration: {result.duration_seconds:.1f}s\")\n\n        # Access documents\n        for doc in result.documents[:5]:\n            print(f\"  - {doc.title}: {doc.url}\")\n    else:\n        print(f\"\u2717 Error: {result.error}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"getting-started/quickstart/#output-formats","title":"Output Formats","text":""},{"location":"getting-started/quickstart/#multi-page-output-default","title":"Multi-Page Output (Default)","text":"<p>Each page becomes a separate Markdown file, preserving the site structure:</p> Text Only<pre><code>output/\n\u251c\u2500\u2500 example.com/\n\u2502   \u251c\u2500\u2500 index.md\n\u2502   \u251c\u2500\u2500 docs/\n\u2502   \u2502   \u251c\u2500\u2500 getting-started.md\n\u2502   \u2502   \u251c\u2500\u2500 configuration.md\n\u2502   \u2502   \u2514\u2500\u2500 api/\n\u2502   \u2502       \u251c\u2500\u2500 overview.md\n\u2502   \u2502       \u2514\u2500\u2500 reference.md\n\u2502   \u2514\u2500\u2500 blog/\n\u2502       \u251c\u2500\u2500 post-1.md\n\u2502       \u2514\u2500\u2500 post-2.md\n\u2514\u2500\u2500 index.md\n</code></pre>"},{"location":"getting-started/quickstart/#single-page-output","title":"Single-Page Output","text":"<p>All content combined into one file with a table of contents:</p> Bash<pre><code>ragcrawl crawl https://docs.example.com --output-mode single\n</code></pre> <p>Output: Text Only<pre><code>output/\n\u2514\u2500\u2500 knowledge_base.md\n</code></pre></p>"},{"location":"getting-started/quickstart/#filtering-urls","title":"Filtering URLs","text":""},{"location":"getting-started/quickstart/#include-only-specific-paths","title":"Include Only Specific Paths","text":"Bash<pre><code>ragcrawl crawl https://example.com \\\n    --include \"/docs/.*\" \\\n    --include \"/api/.*\"\n</code></pre>"},{"location":"getting-started/quickstart/#exclude-paths","title":"Exclude Paths","text":"Bash<pre><code>ragcrawl crawl https://example.com \\\n    --exclude \"/admin/.*\" \\\n    --exclude \"/private/.*\"\n</code></pre>"},{"location":"getting-started/quickstart/#exporting-data","title":"Exporting Data","text":""},{"location":"getting-started/quickstart/#json-export","title":"JSON Export","text":"Bash<pre><code>ragcrawl crawl https://example.com --export-json ./docs.json\n</code></pre>"},{"location":"getting-started/quickstart/#jsonl-export-streaming","title":"JSONL Export (Streaming)","text":"Bash<pre><code>ragcrawl crawl https://example.com --export-jsonl ./docs.jsonl\n</code></pre>"},{"location":"getting-started/quickstart/#incremental-sync","title":"Incremental Sync","text":"<p>After the initial crawl, sync to get only changes:</p> Bash<pre><code># First, find your site ID\nragcrawl sites\n\n# Then sync\nragcrawl sync site_abc123 --output ./updates\n</code></pre>"},{"location":"getting-started/quickstart/#sync-options","title":"Sync Options","text":"Option Short Description <code>--storage</code> <code>-s</code> DuckDB storage path <code>--max-pages</code> <code>-m</code> Maximum pages to sync <code>--max-age</code> Only check pages older than N hours <code>--output</code> <code>-o</code> Output directory for updates <code>--verbose</code> <code>-v</code> Verbose output"},{"location":"getting-started/quickstart/#managing-crawls","title":"Managing Crawls","text":""},{"location":"getting-started/quickstart/#list-sites-and-runs","title":"List Sites and Runs","text":"Bash<pre><code># List all crawled sites\nragcrawl sites\n\n# List all crawl runs\nragcrawl list\n\n# List runs for a specific site\nragcrawl runs site_abc123\n\n# Filter runs by status\nragcrawl list --status completed\nragcrawl list --status running\n</code></pre>"},{"location":"getting-started/quickstart/#configuration-management","title":"Configuration Management","text":"Bash<pre><code># Show current configuration\nragcrawl config show\n\n# Show config file path\nragcrawl config path\n\n# Set configuration values\nragcrawl config set storage_dir ~/.ragcrawl\nragcrawl config set user_agent \"MyBot/1.0\"\nragcrawl config set timeout 30\n\n# Reset to defaults\nragcrawl config reset\n</code></pre>"},{"location":"getting-started/quickstart/#common-workflows","title":"Common Workflows","text":""},{"location":"getting-started/quickstart/#documentation-site","title":"Documentation Site","text":"Bash<pre><code>ragcrawl crawl https://docs.myproject.com \\\n    --max-pages 1000 \\\n    --include \"/docs/.*\" \\\n    --output ./project-docs \\\n    --output-mode multi\n</code></pre>"},{"location":"getting-started/quickstart/#blog-archive","title":"Blog Archive","text":"Bash<pre><code>ragcrawl crawl https://blog.example.com \\\n    --max-pages 500 \\\n    --include \"/posts/.*\" \\\n    --output-mode single \\\n    --export-jsonl ./blog-posts.jsonl\n</code></pre>"},{"location":"getting-started/quickstart/#api-documentation","title":"API Documentation","text":"Bash<pre><code>ragcrawl crawl https://api.example.com/docs \\\n    --js \\\n    --max-pages 200 \\\n    --output ./api-docs\n</code></pre>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Crawling Guide - Advanced crawling options</li> <li>Syncing Guide - Keep your knowledge base updated</li> <li>Configuration - Full configuration reference</li> </ul>"},{"location":"user-guide/","title":"User Guide","text":"<p>This guide covers the core functionality of ragcrawl in detail.</p>"},{"location":"user-guide/#overview","title":"Overview","text":"<p>ragcrawl provides a complete pipeline for converting websites into LLM-ready knowledge bases:</p> <pre><code>graph LR\n    A[Web Pages] --&gt; B[Fetcher]\n    B --&gt; C[Extractor]\n    C --&gt; D[Storage]\n    D --&gt; E[Chunker]\n    E --&gt; F[Exporter]\n    F --&gt; G[Output Files]</code></pre>"},{"location":"user-guide/#guide-contents","title":"Guide Contents","text":""},{"location":"user-guide/#crawling-websites","title":"Crawling Websites","text":"<p>Learn how to crawl websites effectively:</p> <ul> <li>Starting a basic crawl</li> <li>Configuring URL filters</li> <li>Handling JavaScript-rendered content</li> <li>Respecting robots.txt and rate limits</li> <li>Managing large-scale crawls</li> </ul>"},{"location":"user-guide/#incremental-sync","title":"Incremental Sync","text":"<p>Keep your knowledge base up-to-date:</p> <ul> <li>Understanding sync strategies</li> <li>Using sitemaps for efficient updates</li> <li>Conditional requests with ETags</li> <li>Content change detection</li> <li>Handling deleted pages</li> </ul>"},{"location":"user-guide/#chunking-content","title":"Chunking Content","text":"<p>Prepare content for embedding models:</p> <ul> <li>Heading-aware chunking</li> <li>Token-based chunking</li> <li>Configuring chunk sizes</li> <li>Preserving context in chunks</li> <li>Metadata in chunks</li> </ul>"},{"location":"user-guide/#exporting-data","title":"Exporting Data","text":"<p>Export your crawled data:</p> <ul> <li>JSON and JSONL formats</li> <li>Single-page combined output</li> <li>Multi-page with preserved structure</li> <li>Link rewriting for local files</li> <li>Custom export formats</li> </ul>"},{"location":"user-guide/#common-workflows","title":"Common Workflows","text":""},{"location":"user-guide/#documentation-site-to-rag","title":"Documentation Site to RAG","text":"Python<pre><code>from ragcrawl import CrawlJob, CrawlerConfig\n\nconfig = CrawlerConfig(\n    seeds=[\"https://docs.example.com\"],\n    max_pages=500,\n    include_patterns=[\"/docs/*\"],\n)\n\njob = CrawlJob(config)\nresult = await job.run()\n\n# Chunk for embeddings\nfrom ragcrawl.chunking import HeadingChunker\nchunker = HeadingChunker(max_tokens=500)\nchunks = chunker.chunk_documents(result.documents)\n</code></pre>"},{"location":"user-guide/#keep-knowledge-base-fresh","title":"Keep Knowledge Base Fresh","text":"Python<pre><code>from ragcrawl import SyncJob, SyncConfig\n\nconfig = SyncConfig(\n    site_id=\"site_abc123\",\n    use_sitemap=True,\n    use_conditional_requests=True,\n)\n\njob = SyncJob(config)\nresult = await job.run()\n\nprint(f\"Updated: {result.stats.pages_changed}\")\nprint(f\"New: {result.stats.pages_new}\")\nprint(f\"Deleted: {result.stats.pages_deleted}\")\n</code></pre>"},{"location":"user-guide/#best-practices","title":"Best Practices","text":"<p>Start Small</p> <p>Begin with a small <code>max_pages</code> limit to test your configuration before crawling an entire site.</p> <p>Use Include Patterns</p> <p>Focus your crawl on relevant content with <code>include_patterns</code> to avoid noise.</p> <p>Enable Caching</p> <p>Use DuckDB storage to enable efficient incremental syncs.</p> <p>Respect Rate Limits</p> <p>Always configure appropriate delays between requests to avoid overloading target servers.</p>"},{"location":"user-guide/#next-steps","title":"Next Steps","text":"<ul> <li>Configuration Reference - All configuration options</li> <li>CLI Reference - Command-line usage</li> <li>API Reference - Python API documentation</li> </ul>"},{"location":"user-guide/chunking/","title":"Chunking Guide","text":"<p>Split documents into chunks optimized for RAG (Retrieval-Augmented Generation).</p>"},{"location":"user-guide/chunking/#why-chunk","title":"Why Chunk?","text":"<p>LLMs have context limits. Chunking helps you:</p> <ul> <li>Fit context windows: Keep chunks under token limits</li> <li>Improve retrieval: Smaller, focused chunks match queries better</li> <li>Preserve structure: Maintain document hierarchy in chunks</li> </ul>"},{"location":"user-guide/chunking/#chunking-strategies","title":"Chunking Strategies","text":""},{"location":"user-guide/chunking/#heading-based-chunking","title":"Heading-Based Chunking","text":"<p>Splits content at Markdown headings, preserving document structure:</p> Python<pre><code>from ragcrawl.chunking.heading_chunker import HeadingChunker\n\nchunker = HeadingChunker(\n    min_level=1,        # Start splitting at H1\n    max_level=3,        # Stop at H3 (don't split H4+)\n    min_chunk_chars=100,  # Minimum chunk size\n)\n\nchunks = chunker.chunk(markdown_content)\n\nfor chunk in chunks:\n    print(f\"Heading: {' &gt; '.join(chunk.heading_path)}\")\n    print(f\"Content: {chunk.content[:100]}...\")\n    print()\n</code></pre> <p>Output: Text Only<pre><code>Heading: Getting Started\nContent: This guide helps you get started with...\n\nHeading: Getting Started &gt; Installation\nContent: Install the package using pip...\n\nHeading: Getting Started &gt; Configuration\nContent: Configure your settings in config.yaml...\n</code></pre></p>"},{"location":"user-guide/chunking/#token-based-chunking","title":"Token-Based Chunking","text":"<p>Splits content by token count with overlap:</p> Python<pre><code>from ragcrawl.chunking.token_chunker import TokenChunker\n\nchunker = TokenChunker(\n    max_tokens=500,      # Maximum tokens per chunk\n    overlap_tokens=50,   # Overlap between chunks\n    encoding_name=\"cl100k_base\",  # OpenAI tokenizer\n)\n\nchunks = chunker.chunk(content)\n\nfor chunk in chunks:\n    print(f\"Chunk {chunk.chunk_index}: {chunk.token_count} tokens\")\n</code></pre>"},{"location":"user-guide/chunking/#chunking-documents","title":"Chunking Documents","text":""},{"location":"user-guide/chunking/#single-document","title":"Single Document","text":"Python<pre><code>from ragcrawl.models.document import Document\nfrom ragcrawl.chunking.heading_chunker import HeadingChunker\n\ndoc = Document(\n    doc_id=\"doc123\",\n    url=\"https://example.com/guide\",\n    title=\"User Guide\",\n    content=\"# Getting Started\\n\\n...\",\n    # ... other fields\n)\n\nchunker = HeadingChunker()\nchunks = chunker.chunk(doc.content)\n\n# Associate chunks with document\nfor chunk in chunks:\n    chunk.doc_id = doc.doc_id\n</code></pre>"},{"location":"user-guide/chunking/#batch-processing","title":"Batch Processing","text":"Python<pre><code>from ragcrawl.chunking.heading_chunker import HeadingChunker\n\nchunker = HeadingChunker()\nall_chunks = []\n\nfor doc in documents:\n    chunks = chunker.chunk(doc.content)\n    for chunk in chunks:\n        chunk.doc_id = doc.doc_id\n        all_chunks.append(chunk)\n\nprint(f\"Created {len(all_chunks)} chunks from {len(documents)} documents\")\n</code></pre>"},{"location":"user-guide/chunking/#chunk-metadata","title":"Chunk Metadata","text":"<p>Each chunk includes metadata:</p> Python<pre><code>from ragcrawl.models.chunk import Chunk\n\nchunk = Chunk(\n    chunk_id=\"chunk_abc123\",\n    doc_id=\"doc123\",\n    content=\"The actual chunk content...\",\n    chunk_index=0,           # Position in document\n    char_count=500,          # Character count\n    token_count=120,         # Token count (if computed)\n    heading_path=[\"Guide\", \"Setup\"],  # Heading hierarchy\n)\n</code></pre>"},{"location":"user-guide/chunking/#configuration-examples","title":"Configuration Examples","text":""},{"location":"user-guide/chunking/#for-rag-systems","title":"For RAG Systems","text":"<p>Optimize for semantic search:</p> Python<pre><code># Heading-based for structured content\nheading_chunker = HeadingChunker(\n    min_level=2,          # Keep H1 content together\n    max_level=3,\n    min_chunk_chars=200,  # Avoid tiny chunks\n)\n\n# Token-based for unstructured content\ntoken_chunker = TokenChunker(\n    max_tokens=256,       # Smaller chunks for better matching\n    overlap_tokens=30,    # Overlap for context continuity\n)\n</code></pre>"},{"location":"user-guide/chunking/#for-summarization","title":"For Summarization","text":"<p>Larger chunks preserve more context:</p> Python<pre><code>chunker = TokenChunker(\n    max_tokens=1000,\n    overlap_tokens=100,\n)\n</code></pre>"},{"location":"user-guide/chunking/#for-qa","title":"For Q&amp;A","text":"<p>Balance chunk size and specificity:</p> Python<pre><code>chunker = HeadingChunker(\n    min_level=2,\n    max_level=4,          # More granular splitting\n    min_chunk_chars=100,\n)\n</code></pre>"},{"location":"user-guide/chunking/#hybrid-chunking","title":"Hybrid Chunking","text":"<p>Combine strategies for best results:</p> Python<pre><code>from ragcrawl.chunking.heading_chunker import HeadingChunker\nfrom ragcrawl.chunking.token_chunker import TokenChunker\n\n# First split by headings\nheading_chunker = HeadingChunker(max_level=2)\nheading_chunks = heading_chunker.chunk(content)\n\n# Then split large sections by tokens\ntoken_chunker = TokenChunker(max_tokens=500, overlap_tokens=50)\nfinal_chunks = []\n\nfor chunk in heading_chunks:\n    if chunk.token_count &gt; 500:\n        # Split large sections\n        sub_chunks = token_chunker.chunk(chunk.content)\n        for sub in sub_chunks:\n            sub.heading_path = chunk.heading_path\n            final_chunks.append(sub)\n    else:\n        final_chunks.append(chunk)\n</code></pre>"},{"location":"user-guide/chunking/#exporting-chunks","title":"Exporting Chunks","text":""},{"location":"user-guide/chunking/#to-json","title":"To JSON","text":"Python<pre><code>import json\n\nchunks_data = [\n    {\n        \"chunk_id\": chunk.chunk_id,\n        \"doc_id\": chunk.doc_id,\n        \"content\": chunk.content,\n        \"heading_path\": chunk.heading_path,\n        \"token_count\": chunk.token_count,\n    }\n    for chunk in chunks\n]\n\nwith open(\"chunks.json\", \"w\") as f:\n    json.dump(chunks_data, f, indent=2)\n</code></pre>"},{"location":"user-guide/chunking/#to-vector-database-format","title":"To Vector Database Format","text":"Python<pre><code># Format for Pinecone, Weaviate, etc.\nvectors = []\nfor chunk in chunks:\n    vectors.append({\n        \"id\": chunk.chunk_id,\n        \"text\": chunk.content,\n        \"metadata\": {\n            \"doc_id\": chunk.doc_id,\n            \"heading\": \" &gt; \".join(chunk.heading_path or []),\n            \"char_count\": chunk.char_count,\n        },\n    })\n</code></pre>"},{"location":"user-guide/chunking/#best-practices","title":"Best Practices","text":"<ol> <li>Match chunk size to your model: GPT-4 handles larger chunks than smaller models</li> <li>Use overlap for continuity: Prevents information loss at boundaries</li> <li>Preserve structure: Heading paths help with retrieval and citation</li> <li>Test chunk quality: Evaluate retrieval performance with your queries</li> <li>Consider content type: Code needs different chunking than prose</li> </ol>"},{"location":"user-guide/crawling/","title":"Crawling Guide","text":"<p>This guide covers all aspects of crawling websites with ragcrawl.</p>"},{"location":"user-guide/crawling/#how-crawling-works","title":"How Crawling Works","text":"<ol> <li>Seed URLs: The crawler starts from one or more seed URLs</li> <li>Fetch: Each page is fetched via HTTP or browser rendering</li> <li>Extract: Content is converted to Markdown, links are extracted</li> <li>Filter: Links are filtered based on domain, patterns, and depth</li> <li>Queue: New links are added to the priority queue (frontier)</li> <li>Store: Page content and metadata are stored</li> <li>Repeat: Process continues until limits are reached</li> </ol>"},{"location":"user-guide/crawling/#cli-usage","title":"CLI Usage","text":""},{"location":"user-guide/crawling/#basic-crawl","title":"Basic Crawl","text":"Bash<pre><code>ragcrawl crawl https://docs.example.com\n</code></pre>"},{"location":"user-guide/crawling/#full-options","title":"Full Options","text":"Bash<pre><code>ragcrawl crawl https://docs.example.com \\\n    --max-pages 500 \\\n    --max-depth 10 \\\n    --output ./output \\\n    --output-mode multi \\\n    --storage ./crawler.duckdb \\\n    --include \"/docs/.*\" \\\n    --exclude \"/admin/.*\" \\\n    --robots \\\n    --js \\\n    --export-json ./docs.json \\\n    --export-jsonl ./docs.jsonl \\\n    --verbose\n</code></pre>"},{"location":"user-guide/crawling/#cli-options-reference","title":"CLI Options Reference","text":"Option Short Description <code>--max-pages</code> <code>-m</code> Maximum pages to crawl (default: from config) <code>--max-depth</code> <code>-d</code> Maximum crawl depth (default: from config) <code>--output</code> <code>-o</code> Output directory <code>--output-mode</code> <code>single</code> or <code>multi</code> page output <code>--storage</code> <code>-s</code> DuckDB storage path (default: <code>~/.ragcrawl/ragcrawl.duckdb</code>) <code>--include</code> <code>-i</code> Include URL patterns (regex, repeatable) <code>--exclude</code> <code>-e</code> Exclude URL patterns (regex, repeatable) <code>--robots/--no-robots</code> Respect robots.txt (default: enabled) <code>--js/--no-js</code> Enable JavaScript rendering <code>--export-json</code> Export documents to JSON file <code>--export-jsonl</code> Export documents to JSONL file <code>--markdown-config</code> Path to MarkdownConfig overrides (TOML/JSON) <code>--verbose</code> <code>-v</code> Verbose output"},{"location":"user-guide/crawling/#multiple-seeds","title":"Multiple Seeds","text":"<p>You can specify multiple seed URLs:</p> Bash<pre><code>ragcrawl crawl https://docs.example.com https://api.example.com\n</code></pre>"},{"location":"user-guide/crawling/#crawl-modes","title":"Crawl Modes","text":""},{"location":"user-guide/crawling/#http-mode-default","title":"HTTP Mode (Default)","text":"<p>Fast crawling using HTTP requests. Best for static sites:</p> Python<pre><code>from ragcrawl.config.crawler_config import CrawlerConfig, FetchMode\n\nconfig = CrawlerConfig(\n    seeds=[\"https://docs.example.com\"],\n    fetch_mode=FetchMode.HTTP,\n)\n</code></pre>"},{"location":"user-guide/crawling/#browser-mode","title":"Browser Mode","text":"<p>Uses headless Chromium for JavaScript-heavy sites:</p> Python<pre><code>config = CrawlerConfig(\n    seeds=[\"https://app.example.com\"],\n    fetch_mode=FetchMode.BROWSER,\n)\n</code></pre> <p>CLI: Bash<pre><code>ragcrawl crawl https://app.example.com --js\n</code></pre></p>"},{"location":"user-guide/crawling/#hybrid-mode","title":"Hybrid Mode","text":"<p>Tries HTTP first, falls back to browser on failure:</p> Python<pre><code>config = CrawlerConfig(\n    seeds=[\"https://example.com\"],\n    fetch_mode=FetchMode.HYBRID,\n)\n</code></pre>"},{"location":"user-guide/crawling/#url-filtering","title":"URL Filtering","text":""},{"location":"user-guide/crawling/#domain-restrictions","title":"Domain Restrictions","text":"<p>By default, the crawler stays within the seed domains:</p> Python<pre><code>config = CrawlerConfig(\n    seeds=[\"https://docs.example.com\"],\n    allowed_domains=[\"docs.example.com\", \"api.example.com\"],\n    allow_subdomains=True,  # Also allows sub.docs.example.com\n)\n</code></pre>"},{"location":"user-guide/crawling/#path-patterns","title":"Path Patterns","text":"<p>Use regex patterns to include or exclude URLs:</p> Python<pre><code>config = CrawlerConfig(\n    seeds=[\"https://example.com\"],\n    include_patterns=[\n        r\"/docs/.*\",      # Only crawl /docs/\n        r\"/api/v\\d+/.*\",  # API versioned paths\n    ],\n    exclude_patterns=[\n        r\"/admin/.*\",     # Skip admin pages\n        r\".*\\.pdf$\",      # Skip PDFs\n    ],\n)\n</code></pre>"},{"location":"user-guide/crawling/#depth-limiting","title":"Depth Limiting","text":"<p>Control how deep the crawler goes from seed URLs:</p> Python<pre><code>config = CrawlerConfig(\n    seeds=[\"https://example.com\"],\n    max_depth=3,  # Maximum 3 clicks from seed\n)\n</code></pre>"},{"location":"user-guide/crawling/#rate-limiting","title":"Rate Limiting","text":""},{"location":"user-guide/crawling/#per-domain-rate-limits","title":"Per-Domain Rate Limits","text":"<p>Respect website resources:</p> Python<pre><code>config = CrawlerConfig(\n    seeds=[\"https://example.com\"],\n    requests_per_second=2.0,  # Max 2 requests/second per domain\n    concurrent_requests=5,     # Max 5 concurrent requests total\n)\n</code></pre>"},{"location":"user-guide/crawling/#delays","title":"Delays","text":"<p>Add delays between requests:</p> Python<pre><code>config = CrawlerConfig(\n    seeds=[\"https://example.com\"],\n    delay_range=(1.0, 3.0),  # Random delay 1-3 seconds\n)\n</code></pre>"},{"location":"user-guide/crawling/#robotstxt-compliance","title":"Robots.txt Compliance","text":""},{"location":"user-guide/crawling/#strict-mode-default","title":"Strict Mode (Default)","text":"<p>Respects all robots.txt directives:</p> Python<pre><code>from ragcrawl.config.crawler_config import RobotsMode\n\nconfig = CrawlerConfig(\n    seeds=[\"https://example.com\"],\n    robots_mode=RobotsMode.STRICT,\n)\n</code></pre>"},{"location":"user-guide/crawling/#off-mode","title":"Off Mode","text":"<p>Ignores robots.txt (use responsibly):</p> Python<pre><code>config = CrawlerConfig(\n    seeds=[\"https://example.com\"],\n    robots_mode=RobotsMode.OFF,\n)\n</code></pre>"},{"location":"user-guide/crawling/#page-limits","title":"Page Limits","text":""},{"location":"user-guide/crawling/#total-pages","title":"Total Pages","text":"Python<pre><code>config = CrawlerConfig(\n    seeds=[\"https://example.com\"],\n    max_pages=1000,  # Stop after 1000 pages\n)\n</code></pre>"},{"location":"user-guide/crawling/#per-domain-limits","title":"Per-Domain Limits","text":"Python<pre><code>config = CrawlerConfig(\n    seeds=[\"https://example.com\", \"https://other.com\"],\n    max_pages_per_domain=500,  # Max 500 pages per domain\n)\n</code></pre>"},{"location":"user-guide/crawling/#error-handling","title":"Error Handling","text":""},{"location":"user-guide/crawling/#retry-configuration","title":"Retry Configuration","text":"Python<pre><code>config = CrawlerConfig(\n    seeds=[\"https://example.com\"],\n    max_retries=3,           # Retry failed requests\n    retry_delay=5.0,         # Wait 5 seconds between retries\n    timeout=30.0,            # 30 second timeout per request\n)\n</code></pre>"},{"location":"user-guide/crawling/#circuit-breaker","title":"Circuit Breaker","text":"<p>The crawler automatically stops requesting from failing domains:</p> Python<pre><code>config = CrawlerConfig(\n    seeds=[\"https://example.com\"],\n    circuit_breaker_threshold=10,  # Stop after 10 consecutive failures\n    circuit_breaker_reset=300,     # Reset after 5 minutes\n)\n</code></pre>"},{"location":"user-guide/crawling/#hooks-and-callbacks","title":"Hooks and Callbacks","text":""},{"location":"user-guide/crawling/#on-page-crawled","title":"On Page Crawled","text":"Python<pre><code>from ragcrawl.hooks.callbacks import HookManager\n\ndef on_page(url: str, content: str, metadata: dict):\n    print(f\"Crawled: {url} ({len(content)} chars)\")\n\nhooks = HookManager()\nhooks.on_page_crawled(on_page)\n\nconfig = CrawlerConfig(\n    seeds=[\"https://example.com\"],\n    hooks=hooks,\n)\n</code></pre>"},{"location":"user-guide/crawling/#on-error","title":"On Error","text":"Python<pre><code>def on_error(url: str, error: Exception):\n    print(f\"Error crawling {url}: {error}\")\n\nhooks.on_error(on_error)\n</code></pre>"},{"location":"user-guide/crawling/#content-processing","title":"Content Processing","text":""},{"location":"user-guide/crawling/#redaction","title":"Redaction","text":"<p>Remove sensitive content before storage:</p> Python<pre><code>from ragcrawl.hooks.callbacks import PatternRedactor\n\nredactor = PatternRedactor([\n    r\"\\b\\d{3}-\\d{2}-\\d{4}\\b\",  # SSN pattern\n    r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b\",  # Email\n])\n\nhooks = HookManager()\nhooks.add_redactor(redactor)\n</code></pre>"},{"location":"user-guide/crawling/#complete-example","title":"Complete Example","text":"Python<pre><code>import asyncio\nfrom ragcrawl.config.crawler_config import (\n    CrawlerConfig,\n    FetchMode,\n    RobotsMode,\n)\nfrom ragcrawl.config.output_config import OutputConfig, OutputMode\nfrom ragcrawl.config.storage_config import DuckDBConfig, StorageConfig\nfrom ragcrawl.core.crawl_job import CrawlJob\n\nasync def crawl_documentation():\n    config = CrawlerConfig(\n        # Seeds\n        seeds=[\"https://docs.example.com\"],\n\n        # Limits\n        max_pages=1000,\n        max_depth=10,\n\n        # Filtering\n        include_patterns=[r\"/docs/.*\", r\"/tutorials/.*\"],\n        exclude_patterns=[r\"/admin/.*\"],\n\n        # Fetching\n        fetch_mode=FetchMode.HTTP,\n        robots_mode=RobotsMode.STRICT,\n        requests_per_second=2.0,\n        concurrent_requests=5,\n        timeout=30.0,\n\n        # Storage\n        storage=StorageConfig(\n            backend=DuckDBConfig(path=\"./docs.duckdb\")\n        ),\n\n        # Output\n        output=OutputConfig(\n            mode=OutputMode.MULTI,\n            root_dir=\"./docs-output\",\n            include_metadata=True,\n            rewrite_links=True,\n        ),\n    )\n\n    job = CrawlJob(config)\n    result = await job.run()\n\n    if result.success:\n        print(f\"Successfully crawled {result.stats.pages_crawled} pages\")\n        print(f\"Failed: {result.stats.pages_failed}\")\n        print(f\"Duration: {result.duration_seconds:.1f}s\")\n    else:\n        print(f\"Crawl failed: {result.error}\")\n\nasyncio.run(crawl_documentation())\n</code></pre>"},{"location":"user-guide/exporting/","title":"Exporting Guide","text":"<p>Export crawled content in various formats.</p>"},{"location":"user-guide/exporting/#export-formats","title":"Export Formats","text":""},{"location":"user-guide/exporting/#json-export","title":"JSON Export","text":"<p>Export all documents to a single JSON file:</p> Python<pre><code>from ragcrawl.export.json_exporter import JSONExporter\nfrom pathlib import Path\n\nexporter = JSONExporter(indent=2)\nexporter.export_documents(documents, Path(\"./docs.json\"))\n</code></pre> <p>Output format: JSON<pre><code>[\n  {\n    \"doc_id\": \"abc123\",\n    \"url\": \"https://example.com/page1\",\n    \"title\": \"Page 1\",\n    \"content\": \"# Page 1\\n\\nContent here...\",\n    \"fetched_at\": \"2024-01-15T10:30:00Z\",\n    \"status_code\": 200,\n    \"word_count\": 500\n  },\n  {\n    \"doc_id\": \"def456\",\n    \"url\": \"https://example.com/page2\",\n    \"title\": \"Page 2\",\n    \"content\": \"# Page 2\\n\\nMore content...\",\n    \"fetched_at\": \"2024-01-15T10:31:00Z\",\n    \"status_code\": 200,\n    \"word_count\": 300\n  }\n]\n</code></pre></p>"},{"location":"user-guide/exporting/#jsonl-export-streaming","title":"JSONL Export (Streaming)","text":"<p>Export one document per line for streaming/large datasets:</p> Python<pre><code>from ragcrawl.export.json_exporter import JSONLExporter\nfrom pathlib import Path\n\nexporter = JSONLExporter()\nexporter.export_documents(documents, Path(\"./docs.jsonl\"))\n</code></pre> <p>Output format: Text Only<pre><code>{\"doc_id\":\"abc123\",\"url\":\"https://example.com/page1\",\"title\":\"Page 1\",...}\n{\"doc_id\":\"def456\",\"url\":\"https://example.com/page2\",\"title\":\"Page 2\",...}\n</code></pre></p>"},{"location":"user-guide/exporting/#cli-export","title":"CLI Export","text":""},{"location":"user-guide/exporting/#during-crawl","title":"During Crawl","text":"Bash<pre><code># Export to JSON\nragcrawl crawl https://example.com --export-json ./docs.json\n\n# Export to JSONL\nragcrawl crawl https://example.com --export-jsonl ./docs.jsonl\n\n# Both formats\nragcrawl crawl https://example.com \\\n    --export-json ./docs.json \\\n    --export-jsonl ./docs.jsonl\n</code></pre>"},{"location":"user-guide/exporting/#from-storage","title":"From Storage","text":"<p>Export previously crawled content:</p> Python<pre><code>from ragcrawl.storage.backend import create_storage_backend\nfrom ragcrawl.config.storage_config import DuckDBConfig, StorageConfig\nfrom ragcrawl.export.json_exporter import JSONExporter\nfrom ragcrawl.models.document import Document\n\n# Connect to storage\nconfig = StorageConfig(backend=DuckDBConfig(path=\"./crawler.duckdb\"))\nbackend = create_storage_backend(config)\nbackend.initialize()\n\n# Get site\nsite = backend.list_sites()[0]\n\n# Get all pages with latest versions\ndocuments = []\npages = backend.list_pages(site.site_id)\n\nfor page in pages:\n    if page.is_tombstone:\n        continue\n\n    version = backend.get_latest_version(page.page_id)\n    if version:\n        doc = Document(\n            doc_id=page.page_id,\n            url=page.url,\n            title=version.title,\n            content=version.markdown,\n            fetched_at=version.crawled_at,\n            status_code=version.status_code,\n            content_type=version.content_type,\n            word_count=version.word_count,\n        )\n        documents.append(doc)\n\n# Export\nexporter = JSONExporter()\nexporter.export_documents(documents, Path(\"./export.json\"))\n\nbackend.close()\n</code></pre>"},{"location":"user-guide/exporting/#custom-export-fields","title":"Custom Export Fields","text":""},{"location":"user-guide/exporting/#select-specific-fields","title":"Select Specific Fields","text":"Python<pre><code>import json\nfrom pathlib import Path\n\ndef export_minimal(documents, output_path):\n    \"\"\"Export only essential fields.\"\"\"\n    data = [\n        {\n            \"url\": doc.url,\n            \"title\": doc.title,\n            \"content\": doc.content,\n        }\n        for doc in documents\n    ]\n\n    with open(output_path, \"w\") as f:\n        json.dump(data, f, indent=2)\n</code></pre>"},{"location":"user-guide/exporting/#add-custom-fields","title":"Add Custom Fields","text":"Python<pre><code>def export_with_metadata(documents, output_path, site_name):\n    \"\"\"Export with additional metadata.\"\"\"\n    data = [\n        {\n            \"id\": doc.doc_id,\n            \"source\": site_name,\n            \"url\": doc.url,\n            \"title\": doc.title,\n            \"content\": doc.content,\n            \"crawled_at\": doc.fetched_at.isoformat(),\n            \"word_count\": doc.word_count,\n            \"char_count\": doc.char_count,\n        }\n        for doc in documents\n    ]\n\n    with open(output_path, \"w\") as f:\n        json.dump(data, f, indent=2, default=str)\n</code></pre>"},{"location":"user-guide/exporting/#export-for-rag-systems","title":"Export for RAG Systems","text":""},{"location":"user-guide/exporting/#openailangchain-format","title":"OpenAI/LangChain Format","text":"Python<pre><code>def export_for_langchain(documents, output_path):\n    \"\"\"Export in LangChain Document format.\"\"\"\n    data = [\n        {\n            \"page_content\": doc.content,\n            \"metadata\": {\n                \"source\": doc.url,\n                \"title\": doc.title,\n                \"language\": doc.language,\n            },\n        }\n        for doc in documents\n    ]\n\n    with open(output_path, \"w\") as f:\n        json.dump(data, f, indent=2)\n</code></pre>"},{"location":"user-guide/exporting/#vector-database-format","title":"Vector Database Format","text":"Python<pre><code>def export_for_pinecone(documents, chunks):\n    \"\"\"Export chunks with embeddings format.\"\"\"\n    records = []\n\n    for chunk in chunks:\n        doc = next(d for d in documents if d.doc_id == chunk.doc_id)\n        records.append({\n            \"id\": chunk.chunk_id,\n            \"text\": chunk.content,\n            \"metadata\": {\n                \"doc_id\": chunk.doc_id,\n                \"url\": doc.url,\n                \"title\": doc.title,\n                \"heading\": \" &gt; \".join(chunk.heading_path or []),\n                \"chunk_index\": chunk.chunk_index,\n            },\n        })\n\n    return records\n</code></pre>"},{"location":"user-guide/exporting/#incremental-export","title":"Incremental Export","text":""},{"location":"user-guide/exporting/#export-changes-only","title":"Export Changes Only","text":"Python<pre><code>from ragcrawl.export.events import EventEmitter\n\nemitter = EventEmitter()\nchanged_docs = []\n\n@emitter.on(\"page_changed\")\ndef collect_change(event):\n    changed_docs.append(event.document)\n\n# After sync completes\nif changed_docs:\n    exporter = JSONExporter()\n    exporter.export_documents(changed_docs, Path(\"./changes.json\"))\n</code></pre>"},{"location":"user-guide/exporting/#append-to-jsonl","title":"Append to JSONL","text":"Python<pre><code>def append_to_jsonl(documents, output_path):\n    \"\"\"Append new documents to existing JSONL file.\"\"\"\n    import json\n\n    with open(output_path, \"a\") as f:\n        for doc in documents:\n            line = json.dumps(doc.model_dump(), default=str)\n            f.write(line + \"\\n\")\n</code></pre>"},{"location":"user-guide/exporting/#compression","title":"Compression","text":""},{"location":"user-guide/exporting/#gzip-export","title":"Gzip Export","text":"Python<pre><code>import gzip\nimport json\n\ndef export_gzipped(documents, output_path):\n    \"\"\"Export as gzipped JSON.\"\"\"\n    data = [doc.model_dump() for doc in documents]\n\n    with gzip.open(output_path, \"wt\", encoding=\"utf-8\") as f:\n        json.dump(data, f, default=str)\n</code></pre>"},{"location":"user-guide/exporting/#read-gzipped","title":"Read Gzipped","text":"Python<pre><code>import gzip\nimport json\n\nwith gzip.open(\"docs.json.gz\", \"rt\") as f:\n    documents = json.load(f)\n</code></pre>"},{"location":"user-guide/exporting/#best-practices","title":"Best Practices","text":"<ol> <li>Use JSONL for large datasets: Better for streaming and memory efficiency</li> <li>Include source URLs: Essential for citation and verification</li> <li>Add timestamps: Track when content was crawled</li> <li>Compress large exports: Save disk space and transfer time</li> <li>Export incrementally: Only export changes for efficiency</li> </ol>"},{"location":"user-guide/syncing/","title":"Syncing Guide","text":"<p>Keep your knowledge base up-to-date with incremental syncing.</p>"},{"location":"user-guide/syncing/#how-sync-works","title":"How Sync Works","text":"<p>The sync process efficiently updates your knowledge base:</p> <ol> <li>Sitemap Check: Parse sitemap.xml for recently changed URLs</li> <li>Conditional Requests: Use ETag/Last-Modified headers</li> <li>Content Hashing: Compare content hashes to detect changes</li> <li>Tombstones: Mark deleted pages (404/410 responses)</li> </ol>"},{"location":"user-guide/syncing/#basic-sync","title":"Basic Sync","text":""},{"location":"user-guide/syncing/#cli","title":"CLI","text":"Bash<pre><code># Find your site ID\nragcrawl sites\n\n# Sync the site\nragcrawl sync site_abc123\n\n# Sync with options\nragcrawl sync site_abc123 \\\n    --max-pages 500 \\\n    --max-age 24 \\\n    --output ./updates \\\n    --verbose\n</code></pre>"},{"location":"user-guide/syncing/#cli-options-reference","title":"CLI Options Reference","text":"Option Short Description <code>--storage</code> <code>-s</code> DuckDB storage path (default: <code>~/.ragcrawl/ragcrawl.duckdb</code>) <code>--max-pages</code> <code>-m</code> Maximum pages to sync <code>--max-age</code> Only check pages older than N hours <code>--output</code> <code>-o</code> Output directory for updates <code>--verbose</code> <code>-v</code> Verbose output"},{"location":"user-guide/syncing/#listing-sites-and-runs","title":"Listing Sites and Runs","text":"<p>Before syncing, you may need to find your site ID:</p> Bash<pre><code># List all crawled sites\nragcrawl sites\n\n# List all crawl runs\nragcrawl list\n\n# List runs for a specific site\nragcrawl runs site_abc123 --limit 10\n\n# Filter runs by status\nragcrawl list --status completed\nragcrawl list --site site_abc123\n</code></pre>"},{"location":"user-guide/syncing/#python-api","title":"Python API","text":"Python<pre><code>import asyncio\nfrom ragcrawl.config.sync_config import SyncConfig\nfrom ragcrawl.core.sync_job import SyncJob\n\nasync def sync_site():\n    config = SyncConfig(\n        site_id=\"site_abc123\",\n    )\n\n    job = SyncJob(config)\n    result = await job.run()\n\n    if result.success:\n        print(f\"Checked: {result.stats.pages_crawled}\")\n        print(f\"Changed: {result.stats.pages_changed}\")\n        print(f\"Deleted: {result.stats.pages_deleted}\")\n\nasyncio.run(sync_site())\n</code></pre>"},{"location":"user-guide/syncing/#sync-strategies","title":"Sync Strategies","text":""},{"location":"user-guide/syncing/#sitemap-based-fastest","title":"Sitemap-Based (Fastest)","text":"<p>Prioritizes pages listed in sitemap.xml with recent lastmod dates:</p> Python<pre><code>config = SyncConfig(\n    site_id=\"site_abc123\",\n    use_sitemap=True,\n    sitemap_url=\"https://example.com/sitemap.xml\",  # Auto-detected if not specified\n)\n</code></pre>"},{"location":"user-guide/syncing/#conditional-headers","title":"Conditional Headers","text":"<p>Uses HTTP caching headers to skip unchanged content:</p> Python<pre><code>config = SyncConfig(\n    site_id=\"site_abc123\",\n    use_conditional_requests=True,  # Default: True\n)\n</code></pre> <p>The crawler sends: - <code>If-None-Match: &lt;etag&gt;</code> if ETag is stored - <code>If-Modified-Since: &lt;date&gt;</code> if Last-Modified is stored</p> <p>304 Not Modified responses are skipped efficiently.</p>"},{"location":"user-guide/syncing/#content-hash-diffing","title":"Content Hash Diffing","text":"<p>Compares content hashes for pages without caching headers:</p> Python<pre><code>config = SyncConfig(\n    site_id=\"site_abc123\",\n    use_hash_diffing=True,  # Default: True\n)\n</code></pre>"},{"location":"user-guide/syncing/#sync-options","title":"Sync Options","text":""},{"location":"user-guide/syncing/#page-limits","title":"Page Limits","text":"Python<pre><code>config = SyncConfig(\n    site_id=\"site_abc123\",\n    max_pages=500,  # Maximum pages to check\n)\n</code></pre>"},{"location":"user-guide/syncing/#age-based-filtering","title":"Age-Based Filtering","text":"<p>Only check pages older than a certain age:</p> Python<pre><code>config = SyncConfig(\n    site_id=\"site_abc123\",\n    max_age_hours=24,  # Only check pages not synced in last 24 hours\n)\n</code></pre>"},{"location":"user-guide/syncing/#priority-based-selection","title":"Priority-Based Selection","text":"<p>Check high-priority pages first:</p> Python<pre><code>config = SyncConfig(\n    site_id=\"site_abc123\",\n    priority_patterns=[\n        r\"/docs/.*\",      # Check docs first\n        r\"/api/.*\",       # Then API docs\n    ],\n)\n</code></pre>"},{"location":"user-guide/syncing/#handling-changes","title":"Handling Changes","text":""},{"location":"user-guide/syncing/#change-events","title":"Change Events","text":"<p>Subscribe to change events during sync:</p> Python<pre><code>from ragcrawl.export.events import EventEmitter, ChangeEvent\n\nemitter = EventEmitter()\n\n@emitter.on(\"page_changed\")\ndef handle_change(event: ChangeEvent):\n    print(f\"Changed: {event.url}\")\n    print(f\"  Old hash: {event.old_hash}\")\n    print(f\"  New hash: {event.new_hash}\")\n\n@emitter.on(\"page_deleted\")\ndef handle_deletion(event: ChangeEvent):\n    print(f\"Deleted: {event.url}\")\n\nconfig = SyncConfig(\n    site_id=\"site_abc123\",\n    event_emitter=emitter,\n)\n</code></pre>"},{"location":"user-guide/syncing/#output-changes-only","title":"Output Changes Only","text":"<p>Export only changed content:</p> Python<pre><code>from ragcrawl.config.output_config import OutputConfig, OutputMode\n\nconfig = SyncConfig(\n    site_id=\"site_abc123\",\n    output=OutputConfig(\n        mode=OutputMode.MULTI,\n        root_dir=\"./updates\",\n    ),\n)\n\njob = SyncJob(config)\nresult = await job.run()\n\n# result.changed_pages contains list of changed URLs\n# result.documents contains only changed documents\n</code></pre>"},{"location":"user-guide/syncing/#tombstones","title":"Tombstones","text":"<p>Pages returning 404 or 410 are marked as \"tombstones\":</p> Python<pre><code># Check for tombstones\nfrom ragcrawl.storage.backend import create_storage_backend\n\nbackend = create_storage_backend(storage_config)\npages = backend.list_pages(site_id)\n\nfor page in pages:\n    if page.is_tombstone:\n        print(f\"Deleted page: {page.url}\")\n</code></pre>"},{"location":"user-guide/syncing/#sync-scheduling","title":"Sync Scheduling","text":""},{"location":"user-guide/syncing/#manual-scheduling","title":"Manual Scheduling","text":"Bash<pre><code># Run via cron\n0 * * * * cd /app &amp;&amp; ragcrawl sync site_abc123 &gt;&gt; /var/log/sync.log 2&gt;&amp;1\n</code></pre>"},{"location":"user-guide/syncing/#python-scheduler","title":"Python Scheduler","text":"Python<pre><code>import asyncio\nfrom datetime import datetime\n\nasync def scheduled_sync():\n    while True:\n        print(f\"Starting sync at {datetime.now()}\")\n\n        config = SyncConfig(site_id=\"site_abc123\")\n        job = SyncJob(config)\n        result = await job.run()\n\n        if result.stats.pages_changed &gt; 0:\n            print(f\"Updated {result.stats.pages_changed} pages\")\n\n        # Wait 1 hour\n        await asyncio.sleep(3600)\n\nasyncio.run(scheduled_sync())\n</code></pre>"},{"location":"user-guide/syncing/#complete-example","title":"Complete Example","text":"Python<pre><code>import asyncio\nfrom ragcrawl.config.sync_config import SyncConfig\nfrom ragcrawl.config.output_config import OutputConfig, OutputMode\nfrom ragcrawl.config.storage_config import DuckDBConfig, StorageConfig\nfrom ragcrawl.core.sync_job import SyncJob\nfrom ragcrawl.export.events import EventEmitter\n\nasync def sync_and_export():\n    # Set up event handling\n    emitter = EventEmitter()\n\n    @emitter.on(\"page_changed\")\n    def on_change(event):\n        print(f\"\ud83d\udcdd Updated: {event.url}\")\n\n    @emitter.on(\"page_deleted\")\n    def on_delete(event):\n        print(f\"\ud83d\uddd1\ufe0f Deleted: {event.url}\")\n\n    config = SyncConfig(\n        site_id=\"site_abc123\",\n\n        # Sync options\n        max_pages=1000,\n        max_age_hours=24,\n        use_sitemap=True,\n        use_conditional_requests=True,\n\n        # Storage\n        storage=StorageConfig(\n            backend=DuckDBConfig(path=\"./crawler.duckdb\")\n        ),\n\n        # Output changes\n        output=OutputConfig(\n            mode=OutputMode.MULTI,\n            root_dir=\"./updates\",\n        ),\n\n        event_emitter=emitter,\n    )\n\n    job = SyncJob(config)\n    result = await job.run()\n\n    print(f\"\\nSync Summary:\")\n    print(f\"  Pages checked: {result.stats.pages_crawled}\")\n    print(f\"  Pages changed: {result.stats.pages_changed}\")\n    print(f\"  Pages deleted: {result.stats.pages_deleted}\")\n    print(f\"  Duration: {result.duration_seconds:.1f}s\")\n\n    if result.changed_pages:\n        print(f\"\\nChanged pages:\")\n        for url in result.changed_pages[:10]:\n            print(f\"  - {url}\")\n\nasyncio.run(sync_and_export())\n</code></pre>"}]}