{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"ragcrawl","text":"<p>A Python library for crawling websites and producing LLM-ready knowledge base artifacts.</p>"},{"location":"#overview","title":"Overview","text":"<p>ragcrawl helps you create clean, structured knowledge bases from websites. It's designed specifically for preparing content for Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) systems.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Clean Markdown Output: Converts web pages to clean, readable Markdown</li> <li>Smart Crawling: Respects robots.txt, handles rate limiting, and prevents duplicate content</li> <li>Incremental Sync: Efficiently update your knowledge base with only changed content</li> <li>Multiple Output Formats: Single-page combined output or multi-page with preserved structure</li> <li>Chunking for RAG: Built-in support for heading-aware and token-based chunking</li> <li>Flexible Storage: DuckDB (default) for local use, DynamoDB for cloud deployments</li> <li>Export Options: JSON and JSONL export for integration with other tools</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#installation","title":"Installation","text":"<pre><code># Basic installation (DuckDB storage, HTTP-only fetching)\npip install ragcrawl\n\n# With browser rendering support\npip install ragcrawl[browser]\n\n# With DynamoDB support\npip install ragcrawl[dynamodb]\n\n# Full installation\npip install ragcrawl[all]\n</code></pre>"},{"location":"#basic-usage","title":"Basic Usage","text":"<pre><code># Crawl a website\nragcrawl crawl https://docs.example.com --max-pages 100 --output ./output\n\n# Sync for changes\nragcrawl sync site_abc123 --output ./updates\n\n# List crawled sites\nragcrawl sites\n</code></pre>"},{"location":"#python-api","title":"Python API","text":"<pre><code>import asyncio\nfrom ragcrawl.config.crawler_config import CrawlerConfig\nfrom ragcrawl.core.crawl_job import CrawlJob\n\nconfig = CrawlerConfig(\n    seeds=[\"https://docs.example.com\"],\n    max_pages=100,\n    max_depth=5,\n)\n\njob = CrawlJob(config)\nresult = asyncio.run(job.run())\n\nprint(f\"Crawled {result.stats.pages_crawled} pages\")\nfor doc in result.documents:\n    print(f\"  - {doc.url}: {doc.title}\")\n</code></pre>"},{"location":"#architecture","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Fetcher   \u2502\u2500\u2500\u2500\u2500\u25b6\u2502 Extractor\u2502\u2500\u2500\u2500\u2500\u25b6\u2502  Storage  \u2502\n\u2502 (Crawl4AI)  \u2502     \u2502          \u2502     \u2502 (DuckDB)  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502                                    \u2502\n       \u25bc                                    \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Frontier  \u2502     \u2502 Chunker  \u2502\u25c0\u2500\u2500\u2500\u2500\u2502  Export   \u2502\n\u2502  (Priority  \u2502     \u2502          \u2502     \u2502 (JSON/L)  \u2502\n\u2502   Queue)    \u2502     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518           \u2502\n                          \u25bc\n                   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                   \u2502  Output   \u2502\n                   \u2502 Publisher \u2502\n                   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"#use-cases","title":"Use Cases","text":""},{"location":"#documentation-sites","title":"Documentation Sites","text":"<p>Crawl technical documentation to create a searchable knowledge base for your AI assistant.</p>"},{"location":"#content-migration","title":"Content Migration","text":"<p>Extract content from legacy CMS systems in a structured format.</p>"},{"location":"#knowledge-management","title":"Knowledge Management","text":"<p>Build internal knowledge bases from company wikis and intranets.</p>"},{"location":"#research","title":"Research","text":"<p>Collect and structure web content for research and analysis.</p>"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li>Getting Started - Detailed installation and setup</li> <li>User Guide - Learn how to crawl websites</li> <li>Configuration - Customize crawler behavior</li> <li>API Reference - Full API documentation</li> </ul>"},{"location":"api/","title":"API Reference","text":"<p>Complete API documentation for ragcrawl.</p>"},{"location":"api/#core-classes","title":"Core Classes","text":""},{"location":"api/#crawljob","title":"CrawlJob","text":"<p>The main entry point for crawling:</p> <pre><code>from ragcrawl.core.crawl_job import CrawlJob\n\njob = CrawlJob(config)\nresult = await job.run()\n</code></pre> <p>Methods:</p> Method Returns Description <code>run()</code> <code>CrawlResult</code> Execute the crawl <code>stop()</code> <code>None</code> Stop crawling gracefully"},{"location":"api/#syncjob","title":"SyncJob","text":"<p>For incremental updates:</p> <pre><code>from ragcrawl.core.sync_job import SyncJob\n\njob = SyncJob(config)\nresult = await job.run()\n</code></pre> <p>Methods:</p> Method Returns Description <code>run()</code> <code>SyncResult</code> Execute the sync <code>stop()</code> <code>None</code> Stop syncing gracefully"},{"location":"api/#configuration-classes","title":"Configuration Classes","text":""},{"location":"api/#crawlerconfig","title":"CrawlerConfig","text":"<p>See Crawler Configuration.</p>"},{"location":"api/#syncconfig","title":"SyncConfig","text":"<pre><code>from ragcrawl.config.sync_config import SyncConfig\n\nconfig = SyncConfig(\n    site_id=\"site_abc123\",\n    max_pages=None,\n    max_age_hours=None,\n    use_sitemap=True,\n    use_conditional_requests=True,\n)\n</code></pre>"},{"location":"api/#storageconfig","title":"StorageConfig","text":"<p>See Storage Configuration.</p>"},{"location":"api/#outputconfig","title":"OutputConfig","text":"<p>See Output Configuration.</p>"},{"location":"api/#models","title":"Models","text":""},{"location":"api/#document","title":"Document","text":"<pre><code>from ragcrawl.models.document import Document\n\ndoc = Document(\n    doc_id=\"doc123\",\n    url=\"https://example.com/page\",\n    title=\"Page Title\",\n    description=\"Page description\",\n    content=\"# Markdown content\",\n    fetched_at=datetime.now(timezone.utc),\n    status_code=200,\n    content_type=\"text/html\",\n    language=\"en\",\n    word_count=500,\n    char_count=3000,\n    outlinks=[\"https://example.com/other\"],\n)\n</code></pre>"},{"location":"api/#site","title":"Site","text":"<pre><code>from ragcrawl.models.site import Site\n\nsite = Site(\n    site_id=\"site_abc123\",\n    name=\"Example Site\",\n    seeds=[\"https://example.com\"],\n    allowed_domains=[\"example.com\"],\n    created_at=datetime.now(timezone.utc),\n    updated_at=datetime.now(timezone.utc),\n)\n</code></pre>"},{"location":"api/#page","title":"Page","text":"<pre><code>from ragcrawl.models.page import Page\n\npage = Page(\n    page_id=\"page123\",\n    site_id=\"site_abc123\",\n    url=\"https://example.com/page\",\n    first_seen=datetime.now(timezone.utc),\n    last_seen=datetime.now(timezone.utc),\n    depth=1,\n)\n</code></pre>"},{"location":"api/#pageversion","title":"PageVersion","text":"<pre><code>from ragcrawl.models.page_version import PageVersion\n\nversion = PageVersion(\n    version_id=\"ver123\",\n    page_id=\"page123\",\n    site_id=\"site_abc123\",\n    run_id=\"run123\",\n    markdown=\"# Content\",\n    content_hash=\"abc123\",\n    url=\"https://example.com/page\",\n    status_code=200,\n    crawled_at=datetime.now(timezone.utc),\n    created_at=datetime.now(timezone.utc),\n)\n</code></pre>"},{"location":"api/#crawlrun","title":"CrawlRun","text":"<pre><code>from ragcrawl.models.crawl_run import CrawlRun, CrawlStats, RunStatus\n\nrun = CrawlRun(\n    run_id=\"run123\",\n    site_id=\"site_abc123\",\n    status=RunStatus.RUNNING,\n    created_at=datetime.now(timezone.utc),\n    stats=CrawlStats(\n        pages_crawled=50,\n        pages_failed=2,\n    ),\n)\n</code></pre>"},{"location":"api/#chunk","title":"Chunk","text":"<pre><code>from ragcrawl.models.chunk import Chunk\n\nchunk = Chunk(\n    chunk_id=\"chunk123\",\n    doc_id=\"doc123\",\n    content=\"Chunk content\",\n    chunk_index=0,\n    char_count=100,\n    token_count=25,\n    heading_path=[\"Section\", \"Subsection\"],\n)\n</code></pre>"},{"location":"api/#storage-backend","title":"Storage Backend","text":""},{"location":"api/#storagebackend-protocol","title":"StorageBackend Protocol","text":"<pre><code>from ragcrawl.storage.backend import StorageBackend\n\nclass StorageBackend(Protocol):\n    def initialize(self) -&gt; None: ...\n    def close(self) -&gt; None: ...\n\n    # Sites\n    def save_site(self, site: Site) -&gt; None: ...\n    def get_site(self, site_id: str) -&gt; Optional[Site]: ...\n    def list_sites(self) -&gt; list[Site]: ...\n\n    # Runs\n    def save_crawl_run(self, run: CrawlRun) -&gt; None: ...\n    def get_crawl_run(self, run_id: str) -&gt; Optional[CrawlRun]: ...\n    def list_runs(self, site_id: str, limit: int = 10) -&gt; list[CrawlRun]: ...\n\n    # Pages\n    def save_page(self, page: Page) -&gt; None: ...\n    def get_page(self, page_id: str) -&gt; Optional[Page]: ...\n    def get_page_by_url(self, site_id: str, url: str) -&gt; Optional[Page]: ...\n    def list_pages(self, site_id: str) -&gt; list[Page]: ...\n\n    # Versions\n    def save_page_version(self, version: PageVersion) -&gt; None: ...\n    def get_page_version(self, version_id: str) -&gt; Optional[PageVersion]: ...\n    def get_latest_version(self, page_id: str) -&gt; Optional[PageVersion]: ...\n    def list_page_versions(self, page_id: str) -&gt; list[PageVersion]: ...\n\n    # Frontier\n    def save_frontier_item(self, item: FrontierItem) -&gt; None: ...\n    def get_pending_frontier_items(self, run_id: str, limit: int) -&gt; list[FrontierItem]: ...\n    def frontier_url_exists(self, run_id: str, url: str) -&gt; bool: ...\n</code></pre>"},{"location":"api/#creating-a-backend","title":"Creating a Backend","text":"<pre><code>from ragcrawl.storage.backend import create_storage_backend\nfrom ragcrawl.config.storage_config import DuckDBConfig, StorageConfig\n\nconfig = StorageConfig(backend=DuckDBConfig(path=\"./db.duckdb\"))\nbackend = create_storage_backend(config)\nbackend.initialize()\n</code></pre>"},{"location":"api/#chunkers","title":"Chunkers","text":""},{"location":"api/#headingchunker","title":"HeadingChunker","text":"<pre><code>from ragcrawl.chunking.heading_chunker import HeadingChunker\n\nchunker = HeadingChunker(\n    min_level=1,\n    max_level=3,\n    min_chunk_chars=100,\n)\n\nchunks = chunker.chunk(markdown_content)\n</code></pre>"},{"location":"api/#tokenchunker","title":"TokenChunker","text":"<pre><code>from ragcrawl.chunking.token_chunker import TokenChunker\n\nchunker = TokenChunker(\n    max_tokens=500,\n    overlap_tokens=50,\n    encoding_name=\"cl100k_base\",\n)\n\nchunks = chunker.chunk(content)\n</code></pre>"},{"location":"api/#exporters","title":"Exporters","text":""},{"location":"api/#jsonexporter","title":"JSONExporter","text":"<pre><code>from ragcrawl.export.json_exporter import JSONExporter\n\nexporter = JSONExporter(indent=2)\nexporter.export_documents(documents, Path(\"output.json\"))\n</code></pre>"},{"location":"api/#jsonlexporter","title":"JSONLExporter","text":"<pre><code>from ragcrawl.export.json_exporter import JSONLExporter\n\nexporter = JSONLExporter()\nexporter.export_documents(documents, Path(\"output.jsonl\"))\n</code></pre>"},{"location":"api/#publishers","title":"Publishers","text":""},{"location":"api/#singlepagepublisher","title":"SinglePagePublisher","text":"<pre><code>from ragcrawl.output.single_page import SinglePagePublisher\n\npublisher = SinglePagePublisher(output_config)\nfiles = publisher.publish(documents)\n</code></pre>"},{"location":"api/#multipagepublisher","title":"MultiPagePublisher","text":"<pre><code>from ragcrawl.output.multi_page import MultiPagePublisher\n\npublisher = MultiPagePublisher(output_config)\nfiles = publisher.publish(documents)\n</code></pre>"},{"location":"api/#utilities","title":"Utilities","text":""},{"location":"api/#url-normalizer","title":"URL Normalizer","text":"<pre><code>from ragcrawl.filters.url_normalizer import URLNormalizer\n\nnormalizer = URLNormalizer()\nnormalized = normalizer.normalize(url)\ndomain = normalizer.extract_domain(url)\nurl_hash = normalizer.hash_url(url)\n</code></pre>"},{"location":"api/#link-filter","title":"Link Filter","text":"<pre><code>from ragcrawl.filters.link_filter import LinkFilter\n\nfilter = LinkFilter(\n    allowed_domains=[\"example.com\"],\n    include_patterns=[r\"/docs/.*\"],\n    exclude_patterns=[r\"/admin/.*\"],\n)\n\nif filter.should_follow(url):\n    # Crawl the URL\n    pass\n</code></pre>"},{"location":"api/#hashing","title":"Hashing","text":"<pre><code>from ragcrawl.utils.hashing import (\n    compute_doc_id,\n    compute_content_hash,\n    compute_url_hash,\n)\n\ndoc_id = compute_doc_id(url)\ncontent_hash = compute_content_hash(content)\nurl_hash = compute_url_hash(url)\n</code></pre>"},{"location":"configuration/crawler/","title":"Crawler Configuration","text":"<p>Complete reference for <code>CrawlerConfig</code>.</p>"},{"location":"configuration/crawler/#basic-configuration","title":"Basic Configuration","text":"<pre><code>from ragcrawl.config.crawler_config import CrawlerConfig\n\nconfig = CrawlerConfig(\n    seeds=[\"https://docs.example.com\"],\n    max_pages=100,\n    max_depth=5,\n)\n</code></pre>"},{"location":"configuration/crawler/#all-options","title":"All Options","text":""},{"location":"configuration/crawler/#seeds-and-scope","title":"Seeds and Scope","text":"Option Type Default Description <code>seeds</code> <code>list[str]</code> Required Starting URLs for crawl <code>allowed_domains</code> <code>list[str]</code> Auto Domains to crawl (defaults to seed domains) <code>allow_subdomains</code> <code>bool</code> <code>True</code> Also crawl subdomains of allowed domains"},{"location":"configuration/crawler/#limits","title":"Limits","text":"Option Type Default Description <code>max_pages</code> <code>int</code> <code>100</code> Maximum total pages to crawl <code>max_depth</code> <code>int</code> <code>5</code> Maximum depth from seed URLs <code>max_pages_per_domain</code> <code>int</code> <code>None</code> Maximum pages per domain"},{"location":"configuration/crawler/#url-filtering","title":"URL Filtering","text":"Option Type Default Description <code>include_patterns</code> <code>list[str]</code> <code>[]</code> Regex patterns URLs must match <code>exclude_patterns</code> <code>list[str]</code> <code>[]</code> Regex patterns to skip <code>skip_extensions</code> <code>list[str]</code> Binary File extensions to skip"},{"location":"configuration/crawler/#fetching","title":"Fetching","text":"Option Type Default Description <code>fetch_mode</code> <code>FetchMode</code> <code>HTTP</code> HTTP, BROWSER, or HYBRID <code>timeout</code> <code>float</code> <code>30.0</code> Request timeout in seconds <code>user_agent</code> <code>str</code> Default Custom user agent string"},{"location":"configuration/crawler/#rate-limiting","title":"Rate Limiting","text":"Option Type Default Description <code>requests_per_second</code> <code>float</code> <code>2.0</code> Max requests per second per domain <code>concurrent_requests</code> <code>int</code> <code>10</code> Max concurrent requests <code>delay_range</code> <code>tuple[float, float]</code> <code>None</code> Random delay range (min, max)"},{"location":"configuration/crawler/#robotstxt","title":"Robots.txt","text":"Option Type Default Description <code>robots_mode</code> <code>RobotsMode</code> <code>STRICT</code> STRICT, OFF, or ALLOWLIST <code>robots_allowlist</code> <code>list[str]</code> <code>[]</code> Paths to allow despite robots.txt"},{"location":"configuration/crawler/#error-handling","title":"Error Handling","text":"Option Type Default Description <code>max_retries</code> <code>int</code> <code>3</code> Maximum retry attempts <code>retry_delay</code> <code>float</code> <code>1.0</code> Delay between retries <code>circuit_breaker_threshold</code> <code>int</code> <code>10</code> Failures before circuit break <code>circuit_breaker_reset</code> <code>float</code> <code>300.0</code> Seconds before circuit reset"},{"location":"configuration/crawler/#fetchmode-enum","title":"FetchMode Enum","text":"<pre><code>from ragcrawl.config.crawler_config import FetchMode\n\nFetchMode.HTTP     # HTTP requests only (fastest)\nFetchMode.BROWSER  # Headless browser (for JS sites)\nFetchMode.HYBRID   # Try HTTP first, fall back to browser\n</code></pre>"},{"location":"configuration/crawler/#robotsmode-enum","title":"RobotsMode Enum","text":"<pre><code>from ragcrawl.config.crawler_config import RobotsMode\n\nRobotsMode.STRICT     # Respect all robots.txt rules\nRobotsMode.OFF        # Ignore robots.txt\nRobotsMode.ALLOWLIST  # Respect except for allowlisted paths\n</code></pre>"},{"location":"configuration/crawler/#complete-example","title":"Complete Example","text":"<pre><code>from ragcrawl.config.crawler_config import (\n    CrawlerConfig,\n    FetchMode,\n    RobotsMode,\n)\nfrom ragcrawl.config.storage_config import DuckDBConfig, StorageConfig\nfrom ragcrawl.config.output_config import OutputConfig, OutputMode\n\nconfig = CrawlerConfig(\n    # Seeds and scope\n    seeds=[\"https://docs.example.com\", \"https://api.example.com\"],\n    allowed_domains=[\"docs.example.com\", \"api.example.com\"],\n    allow_subdomains=True,\n\n    # Limits\n    max_pages=1000,\n    max_depth=10,\n    max_pages_per_domain=500,\n\n    # URL filtering\n    include_patterns=[\n        r\"/docs/.*\",\n        r\"/api/v\\d+/.*\",\n    ],\n    exclude_patterns=[\n        r\"/admin/.*\",\n        r\"/internal/.*\",\n        r\".*\\.(pdf|zip|exe)$\",\n    ],\n\n    # Fetching\n    fetch_mode=FetchMode.HTTP,\n    timeout=30.0,\n    user_agent=\"MyBot/1.0\",\n\n    # Rate limiting\n    requests_per_second=2.0,\n    concurrent_requests=5,\n    delay_range=(0.5, 1.5),\n\n    # Robots\n    robots_mode=RobotsMode.STRICT,\n\n    # Error handling\n    max_retries=3,\n    retry_delay=2.0,\n    circuit_breaker_threshold=10,\n\n    # Storage\n    storage=StorageConfig(\n        backend=DuckDBConfig(path=\"./crawler.duckdb\")\n    ),\n\n    # Output\n    output=OutputConfig(\n        mode=OutputMode.MULTI,\n        root_dir=\"./output\",\n    ),\n)\n</code></pre>"},{"location":"configuration/crawler/#environment-variables","title":"Environment Variables","text":"<p>Some options can be set via environment variables:</p> <pre><code>export RAGCRAWL_USER_AGENT=\"MyBot/1.0\"\nexport RAGCRAWL_TIMEOUT=60\nexport RAGCRAWL_MAX_RETRIES=5\n</code></pre>"},{"location":"configuration/output/","title":"Output Configuration","text":"<p>Configure how crawled content is published.</p>"},{"location":"configuration/output/#overview","title":"Overview","text":"<p>ragcrawl can output content in two modes:</p> <ul> <li>Multi-page: Each page becomes a separate Markdown file</li> <li>Single-page: All content combined into one file</li> </ul>"},{"location":"configuration/output/#outputconfig","title":"OutputConfig","text":"<pre><code>from ragcrawl.config.output_config import OutputConfig, OutputMode\n\nconfig = OutputConfig(\n    mode=OutputMode.MULTI,\n    root_dir=\"./output\",\n)\n</code></pre>"},{"location":"configuration/output/#options","title":"Options","text":"Option Type Default Description <code>mode</code> <code>OutputMode</code> <code>MULTI</code> SINGLE or MULTI <code>root_dir</code> <code>str</code> <code>\"./output\"</code> Output directory <code>include_metadata</code> <code>bool</code> <code>True</code> Include source URLs in output <code>include_toc</code> <code>bool</code> <code>True</code> Include table of contents (SINGLE mode) <code>rewrite_links</code> <code>bool</code> <code>True</code> Rewrite internal links (MULTI mode) <code>single_file_name</code> <code>str</code> <code>\"knowledge_base.md\"</code> Output filename (SINGLE mode) <code>generate_index</code> <code>bool</code> <code>True</code> Generate index.md (MULTI mode)"},{"location":"configuration/output/#multi-page-output","title":"Multi-Page Output","text":"<p>Each crawled page becomes a separate Markdown file:</p> <pre><code>config = OutputConfig(\n    mode=OutputMode.MULTI,\n    root_dir=\"./docs-output\",\n    include_metadata=True,\n    rewrite_links=True,\n    generate_index=True,\n)\n</code></pre>"},{"location":"configuration/output/#output-structure","title":"Output Structure","text":"<pre><code>docs-output/\n\u251c\u2500\u2500 index.md                 # Generated index\n\u251c\u2500\u2500 example.com/\n\u2502   \u251c\u2500\u2500 index.md\n\u2502   \u251c\u2500\u2500 docs/\n\u2502   \u2502   \u251c\u2500\u2500 getting-started.md\n\u2502   \u2502   \u251c\u2500\u2500 installation.md\n\u2502   \u2502   \u2514\u2500\u2500 api/\n\u2502   \u2502       \u251c\u2500\u2500 overview.md\n\u2502   \u2502       \u2514\u2500\u2500 reference.md\n\u2502   \u2514\u2500\u2500 blog/\n\u2502       \u251c\u2500\u2500 post-1.md\n\u2502       \u2514\u2500\u2500 post-2.md\n</code></pre>"},{"location":"configuration/output/#link-rewriting","title":"Link Rewriting","text":"<p>Internal links are automatically converted:</p> <p>Original HTML: <pre><code>&lt;a href=\"/docs/api/overview\"&gt;API Overview&lt;/a&gt;\n</code></pre></p> <p>Output Markdown: <pre><code>[API Overview](./api/overview.md)\n</code></pre></p>"},{"location":"configuration/output/#metadata-headers","title":"Metadata Headers","text":"<p>Each file includes source information:</p> <pre><code>&lt;!-- Source: https://example.com/docs/getting-started --&gt;\n&lt;!-- Crawled: 2024-01-15T10:30:00Z --&gt;\n\n# Getting Started\n\nContent here...\n</code></pre>"},{"location":"configuration/output/#single-page-output","title":"Single-Page Output","text":"<p>All content combined into one file:</p> <pre><code>config = OutputConfig(\n    mode=OutputMode.SINGLE,\n    root_dir=\"./output\",\n    single_file_name=\"knowledge_base.md\",\n    include_toc=True,\n    include_metadata=True,\n)\n</code></pre>"},{"location":"configuration/output/#output-structure_1","title":"Output Structure","text":"<pre><code>output/\n\u2514\u2500\u2500 knowledge_base.md\n</code></pre>"},{"location":"configuration/output/#file-format","title":"File Format","text":"<pre><code># Knowledge Base\n\nGenerated from https://docs.example.com\n\n## Table of Contents\n\n- [Getting Started](#getting-started)\n- [Installation](#installation)\n- [Configuration](#configuration)\n\n---\n\n## Getting Started\n\n&lt;!-- Source: https://example.com/docs/getting-started --&gt;\n\nContent here...\n\n---\n\n## Installation\n\n&lt;!-- Source: https://example.com/docs/installation --&gt;\n\nContent here...\n</code></pre>"},{"location":"configuration/output/#custom-publishers","title":"Custom Publishers","text":"<p>Create custom output formats:</p> <pre><code>from ragcrawl.output.publisher import MarkdownPublisher\nfrom ragcrawl.config.output_config import OutputConfig\nfrom pathlib import Path\n\nclass MyCustomPublisher(MarkdownPublisher):\n    def publish(self, documents: list) -&gt; list[Path]:\n        output_files = []\n\n        for doc in documents:\n            # Custom processing\n            content = self.format_document(doc)\n\n            # Custom filename\n            filename = f\"{doc.doc_id}.md\"\n            path = Path(self.config.root_dir) / filename\n\n            path.parent.mkdir(parents=True, exist_ok=True)\n            path.write_text(content)\n            output_files.append(path)\n\n        return output_files\n\n    def format_document(self, doc):\n        return f\"\"\"---\ntitle: {doc.title}\nurl: {doc.url}\ndate: {doc.fetched_at.isoformat()}\n---\n\n{doc.content}\n\"\"\"\n</code></pre>"},{"location":"configuration/output/#cli-options","title":"CLI Options","text":"<pre><code># Multi-page output (default)\nragcrawl crawl https://example.com --output ./output --output-mode multi\n\n# Single-page output\nragcrawl crawl https://example.com --output ./output --output-mode single\n</code></pre>"},{"location":"configuration/output/#best-practices","title":"Best Practices","text":"<ol> <li>Multi-page for large sites: Easier to navigate and search</li> <li>Single-page for LLM context: One file for full-text RAG</li> <li>Enable link rewriting: Keeps navigation working offline</li> <li>Include metadata: Helps trace content back to sources</li> <li>Use consistent structure: Match the site's URL hierarchy</li> </ol>"},{"location":"configuration/storage/","title":"Storage Configuration","text":"<p>Configure where crawl data is stored.</p>"},{"location":"configuration/storage/#overview","title":"Overview","text":"<p>ragcrawl supports two storage backends:</p> <ul> <li>DuckDB (default): Local file-based storage, great for development and single-machine deployments</li> <li>DynamoDB: AWS cloud storage, suitable for distributed and serverless deployments</li> </ul>"},{"location":"configuration/storage/#duckdb-configuration","title":"DuckDB Configuration","text":""},{"location":"configuration/storage/#basic-setup","title":"Basic Setup","text":"<pre><code>from ragcrawl.config.storage_config import DuckDBConfig, StorageConfig\n\nstorage = StorageConfig(\n    backend=DuckDBConfig(path=\"./crawler.duckdb\")\n)\n</code></pre>"},{"location":"configuration/storage/#options","title":"Options","text":"Option Type Default Description <code>path</code> <code>str</code> <code>\"./crawler.duckdb\"</code> Path to database file <code>read_only</code> <code>bool</code> <code>False</code> Open in read-only mode"},{"location":"configuration/storage/#cli-usage","title":"CLI Usage","text":"<pre><code># Default storage\nragcrawl crawl https://example.com\n\n# Custom storage path\nragcrawl crawl https://example.com --storage ./data/my-crawl.duckdb\n</code></pre>"},{"location":"configuration/storage/#multiple-databases","title":"Multiple Databases","text":"<pre><code># Separate databases for different projects\nproject_a = StorageConfig(backend=DuckDBConfig(path=\"./project-a.duckdb\"))\nproject_b = StorageConfig(backend=DuckDBConfig(path=\"./project-b.duckdb\"))\n</code></pre>"},{"location":"configuration/storage/#dynamodb-configuration","title":"DynamoDB Configuration","text":""},{"location":"configuration/storage/#prerequisites","title":"Prerequisites","text":"<ol> <li> <p>Install DynamoDB support:    <pre><code>pip install ragcrawl[dynamodb]\n</code></pre></p> </li> <li> <p>Configure AWS credentials:    <pre><code>export AWS_ACCESS_KEY_ID=your_key\nexport AWS_SECRET_ACCESS_KEY=your_secret\nexport AWS_DEFAULT_REGION=us-east-1\n</code></pre></p> </li> </ol>"},{"location":"configuration/storage/#basic-setup_1","title":"Basic Setup","text":"<pre><code>from ragcrawl.config.storage_config import DynamoDBConfig, StorageConfig\n\nstorage = StorageConfig(\n    backend=DynamoDBConfig(\n        table_prefix=\"myapp\",\n        region=\"us-east-1\",\n    )\n)\n</code></pre>"},{"location":"configuration/storage/#options_1","title":"Options","text":"Option Type Default Description <code>table_prefix</code> <code>str</code> <code>\"ragcrawl\"</code> Prefix for table names <code>region</code> <code>str</code> <code>\"us-east-1\"</code> AWS region <code>endpoint_url</code> <code>str</code> <code>None</code> Custom endpoint (for local testing) <code>create_tables</code> <code>bool</code> <code>True</code> Auto-create tables if missing"},{"location":"configuration/storage/#table-structure","title":"Table Structure","text":"<p>The following tables are created:</p> <ul> <li><code>{prefix}-sites</code>: Site configurations</li> <li><code>{prefix}-runs</code>: Crawl run records</li> <li><code>{prefix}-pages</code>: Page metadata</li> <li><code>{prefix}-versions</code>: Page versions with content</li> <li><code>{prefix}-frontier</code>: Crawl queue items</li> </ul>"},{"location":"configuration/storage/#local-development-with-dynamodb-local","title":"Local Development with DynamoDB Local","text":"<pre><code>storage = StorageConfig(\n    backend=DynamoDBConfig(\n        table_prefix=\"dev\",\n        region=\"us-east-1\",\n        endpoint_url=\"http://localhost:8000\",  # DynamoDB Local\n    )\n)\n</code></pre> <p>Run DynamoDB Local: <pre><code>docker run -p 8000:8000 amazon/dynamodb-local\n</code></pre></p>"},{"location":"configuration/storage/#fallback-configuration","title":"Fallback Configuration","text":"<p>Configure fallback when DynamoDB is unavailable:</p> <pre><code>storage = StorageConfig(\n    backend=DynamoDBConfig(\n        table_prefix=\"prod\",\n        region=\"us-east-1\",\n    ),\n    fallback=DuckDBConfig(path=\"./fallback.duckdb\"),\n    fail_if_unavailable=False,  # Use fallback instead of failing\n)\n</code></pre>"},{"location":"configuration/storage/#accessing-storage-directly","title":"Accessing Storage Directly","text":"<pre><code>from ragcrawl.storage.backend import create_storage_backend\nfrom ragcrawl.config.storage_config import DuckDBConfig, StorageConfig\n\n# Create backend\nconfig = StorageConfig(backend=DuckDBConfig(path=\"./crawler.duckdb\"))\nbackend = create_storage_backend(config)\nbackend.initialize()\n\n# Query data\nsites = backend.list_sites()\nfor site in sites:\n    print(f\"Site: {site.name}\")\n    pages = backend.list_pages(site.site_id)\n    print(f\"  Pages: {len(pages)}\")\n\n# Clean up\nbackend.close()\n</code></pre>"},{"location":"configuration/storage/#data-schema","title":"Data Schema","text":""},{"location":"configuration/storage/#sites-table","title":"Sites Table","text":"Column Type Description site_id string Unique identifier name string Site name seeds json List of seed URLs allowed_domains json Allowed domains config json Crawler configuration created_at datetime Creation timestamp total_pages integer Total pages crawled total_runs integer Total crawl runs"},{"location":"configuration/storage/#pages-table","title":"Pages Table","text":"Column Type Description page_id string Unique identifier site_id string Parent site url string Page URL content_hash string Current content hash first_seen datetime First crawl time last_crawled datetime Last crawl time is_tombstone boolean Deleted page marker"},{"location":"configuration/storage/#versions-table","title":"Versions Table","text":"Column Type Description version_id string Unique identifier page_id string Parent page run_id string Crawl run markdown text Markdown content content_hash string Content hash title string Page title crawled_at datetime Crawl timestamp"},{"location":"configuration/storage/#best-practices","title":"Best Practices","text":"<ol> <li>Development: Use DuckDB for fast local iteration</li> <li>Production: Use DynamoDB for scalability and durability</li> <li>Testing: Use DynamoDB Local or in-memory DuckDB</li> <li>Backups: DuckDB files can be copied; DynamoDB has built-in backups</li> <li>Migration: Export from one backend, import to another via JSON export</li> </ol>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.10 or higher</li> <li>pip or uv package manager</li> </ul>"},{"location":"getting-started/installation/#installation-options","title":"Installation Options","text":""},{"location":"getting-started/installation/#basic-installation","title":"Basic Installation","text":"<p>The basic installation includes DuckDB storage and HTTP-only fetching:</p> <pre><code>pip install ragcrawl\n</code></pre> <p>Or with uv:</p> <pre><code>uv pip install ragcrawl\n</code></pre>"},{"location":"getting-started/installation/#browser-rendering-support","title":"Browser Rendering Support","text":"<p>For JavaScript-heavy sites, install with browser support:</p> <pre><code>pip install ragcrawl[browser]\n</code></pre> <p>This installs Playwright for headless browser rendering.</p> <p>After installation, set up Playwright:</p> <pre><code>playwright install chromium\n</code></pre>"},{"location":"getting-started/installation/#dynamodb-support","title":"DynamoDB Support","text":"<p>For cloud deployments with AWS DynamoDB:</p> <pre><code>pip install ragcrawl[dynamodb]\n</code></pre>"},{"location":"getting-started/installation/#full-installation","title":"Full Installation","text":"<p>Install all optional dependencies:</p> <pre><code>pip install ragcrawl[all]\n</code></pre>"},{"location":"getting-started/installation/#development-installation","title":"Development Installation","text":"<p>For contributing to the project:</p> <pre><code>git clone https://github.com/your-org/ragcrawl.git\ncd ragcrawl\npip install -e \".[dev]\"\n</code></pre>"},{"location":"getting-started/installation/#verifying-installation","title":"Verifying Installation","text":"<p>Test your installation:</p> <pre><code># Check CLI is available\nragcrawl --version\n\n# Run a simple crawl\nragcrawl crawl https://example.com --max-pages 5 --output ./test-output\n</code></pre>"},{"location":"getting-started/installation/#dependencies","title":"Dependencies","text":""},{"location":"getting-started/installation/#core-dependencies","title":"Core Dependencies","text":"Package Purpose crawl4ai Web fetching and HTML-to-Markdown conversion duckdb Default local storage backend httpx Async HTTP client pydantic Data validation and configuration structlog Structured logging xxhash Fast content hashing tiktoken Token counting for chunking click Command-line interface"},{"location":"getting-started/installation/#optional-dependencies","title":"Optional Dependencies","text":"Package Purpose Extra playwright Browser rendering <code>[browser]</code> pynamodb DynamoDB ORM <code>[dynamodb]</code>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/installation/#playwright-installation-issues","title":"Playwright Installation Issues","text":"<p>If you encounter issues with Playwright:</p> <pre><code># Install system dependencies (Ubuntu/Debian)\nsudo apt-get install libnss3 libatk1.0-0 libatk-bridge2.0-0 libcups2 libdrm2 libxkbcommon0 libxcomposite1 libxdamage1 libxfixes3 libxrandr2 libgbm1 libasound2\n\n# Install browsers\nplaywright install chromium\n</code></pre>"},{"location":"getting-started/installation/#duckdb-permission-issues","title":"DuckDB Permission Issues","text":"<p>Ensure the storage directory is writable:</p> <pre><code># Create with proper permissions\nmkdir -p ./data\nchmod 755 ./data\nragcrawl crawl https://example.com --storage ./data/crawler.duckdb\n</code></pre>"},{"location":"getting-started/installation/#aws-credentials-for-dynamodb","title":"AWS Credentials for DynamoDB","text":"<p>Set up AWS credentials for DynamoDB:</p> <pre><code>export AWS_ACCESS_KEY_ID=your_key\nexport AWS_SECRET_ACCESS_KEY=your_secret\nexport AWS_DEFAULT_REGION=us-east-1\n</code></pre> <p>Or use AWS profiles:</p> <pre><code>aws configure --profile crawler\nexport AWS_PROFILE=crawler\n</code></pre>"},{"location":"getting-started/quickstart/","title":"Quick Start Guide","text":"<p>This guide will help you crawl your first website in minutes.</p>"},{"location":"getting-started/quickstart/#your-first-crawl","title":"Your First Crawl","text":""},{"location":"getting-started/quickstart/#using-the-cli","title":"Using the CLI","text":"<p>The simplest way to crawl a website:</p> <pre><code>ragcrawl crawl https://docs.example.com\n</code></pre> <p>This will: - Crawl up to 100 pages (default) - Save to <code>./output</code> directory - Store crawl data in <code>~/.ragcrawl/ragcrawl.duckdb</code></p>"},{"location":"getting-started/quickstart/#customizing-the-crawl","title":"Customizing the Crawl","text":"<pre><code>ragcrawl crawl https://docs.example.com \\\n    --max-pages 500 \\\n    --max-depth 10 \\\n    --output ./knowledge-base \\\n    --output-mode single \\\n    --export-json ./export.json \\\n    --verbose\n</code></pre>"},{"location":"getting-started/quickstart/#cli-options-reference","title":"CLI Options Reference","text":"Option Short Description <code>--max-pages</code> <code>-m</code> Maximum pages to crawl <code>--max-depth</code> <code>-d</code> Maximum crawl depth <code>--output</code> <code>-o</code> Output directory <code>--output-mode</code> <code>single</code> or <code>multi</code> page output <code>--storage</code> <code>-s</code> DuckDB storage path <code>--include</code> <code>-i</code> Include URL patterns (regex) <code>--exclude</code> <code>-e</code> Exclude URL patterns (regex) <code>--robots/--no-robots</code> Respect robots.txt <code>--js/--no-js</code> Enable JavaScript rendering <code>--export-json</code> Export to JSON file <code>--export-jsonl</code> Export to JSONL file <code>--verbose</code> <code>-v</code> Verbose output"},{"location":"getting-started/quickstart/#using-the-python-api","title":"Using the Python API","text":"<pre><code>import asyncio\nfrom ragcrawl.config.crawler_config import CrawlerConfig\nfrom ragcrawl.config.output_config import OutputConfig, OutputMode\nfrom ragcrawl.core.crawl_job import CrawlJob\n\nasync def main():\n    config = CrawlerConfig(\n        seeds=[\"https://docs.example.com\"],\n        max_pages=100,\n        max_depth=5,\n        output=OutputConfig(\n            mode=OutputMode.MULTI,\n            root_dir=\"./output\",\n        ),\n    )\n\n    job = CrawlJob(config)\n    result = await job.run()\n\n    if result.success:\n        print(f\"\u2713 Crawled {result.stats.pages_crawled} pages\")\n        print(f\"\u2713 Duration: {result.duration_seconds:.1f}s\")\n\n        # Access documents\n        for doc in result.documents[:5]:\n            print(f\"  - {doc.title}: {doc.url}\")\n    else:\n        print(f\"\u2717 Error: {result.error}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"getting-started/quickstart/#output-formats","title":"Output Formats","text":""},{"location":"getting-started/quickstart/#multi-page-output-default","title":"Multi-Page Output (Default)","text":"<p>Each page becomes a separate Markdown file, preserving the site structure:</p> <pre><code>output/\n\u251c\u2500\u2500 example.com/\n\u2502   \u251c\u2500\u2500 index.md\n\u2502   \u251c\u2500\u2500 docs/\n\u2502   \u2502   \u251c\u2500\u2500 getting-started.md\n\u2502   \u2502   \u251c\u2500\u2500 configuration.md\n\u2502   \u2502   \u2514\u2500\u2500 api/\n\u2502   \u2502       \u251c\u2500\u2500 overview.md\n\u2502   \u2502       \u2514\u2500\u2500 reference.md\n\u2502   \u2514\u2500\u2500 blog/\n\u2502       \u251c\u2500\u2500 post-1.md\n\u2502       \u2514\u2500\u2500 post-2.md\n\u2514\u2500\u2500 index.md\n</code></pre>"},{"location":"getting-started/quickstart/#single-page-output","title":"Single-Page Output","text":"<p>All content combined into one file with a table of contents:</p> <pre><code>ragcrawl crawl https://docs.example.com --output-mode single\n</code></pre> <p>Output: <pre><code>output/\n\u2514\u2500\u2500 knowledge_base.md\n</code></pre></p>"},{"location":"getting-started/quickstart/#filtering-urls","title":"Filtering URLs","text":""},{"location":"getting-started/quickstart/#include-only-specific-paths","title":"Include Only Specific Paths","text":"<pre><code>ragcrawl crawl https://example.com \\\n    --include \"/docs/.*\" \\\n    --include \"/api/.*\"\n</code></pre>"},{"location":"getting-started/quickstart/#exclude-paths","title":"Exclude Paths","text":"<pre><code>ragcrawl crawl https://example.com \\\n    --exclude \"/admin/.*\" \\\n    --exclude \"/private/.*\"\n</code></pre>"},{"location":"getting-started/quickstart/#exporting-data","title":"Exporting Data","text":""},{"location":"getting-started/quickstart/#json-export","title":"JSON Export","text":"<pre><code>ragcrawl crawl https://example.com --export-json ./docs.json\n</code></pre>"},{"location":"getting-started/quickstart/#jsonl-export-streaming","title":"JSONL Export (Streaming)","text":"<pre><code>ragcrawl crawl https://example.com --export-jsonl ./docs.jsonl\n</code></pre>"},{"location":"getting-started/quickstart/#incremental-sync","title":"Incremental Sync","text":"<p>After the initial crawl, sync to get only changes:</p> <pre><code># First, find your site ID\nragcrawl sites\n\n# Then sync\nragcrawl sync site_abc123 --output ./updates\n</code></pre>"},{"location":"getting-started/quickstart/#sync-options","title":"Sync Options","text":"Option Short Description <code>--storage</code> <code>-s</code> DuckDB storage path <code>--max-pages</code> <code>-m</code> Maximum pages to sync <code>--max-age</code> Only check pages older than N hours <code>--output</code> <code>-o</code> Output directory for updates <code>--verbose</code> <code>-v</code> Verbose output"},{"location":"getting-started/quickstart/#managing-crawls","title":"Managing Crawls","text":""},{"location":"getting-started/quickstart/#list-sites-and-runs","title":"List Sites and Runs","text":"<pre><code># List all crawled sites\nragcrawl sites\n\n# List all crawl runs\nragcrawl list\n\n# List runs for a specific site\nragcrawl runs site_abc123\n\n# Filter runs by status\nragcrawl list --status completed\nragcrawl list --status running\n</code></pre>"},{"location":"getting-started/quickstart/#configuration-management","title":"Configuration Management","text":"<pre><code># Show current configuration\nragcrawl config show\n\n# Show config file path\nragcrawl config path\n\n# Set configuration values\nragcrawl config set storage_dir ~/.ragcrawl\nragcrawl config set user_agent \"MyBot/1.0\"\nragcrawl config set timeout 30\n\n# Reset to defaults\nragcrawl config reset\n</code></pre>"},{"location":"getting-started/quickstart/#common-workflows","title":"Common Workflows","text":""},{"location":"getting-started/quickstart/#documentation-site","title":"Documentation Site","text":"<pre><code>ragcrawl crawl https://docs.myproject.com \\\n    --max-pages 1000 \\\n    --include \"/docs/.*\" \\\n    --output ./project-docs \\\n    --output-mode multi\n</code></pre>"},{"location":"getting-started/quickstart/#blog-archive","title":"Blog Archive","text":"<pre><code>ragcrawl crawl https://blog.example.com \\\n    --max-pages 500 \\\n    --include \"/posts/.*\" \\\n    --output-mode single \\\n    --export-jsonl ./blog-posts.jsonl\n</code></pre>"},{"location":"getting-started/quickstart/#api-documentation","title":"API Documentation","text":"<pre><code>ragcrawl crawl https://api.example.com/docs \\\n    --js \\\n    --max-pages 200 \\\n    --output ./api-docs\n</code></pre>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Crawling Guide - Advanced crawling options</li> <li>Syncing Guide - Keep your knowledge base updated</li> <li>Configuration - Full configuration reference</li> </ul>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>ragcrawl<ul> <li>chunking<ul> <li>chunker</li> <li>heading_chunker</li> <li>token_chunker</li> </ul> </li> <li>cli<ul> <li>config_tui</li> <li>main</li> </ul> </li> <li>config<ul> <li>crawler_config</li> <li>output_config</li> <li>storage_config</li> <li>sync_config</li> <li>user_config</li> </ul> </li> <li>core<ul> <li>crawl_job</li> <li>frontier</li> <li>scheduler</li> <li>sync_job</li> </ul> </li> <li>export<ul> <li>events</li> <li>exporter</li> <li>json_exporter</li> </ul> </li> <li>extraction<ul> <li>extractor</li> <li>link_extractor</li> <li>metadata</li> </ul> </li> <li>fetcher<ul> <li>base</li> <li>crawl4ai_fetcher</li> <li>revalidation</li> <li>robots</li> </ul> </li> <li>filters<ul> <li>link_filter</li> <li>patterns</li> <li>quality_gates</li> <li>url_normalizer</li> </ul> </li> <li>hooks<ul> <li>callbacks</li> </ul> </li> <li>models<ul> <li>chunk</li> <li>crawl_run</li> <li>document</li> <li>frontier_item</li> <li>page</li> <li>page_version</li> <li>site</li> </ul> </li> <li>output<ul> <li>link_rewriter</li> <li>multi_page</li> <li>navigation</li> <li>publisher</li> <li>single_page</li> </ul> </li> <li>storage<ul> <li>backend</li> <li>duckdb<ul> <li>backend</li> <li>schema</li> </ul> </li> <li>dynamodb<ul> <li>backend</li> <li>models</li> </ul> </li> </ul> </li> <li>sync<ul> <li>change_detector</li> <li>sitemap_parser</li> <li>strategies</li> </ul> </li> <li>utils<ul> <li>hashing</li> <li>logging</li> <li>metrics</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/ragcrawl/","title":"Index","text":""},{"location":"reference/ragcrawl/#ragcrawl","title":"<code>ragcrawl</code>","text":"<p>ragcrawl - Recursive website crawler producing LLM-ready knowledge base artifacts.</p>"},{"location":"reference/ragcrawl/#ragcrawl.Chunk","title":"<code>Chunk</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a chunk of content for RAG/embedding pipelines.</p> <p>Chunks are segments of a document optimized for vector embedding and retrieval, with metadata for context reconstruction.</p>"},{"location":"reference/ragcrawl/#ragcrawl.Chunk.is_first","title":"<code>is_first</code>  <code>property</code>","text":"<p>Check if this is the first chunk.</p>"},{"location":"reference/ragcrawl/#ragcrawl.Chunk.is_last","title":"<code>is_last</code>  <code>property</code>","text":"<p>Check if this is the last chunk.</p>"},{"location":"reference/ragcrawl/#ragcrawl.CrawlJob","title":"<code>CrawlJob(config)</code>","text":"<p>Main crawl job orchestrator.</p> <p>Coordinates the frontier, fetcher, extractor, and storage to perform a complete crawl.</p> <p>Initialize a crawl job.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>CrawlerConfig</code> <p>Crawler configuration.</p> required Source code in <code>src/ragcrawl/core/crawl_job.py</code> <pre><code>def __init__(self, config: CrawlerConfig) -&gt; None:\n    \"\"\"\n    Initialize a crawl job.\n\n    Args:\n        config: Crawler configuration.\n    \"\"\"\n    self.config = config\n\n    # Generate IDs\n    self.site_id = config.site_id or generate_site_id(config.seeds)\n    self.run_id = generate_run_id()\n\n    # Initialize components (lazy)\n    self._storage: StorageBackend | None = None\n    self._fetcher: Crawl4AIFetcher | None = None\n    self._robots: RobotsChecker | None = None\n    self._frontier: Frontier | None = None\n    self._scheduler: DomainScheduler | None = None\n    self._extractor: ContentExtractor | None = None\n    self._quality_gate: QualityGate | None = None\n    self._link_filter: LinkFilter | None = None\n\n    # Tracking\n    self._metrics = MetricsCollector()\n    self._logger = CrawlLoggerAdapter(self.run_id, self.site_id)\n    self._crawl_run: CrawlRun | None = None\n    self._documents: list[Document] = []\n</code></pre>"},{"location":"reference/ragcrawl/#ragcrawl.CrawlJob.run","title":"<code>run()</code>  <code>async</code>","text":"<p>Execute the crawl job.</p> <p>Returns:</p> Type Description <code>CrawlResult</code> <p>CrawlResult with statistics and documents.</p> Source code in <code>src/ragcrawl/core/crawl_job.py</code> <pre><code>async def run(self) -&gt; CrawlResult:\n    \"\"\"\n    Execute the crawl job.\n\n    Returns:\n        CrawlResult with statistics and documents.\n    \"\"\"\n    start_time = datetime.now()\n\n    try:\n        # Initialize\n        self._init_components()\n\n        # Create/update site record\n        await self._save_site()\n\n        # Create crawl run record\n        self._crawl_run = CrawlRun(\n            run_id=self.run_id,\n            site_id=self.site_id,\n            config_snapshot=self.config.model_dump(exclude={\"on_page\", \"on_error\", \"on_change_detected\", \"redaction_hook\"}),\n            seeds=self.config.seeds,\n        )\n        self._crawl_run.mark_started()\n        self._storage.save_run(self._crawl_run)\n\n        self._logger.run_started(\n            self.config.seeds,\n            {\"max_pages\": self.config.max_pages, \"max_depth\": self.config.max_depth},\n        )\n\n        # Add seeds to frontier\n        await self._frontier.add_seeds(self.config.seeds)\n\n        # Main crawl loop\n        await self._crawl_loop()\n\n        # Finalize\n        metrics = self._metrics.finalize()\n        self._crawl_run.stats = CrawlStats(\n            pages_discovered=metrics.pages_discovered,\n            pages_crawled=metrics.pages_crawled,\n            pages_failed=metrics.pages_failed,\n            pages_skipped=metrics.pages_skipped,\n            pages_changed=metrics.pages_changed,\n            pages_new=metrics.pages_new,\n            total_bytes_downloaded=metrics.total_bytes,\n            total_fetch_time_ms=metrics.total_fetch_time_ms,\n            total_extraction_time_ms=metrics.total_extraction_time_ms,\n            avg_fetch_latency_ms=metrics.avg_fetch_latency_ms,\n            status_codes=dict(metrics.status_codes),\n            errors_by_type=dict(metrics.errors_by_type),\n        )\n        self._crawl_run.frontier_size = self._frontier.size\n        self._crawl_run.max_depth_reached = self._frontier.max_depth_reached\n\n        partial = metrics.pages_failed &gt; 0\n        self._crawl_run.mark_completed(partial=partial)\n        self._storage.save_run(self._crawl_run)\n\n        duration = (datetime.now() - start_time).total_seconds()\n        self._logger.run_completed(metrics.to_dict(), duration)\n\n        return CrawlResult(\n            run_id=self.run_id,\n            site_id=self.site_id,\n            success=True,\n            stats=self._crawl_run.stats,\n            documents=self._documents,\n            duration_seconds=duration,\n        )\n\n    except Exception as e:\n        logger.error(\"Crawl job failed\", error=str(e))\n\n        if self._crawl_run:\n            self._crawl_run.mark_failed(str(e))\n            self._storage.save_run(self._crawl_run)\n\n        self._logger.run_failed(str(e))\n\n        return CrawlResult(\n            run_id=self.run_id,\n            site_id=self.site_id,\n            success=False,\n            error=str(e),\n            duration_seconds=(datetime.now() - start_time).total_seconds(),\n        )\n\n    finally:\n        # Cleanup\n        if self._fetcher:\n            await self._fetcher.close()\n        if self._storage:\n            self._storage.close()\n</code></pre>"},{"location":"reference/ragcrawl/#ragcrawl.CrawlResult","title":"<code>CrawlResult(run_id, site_id, success, stats=CrawlStats(), documents=list(), error=None, duration_seconds=0.0)</code>  <code>dataclass</code>","text":"<p>Result of a crawl job.</p>"},{"location":"reference/ragcrawl/#ragcrawl.CrawlRun","title":"<code>CrawlRun</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a single crawl or sync execution.</p> <p>Tracks the status, configuration snapshot, and statistics for a crawl run.</p>"},{"location":"reference/ragcrawl/#ragcrawl.CrawlRun.duration_seconds","title":"<code>duration_seconds</code>  <code>property</code>","text":"<p>Get run duration in seconds.</p>"},{"location":"reference/ragcrawl/#ragcrawl.CrawlRun.mark_cancelled","title":"<code>mark_cancelled()</code>","text":"<p>Mark the run as cancelled.</p> Source code in <code>src/ragcrawl/models/crawl_run.py</code> <pre><code>def mark_cancelled(self) -&gt; None:\n    \"\"\"Mark the run as cancelled.\"\"\"\n    self.status = RunStatus.CANCELLED\n    self.completed_at = datetime.now()\n</code></pre>"},{"location":"reference/ragcrawl/#ragcrawl.CrawlRun.mark_completed","title":"<code>mark_completed(partial=False)</code>","text":"<p>Mark the run as completed.</p> Source code in <code>src/ragcrawl/models/crawl_run.py</code> <pre><code>def mark_completed(self, partial: bool = False) -&gt; None:\n    \"\"\"Mark the run as completed.\"\"\"\n    self.status = RunStatus.PARTIAL if partial else RunStatus.COMPLETED\n    self.completed_at = datetime.now()\n</code></pre>"},{"location":"reference/ragcrawl/#ragcrawl.CrawlRun.mark_failed","title":"<code>mark_failed(error)</code>","text":"<p>Mark the run as failed.</p> Source code in <code>src/ragcrawl/models/crawl_run.py</code> <pre><code>def mark_failed(self, error: str) -&gt; None:\n    \"\"\"Mark the run as failed.\"\"\"\n    self.status = RunStatus.FAILED\n    self.error_message = error\n    self.completed_at = datetime.now()\n</code></pre>"},{"location":"reference/ragcrawl/#ragcrawl.CrawlRun.mark_started","title":"<code>mark_started()</code>","text":"<p>Mark the run as started.</p> Source code in <code>src/ragcrawl/models/crawl_run.py</code> <pre><code>def mark_started(self) -&gt; None:\n    \"\"\"Mark the run as started.\"\"\"\n    self.status = RunStatus.RUNNING\n    self.started_at = datetime.now()\n</code></pre>"},{"location":"reference/ragcrawl/#ragcrawl.CrawlerConfig","title":"<code>CrawlerConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Main configuration for the ragcrawl.</p> <p>This is the primary configuration object that controls all aspects of crawling behavior.</p>"},{"location":"reference/ragcrawl/#ragcrawl.CrawlerConfig.get_allowed_domains","title":"<code>get_allowed_domains()</code>","text":"<p>Get the set of allowed domains including seed domains.</p> Source code in <code>src/ragcrawl/config/crawler_config.py</code> <pre><code>def get_allowed_domains(self) -&gt; set[str]:\n    \"\"\"Get the set of allowed domains including seed domains.\"\"\"\n    from urllib.parse import urlparse\n\n    domains = set(self.allowed_domains)\n    for seed in self.seeds:\n        parsed = urlparse(seed)\n        if parsed.netloc:\n            domains.add(parsed.netloc)\n    return domains\n</code></pre>"},{"location":"reference/ragcrawl/#ragcrawl.Document","title":"<code>Document</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A crawled document with rich metadata for LLM/RAG consumption.</p> <p>This is the primary output model containing all extracted content and metadata from a crawled page.</p>"},{"location":"reference/ragcrawl/#ragcrawl.DuckDBConfig","title":"<code>DuckDBConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for DuckDB storage backend.</p>"},{"location":"reference/ragcrawl/#ragcrawl.DynamoDBConfig","title":"<code>DynamoDBConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for DynamoDB storage backend.</p>"},{"location":"reference/ragcrawl/#ragcrawl.OutputConfig","title":"<code>OutputConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for Markdown output/publishing.</p> <p>Supports single-page (concatenated) and multi-page (folder structure) modes.</p>"},{"location":"reference/ragcrawl/#ragcrawl.OutputConfig.output_path","title":"<code>output_path</code>  <code>property</code>","text":"<p>Get the output root path.</p>"},{"location":"reference/ragcrawl/#ragcrawl.OutputMode","title":"<code>OutputMode</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Markdown output mode.</p>"},{"location":"reference/ragcrawl/#ragcrawl.Page","title":"<code>Page</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents the current state of a URL in the crawl database.</p> <p>This model tracks freshness information and points to the current version of the page content. It's used for incremental sync to determine what needs re-crawling.</p>"},{"location":"reference/ragcrawl/#ragcrawl.Page.needs_recrawl","title":"<code>needs_recrawl(max_age_hours=None, force=False)</code>","text":"<p>Determine if this page needs to be re-crawled.</p> <p>Parameters:</p> Name Type Description Default <code>max_age_hours</code> <code>float | None</code> <p>Maximum age in hours before recrawl. None means always recrawl.</p> <code>None</code> <code>force</code> <code>bool</code> <p>If True, always return True.</p> <code>False</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the page should be re-crawled.</p> Source code in <code>src/ragcrawl/models/page.py</code> <pre><code>def needs_recrawl(\n    self,\n    max_age_hours: float | None = None,\n    force: bool = False,\n) -&gt; bool:\n    \"\"\"\n    Determine if this page needs to be re-crawled.\n\n    Args:\n        max_age_hours: Maximum age in hours before recrawl. None means always recrawl.\n        force: If True, always return True.\n\n    Returns:\n        True if the page should be re-crawled.\n    \"\"\"\n    if force:\n        return True\n\n    if self.is_tombstone:\n        return False\n\n    if self.last_crawled is None:\n        return True\n\n    if max_age_hours is None:\n        return True\n\n    age = datetime.now() - self.last_crawled\n    return age.total_seconds() / 3600 &gt; max_age_hours\n</code></pre>"},{"location":"reference/ragcrawl/#ragcrawl.PageVersion","title":"<code>PageVersion</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a specific version of a page's content.</p> <p>Each time content changes, a new PageVersion is created. This enables version history and change tracking for KB updates.</p>"},{"location":"reference/ragcrawl/#ragcrawl.RunStatus","title":"<code>RunStatus</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Status of a crawl run.</p>"},{"location":"reference/ragcrawl/#ragcrawl.Site","title":"<code>Site</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a website/crawl target with its configuration.</p> <p>Stores the configuration snapshot and metadata for a crawl target.</p>"},{"location":"reference/ragcrawl/#ragcrawl.StorageConfig","title":"<code>StorageConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Storage configuration supporting multiple backends.</p> <p>DuckDB is the default when no configuration is provided. DynamoDB is enabled only when explicitly configured.</p>"},{"location":"reference/ragcrawl/#ragcrawl.StorageConfig.storage_type","title":"<code>storage_type</code>  <code>property</code>","text":"<p>Get the storage type.</p>"},{"location":"reference/ragcrawl/#ragcrawl.StorageConfig.duckdb","title":"<code>duckdb(path=None)</code>  <code>classmethod</code>","text":"<p>Create a DuckDB storage configuration.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path | None</code> <p>Path to database file. If None, uses default (~/.ragcrawl/ragcrawl.duckdb).</p> <code>None</code> Source code in <code>src/ragcrawl/config/storage_config.py</code> <pre><code>@classmethod\ndef duckdb(cls, path: str | Path | None = None) -&gt; \"StorageConfig\":\n    \"\"\"Create a DuckDB storage configuration.\n\n    Args:\n        path: Path to database file. If None, uses default (~/.ragcrawl/ragcrawl.duckdb).\n    \"\"\"\n    if path is None:\n        return cls(backend=DuckDBConfig())\n    return cls(backend=DuckDBConfig(path=path))\n</code></pre>"},{"location":"reference/ragcrawl/#ragcrawl.StorageConfig.dynamodb","title":"<code>dynamodb(region='us-east-1', table_prefix='ragcrawl', **kwargs)</code>  <code>classmethod</code>","text":"<p>Create a DynamoDB storage configuration.</p> Source code in <code>src/ragcrawl/config/storage_config.py</code> <pre><code>@classmethod\ndef dynamodb(\n    cls,\n    region: str = \"us-east-1\",\n    table_prefix: str = \"ragcrawl\",\n    **kwargs: str | int | None,\n) -&gt; \"StorageConfig\":\n    \"\"\"Create a DynamoDB storage configuration.\"\"\"\n    return cls(backend=DynamoDBConfig(region=region, table_prefix=table_prefix, **kwargs))\n</code></pre>"},{"location":"reference/ragcrawl/#ragcrawl.SyncConfig","title":"<code>SyncConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for incremental sync/update operations.</p> <p>Sync operations detect and process only changed content, minimizing redundant work and API calls.</p>"},{"location":"reference/ragcrawl/#ragcrawl.SyncJob","title":"<code>SyncJob(config)</code>","text":"<p>Incremental sync job for detecting content changes.</p> <p>Uses multiple strategies: 1. Sitemap lastmod (if available) 2. HTTP conditional requests (ETag/Last-Modified) 3. Content hash diffing (fallback)</p> <p>Initialize sync job.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>SyncConfig</code> <p>Sync configuration.</p> required Source code in <code>src/ragcrawl/core/sync_job.py</code> <pre><code>def __init__(self, config: SyncConfig) -&gt; None:\n    \"\"\"\n    Initialize sync job.\n\n    Args:\n        config: Sync configuration.\n    \"\"\"\n    self.config = config\n    self.site_id = config.site_id\n    self.run_id = generate_run_id()\n\n    # Components\n    self._storage: StorageBackend | None = None\n    self._fetcher: Crawl4AIFetcher | None = None\n    self._extractor: ContentExtractor | None = None\n    self._sitemap_parser: SitemapParser | None = None\n    self._change_detector: ChangeDetector | None = None\n    self._revalidator: Revalidator | None = None\n\n    # Tracking\n    self._metrics = MetricsCollector()\n    self._logger = CrawlLoggerAdapter(self.run_id, self.site_id)\n    self._changed_pages: list[str] = []\n    self._deleted_pages: list[str] = []\n</code></pre>"},{"location":"reference/ragcrawl/#ragcrawl.SyncJob.run","title":"<code>run()</code>  <code>async</code>","text":"<p>Execute the sync job.</p> <p>Returns:</p> Type Description <code>SyncResult</code> <p>SyncResult with changed and deleted pages.</p> Source code in <code>src/ragcrawl/core/sync_job.py</code> <pre><code>async def run(self) -&gt; SyncResult:\n    \"\"\"\n    Execute the sync job.\n\n    Returns:\n        SyncResult with changed and deleted pages.\n    \"\"\"\n    start_time = datetime.now()\n\n    try:\n        self._init_components()\n\n        # Verify site exists\n        site = self._storage.get_site(self.site_id)\n        if not site:\n            raise ValueError(f\"Site not found: {self.site_id}\")\n\n        # Create sync run record\n        crawl_run = CrawlRun(\n            run_id=self.run_id,\n            site_id=self.site_id,\n            is_sync=True,\n            config_snapshot=self.config.model_dump(\n                exclude={\"on_page\", \"on_change_detected\", \"on_deletion_detected\", \"on_error\"}\n            ),\n        )\n        crawl_run.mark_started()\n        self._storage.save_run(crawl_run)\n\n        # Get pages to check\n        pages = await self._get_pages_to_check()\n\n        logger.info(\"Starting sync\", site_id=self.site_id, pages_to_check=len(pages))\n\n        # Process pages\n        for page in pages:\n            await self._process_page(page)\n\n            # Check limit\n            if self.config.max_pages and self._metrics.metrics.pages_crawled &gt;= self.config.max_pages:\n                break\n\n        # Finalize\n        metrics = self._metrics.finalize()\n        crawl_run.stats = CrawlStats(\n            pages_crawled=metrics.pages_crawled,\n            pages_changed=metrics.pages_changed,\n            pages_unchanged=metrics.pages_unchanged,\n            pages_deleted=metrics.pages_deleted,\n            pages_failed=metrics.pages_failed,\n        )\n        crawl_run.mark_completed(partial=metrics.pages_failed &gt; 0)\n        self._storage.save_run(crawl_run)\n\n        # Update site\n        site.last_sync_at = datetime.now()\n        self._storage.save_site(site)\n\n        duration = (datetime.now() - start_time).total_seconds()\n\n        return SyncResult(\n            run_id=self.run_id,\n            site_id=self.site_id,\n            success=True,\n            stats=crawl_run.stats,\n            changed_pages=self._changed_pages,\n            deleted_pages=self._deleted_pages,\n            duration_seconds=duration,\n        )\n\n    except Exception as e:\n        logger.error(\"Sync job failed\", error=str(e))\n        return SyncResult(\n            run_id=self.run_id,\n            site_id=self.site_id,\n            success=False,\n            error=str(e),\n            duration_seconds=(datetime.now() - start_time).total_seconds(),\n        )\n\n    finally:\n        if self._fetcher:\n            await self._fetcher.close()\n        if self._storage:\n            self._storage.close()\n</code></pre>"},{"location":"reference/ragcrawl/#ragcrawl.SyncResult","title":"<code>SyncResult(run_id, site_id, success, stats=CrawlStats(), changed_pages=list(), deleted_pages=list(), error=None, duration_seconds=0.0)</code>  <code>dataclass</code>","text":"<p>Result of a sync job.</p>"},{"location":"reference/ragcrawl/#ragcrawl.SyncStrategy","title":"<code>SyncStrategy</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Strategy for detecting content changes.</p>"},{"location":"reference/ragcrawl/chunking/","title":"Index","text":""},{"location":"reference/ragcrawl/chunking/#ragcrawl.chunking","title":"<code>chunking</code>","text":"<p>Content chunking for RAG pipelines.</p>"},{"location":"reference/ragcrawl/chunking/#ragcrawl.chunking.Chunker","title":"<code>Chunker</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for content chunkers.</p> <p>Chunkers split documents into segments optimized for embedding and retrieval in RAG pipelines.</p>"},{"location":"reference/ragcrawl/chunking/#ragcrawl.chunking.Chunker.chunk","title":"<code>chunk(document)</code>  <code>abstractmethod</code>","text":"<p>Chunk a document into segments.</p> <p>Parameters:</p> Name Type Description Default <code>document</code> <code>Document</code> <p>Document to chunk.</p> required <p>Returns:</p> Type Description <code>list[Chunk]</code> <p>List of chunks.</p> Source code in <code>src/ragcrawl/chunking/chunker.py</code> <pre><code>@abstractmethod\ndef chunk(self, document: Document) -&gt; list[Chunk]:\n    \"\"\"\n    Chunk a document into segments.\n\n    Args:\n        document: Document to chunk.\n\n    Returns:\n        List of chunks.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/ragcrawl/chunking/#ragcrawl.chunking.Chunker.estimate_tokens","title":"<code>estimate_tokens(text)</code>  <code>abstractmethod</code>","text":"<p>Estimate token count for text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text to estimate.</p> required <p>Returns:</p> Type Description <code>int</code> <p>Estimated token count.</p> Source code in <code>src/ragcrawl/chunking/chunker.py</code> <pre><code>@abstractmethod\ndef estimate_tokens(self, text: str) -&gt; int:\n    \"\"\"\n    Estimate token count for text.\n\n    Args:\n        text: Text to estimate.\n\n    Returns:\n        Estimated token count.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/ragcrawl/chunking/#ragcrawl.chunking.HeadingChunker","title":"<code>HeadingChunker(min_chunk_size=100, max_chunk_size=2000, heading_levels=None, include_heading_in_chunk=True, overlap_size=0)</code>","text":"<p>               Bases: <code>Chunker</code></p> <p>Chunks markdown content by headings.</p> <p>Creates chunks that respect document structure by splitting at heading boundaries while maintaining context.</p> <p>Initialize heading chunker.</p> <p>Parameters:</p> Name Type Description Default <code>min_chunk_size</code> <code>int</code> <p>Minimum chunk size in characters.</p> <code>100</code> <code>max_chunk_size</code> <code>int</code> <p>Maximum chunk size in characters.</p> <code>2000</code> <code>heading_levels</code> <code>list[int] | None</code> <p>Heading levels to split on (default: [1, 2, 3]).</p> <code>None</code> <code>include_heading_in_chunk</code> <code>bool</code> <p>Include the heading in chunk content.</p> <code>True</code> <code>overlap_size</code> <code>int</code> <p>Characters to overlap between chunks.</p> <code>0</code> Source code in <code>src/ragcrawl/chunking/heading_chunker.py</code> <pre><code>def __init__(\n    self,\n    min_chunk_size: int = 100,\n    max_chunk_size: int = 2000,\n    heading_levels: list[int] | None = None,\n    include_heading_in_chunk: bool = True,\n    overlap_size: int = 0,\n) -&gt; None:\n    \"\"\"\n    Initialize heading chunker.\n\n    Args:\n        min_chunk_size: Minimum chunk size in characters.\n        max_chunk_size: Maximum chunk size in characters.\n        heading_levels: Heading levels to split on (default: [1, 2, 3]).\n        include_heading_in_chunk: Include the heading in chunk content.\n        overlap_size: Characters to overlap between chunks.\n    \"\"\"\n    self.min_chunk_size = min_chunk_size\n    self.max_chunk_size = max_chunk_size\n    self.heading_levels = heading_levels or [1, 2, 3]\n    self.include_heading_in_chunk = include_heading_in_chunk\n    self.overlap_size = overlap_size\n\n    # Regex for markdown headings\n    self._heading_pattern = re.compile(\n        r\"^(#{1,6})\\s+(.+?)$\", re.MULTILINE\n    )\n</code></pre>"},{"location":"reference/ragcrawl/chunking/#ragcrawl.chunking.HeadingChunker.chunk","title":"<code>chunk(document)</code>","text":"<p>Chunk document by headings.</p> <p>Parameters:</p> Name Type Description Default <code>document</code> <code>Document</code> <p>Document to chunk.</p> required <p>Returns:</p> Type Description <code>list[Chunk]</code> <p>List of chunks.</p> Source code in <code>src/ragcrawl/chunking/heading_chunker.py</code> <pre><code>def chunk(self, document: Document) -&gt; list[Chunk]:\n    \"\"\"\n    Chunk document by headings.\n\n    Args:\n        document: Document to chunk.\n\n    Returns:\n        List of chunks.\n    \"\"\"\n    content = document.markdown\n    if not content:\n        return []\n\n    # Parse sections\n    sections = self._parse_sections(content)\n\n    if not sections:\n        # No headings found, treat as single chunk\n        return self._create_single_chunk(document, content)\n\n    # Create chunks from sections\n    chunks = []\n    section_path_stack: list[str] = []\n\n    for i, section in enumerate(sections):\n        # Update section path\n        while section_path_stack and len(section_path_stack) &gt;= section.level:\n            section_path_stack.pop()\n        section_path_stack.append(section.heading)\n        section_path = \" &gt; \".join(section_path_stack)\n\n        # Build chunk content\n        if self.include_heading_in_chunk:\n            chunk_content = f\"{'#' * section.level} {section.heading}\\n\\n{section.content}\"\n        else:\n            chunk_content = section.content\n\n        # Handle oversized sections\n        if len(chunk_content) &gt; self.max_chunk_size:\n            sub_chunks = self._split_large_section(\n                document,\n                chunk_content,\n                section,\n                section_path,\n                len(chunks),\n            )\n            chunks.extend(sub_chunks)\n        elif len(chunk_content) &gt;= self.min_chunk_size:\n            chunk = self._create_chunk(\n                document=document,\n                content=chunk_content,\n                index=len(chunks),\n                start_offset=section.start_offset,\n                end_offset=section.end_offset,\n                section_path=section_path,\n                heading=section.heading,\n                heading_level=section.level,\n            )\n            chunks.append(chunk)\n        # else: skip chunks that are too small\n\n    # Update total_chunks\n    for chunk in chunks:\n        chunk.total_chunks = len(chunks)\n\n    return chunks\n</code></pre>"},{"location":"reference/ragcrawl/chunking/#ragcrawl.chunking.HeadingChunker.estimate_tokens","title":"<code>estimate_tokens(text)</code>","text":"<p>Estimate token count (roughly 4 chars per token).</p> Source code in <code>src/ragcrawl/chunking/heading_chunker.py</code> <pre><code>def estimate_tokens(self, text: str) -&gt; int:\n    \"\"\"Estimate token count (roughly 4 chars per token).\"\"\"\n    return len(text) // 4\n</code></pre>"},{"location":"reference/ragcrawl/chunking/#ragcrawl.chunking.TokenChunker","title":"<code>TokenChunker(chunk_size=512, chunk_overlap=50, encoding_name='cl100k_base', separators=None)</code>","text":"<p>               Bases: <code>Chunker</code></p> <p>Chunks content by token count.</p> <p>Uses tiktoken for accurate token counting and respects natural text boundaries (sentences, paragraphs).</p> <p>Initialize token chunker.</p> <p>Parameters:</p> Name Type Description Default <code>chunk_size</code> <code>int</code> <p>Target chunk size in tokens.</p> <code>512</code> <code>chunk_overlap</code> <code>int</code> <p>Token overlap between chunks.</p> <code>50</code> <code>encoding_name</code> <code>str</code> <p>Tiktoken encoding name.</p> <code>'cl100k_base'</code> <code>separators</code> <code>list[str] | None</code> <p>Text separators to try, in order.</p> <code>None</code> Source code in <code>src/ragcrawl/chunking/token_chunker.py</code> <pre><code>def __init__(\n    self,\n    chunk_size: int = 512,\n    chunk_overlap: int = 50,\n    encoding_name: str = \"cl100k_base\",\n    separators: list[str] | None = None,\n) -&gt; None:\n    \"\"\"\n    Initialize token chunker.\n\n    Args:\n        chunk_size: Target chunk size in tokens.\n        chunk_overlap: Token overlap between chunks.\n        encoding_name: Tiktoken encoding name.\n        separators: Text separators to try, in order.\n    \"\"\"\n    self.chunk_size = chunk_size\n    self.chunk_overlap = chunk_overlap\n    self.encoding_name = encoding_name\n    self.separators = separators or [\"\\n\\n\", \"\\n\", \". \", \" \"]\n\n    self._encoding = None\n</code></pre>"},{"location":"reference/ragcrawl/chunking/#ragcrawl.chunking.TokenChunker.encoding","title":"<code>encoding</code>  <code>property</code>","text":"<p>Get or create tiktoken encoding.</p>"},{"location":"reference/ragcrawl/chunking/#ragcrawl.chunking.TokenChunker.chunk","title":"<code>chunk(document)</code>","text":"<p>Chunk document by token count.</p> <p>Parameters:</p> Name Type Description Default <code>document</code> <code>Document</code> <p>Document to chunk.</p> required <p>Returns:</p> Type Description <code>list[Chunk]</code> <p>List of chunks.</p> Source code in <code>src/ragcrawl/chunking/token_chunker.py</code> <pre><code>def chunk(self, document: Document) -&gt; list[Chunk]:\n    \"\"\"\n    Chunk document by token count.\n\n    Args:\n        document: Document to chunk.\n\n    Returns:\n        List of chunks.\n    \"\"\"\n    content = document.markdown\n    if not content:\n        return []\n\n    # Split content into chunks\n    text_chunks = self._split_text(content)\n\n    # Create Chunk objects\n    chunks = []\n    current_offset = 0\n\n    for i, text in enumerate(text_chunks):\n        # Find actual offset in original content\n        start_offset = content.find(text[:50], current_offset)\n        if start_offset == -1:\n            start_offset = current_offset\n        end_offset = start_offset + len(text)\n        current_offset = end_offset - self.chunk_overlap * 4  # Approximate\n\n        chunk = Chunk(\n            chunk_id=generate_chunk_id(document.doc_id, i),\n            doc_id=document.doc_id,\n            page_id=document.page_id,\n            version_id=document.version_id,\n            content=text,\n            content_type=\"markdown\",\n            chunk_index=i,\n            total_chunks=len(text_chunks),\n            start_offset=start_offset,\n            end_offset=end_offset,\n            char_count=len(text),\n            word_count=len(text.split()),\n            token_estimate=self.estimate_tokens(text),\n            section_path=None,\n            heading=document.title,\n            heading_level=None,\n            source_url=document.source_url,\n            title=document.title,\n            chunker_type=\"token\",\n            overlap_tokens=self.chunk_overlap if i &gt; 0 else 0,\n        )\n        chunks.append(chunk)\n\n    return chunks\n</code></pre>"},{"location":"reference/ragcrawl/chunking/#ragcrawl.chunking.TokenChunker.estimate_tokens","title":"<code>estimate_tokens(text)</code>","text":"<p>Estimate token count.</p> <p>Uses tiktoken if available, otherwise approximates.</p> Source code in <code>src/ragcrawl/chunking/token_chunker.py</code> <pre><code>def estimate_tokens(self, text: str) -&gt; int:\n    \"\"\"\n    Estimate token count.\n\n    Uses tiktoken if available, otherwise approximates.\n    \"\"\"\n    if self.encoding:\n        return len(self.encoding.encode(text))\n\n    # Fallback: approximately 4 characters per token\n    return len(text) // 4\n</code></pre>"},{"location":"reference/ragcrawl/chunking/chunker/","title":"Chunker","text":""},{"location":"reference/ragcrawl/chunking/chunker/#ragcrawl.chunking.chunker","title":"<code>chunker</code>","text":"<p>Base chunker protocol.</p>"},{"location":"reference/ragcrawl/chunking/chunker/#ragcrawl.chunking.chunker.Chunker","title":"<code>Chunker</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for content chunkers.</p> <p>Chunkers split documents into segments optimized for embedding and retrieval in RAG pipelines.</p>"},{"location":"reference/ragcrawl/chunking/chunker/#ragcrawl.chunking.chunker.Chunker.chunk","title":"<code>chunk(document)</code>  <code>abstractmethod</code>","text":"<p>Chunk a document into segments.</p> <p>Parameters:</p> Name Type Description Default <code>document</code> <code>Document</code> <p>Document to chunk.</p> required <p>Returns:</p> Type Description <code>list[Chunk]</code> <p>List of chunks.</p> Source code in <code>src/ragcrawl/chunking/chunker.py</code> <pre><code>@abstractmethod\ndef chunk(self, document: Document) -&gt; list[Chunk]:\n    \"\"\"\n    Chunk a document into segments.\n\n    Args:\n        document: Document to chunk.\n\n    Returns:\n        List of chunks.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/ragcrawl/chunking/chunker/#ragcrawl.chunking.chunker.Chunker.estimate_tokens","title":"<code>estimate_tokens(text)</code>  <code>abstractmethod</code>","text":"<p>Estimate token count for text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text to estimate.</p> required <p>Returns:</p> Type Description <code>int</code> <p>Estimated token count.</p> Source code in <code>src/ragcrawl/chunking/chunker.py</code> <pre><code>@abstractmethod\ndef estimate_tokens(self, text: str) -&gt; int:\n    \"\"\"\n    Estimate token count for text.\n\n    Args:\n        text: Text to estimate.\n\n    Returns:\n        Estimated token count.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/ragcrawl/chunking/chunker/#ragcrawl.chunking.chunker.ChunkerProtocol","title":"<code>ChunkerProtocol</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for content chunkers.</p>"},{"location":"reference/ragcrawl/chunking/chunker/#ragcrawl.chunking.chunker.ChunkerProtocol.chunk","title":"<code>chunk(document)</code>","text":"<p>Chunk a document into segments.</p> Source code in <code>src/ragcrawl/chunking/chunker.py</code> <pre><code>def chunk(self, document: Document) -&gt; list[Chunk]:\n    \"\"\"Chunk a document into segments.\"\"\"\n    ...\n</code></pre>"},{"location":"reference/ragcrawl/chunking/heading_chunker/","title":"Heading chunker","text":""},{"location":"reference/ragcrawl/chunking/heading_chunker/#ragcrawl.chunking.heading_chunker","title":"<code>heading_chunker</code>","text":"<p>Heading-aware markdown chunker.</p>"},{"location":"reference/ragcrawl/chunking/heading_chunker/#ragcrawl.chunking.heading_chunker.HeadingChunker","title":"<code>HeadingChunker(min_chunk_size=100, max_chunk_size=2000, heading_levels=None, include_heading_in_chunk=True, overlap_size=0)</code>","text":"<p>               Bases: <code>Chunker</code></p> <p>Chunks markdown content by headings.</p> <p>Creates chunks that respect document structure by splitting at heading boundaries while maintaining context.</p> <p>Initialize heading chunker.</p> <p>Parameters:</p> Name Type Description Default <code>min_chunk_size</code> <code>int</code> <p>Minimum chunk size in characters.</p> <code>100</code> <code>max_chunk_size</code> <code>int</code> <p>Maximum chunk size in characters.</p> <code>2000</code> <code>heading_levels</code> <code>list[int] | None</code> <p>Heading levels to split on (default: [1, 2, 3]).</p> <code>None</code> <code>include_heading_in_chunk</code> <code>bool</code> <p>Include the heading in chunk content.</p> <code>True</code> <code>overlap_size</code> <code>int</code> <p>Characters to overlap between chunks.</p> <code>0</code> Source code in <code>src/ragcrawl/chunking/heading_chunker.py</code> <pre><code>def __init__(\n    self,\n    min_chunk_size: int = 100,\n    max_chunk_size: int = 2000,\n    heading_levels: list[int] | None = None,\n    include_heading_in_chunk: bool = True,\n    overlap_size: int = 0,\n) -&gt; None:\n    \"\"\"\n    Initialize heading chunker.\n\n    Args:\n        min_chunk_size: Minimum chunk size in characters.\n        max_chunk_size: Maximum chunk size in characters.\n        heading_levels: Heading levels to split on (default: [1, 2, 3]).\n        include_heading_in_chunk: Include the heading in chunk content.\n        overlap_size: Characters to overlap between chunks.\n    \"\"\"\n    self.min_chunk_size = min_chunk_size\n    self.max_chunk_size = max_chunk_size\n    self.heading_levels = heading_levels or [1, 2, 3]\n    self.include_heading_in_chunk = include_heading_in_chunk\n    self.overlap_size = overlap_size\n\n    # Regex for markdown headings\n    self._heading_pattern = re.compile(\n        r\"^(#{1,6})\\s+(.+?)$\", re.MULTILINE\n    )\n</code></pre>"},{"location":"reference/ragcrawl/chunking/heading_chunker/#ragcrawl.chunking.heading_chunker.HeadingChunker.chunk","title":"<code>chunk(document)</code>","text":"<p>Chunk document by headings.</p> <p>Parameters:</p> Name Type Description Default <code>document</code> <code>Document</code> <p>Document to chunk.</p> required <p>Returns:</p> Type Description <code>list[Chunk]</code> <p>List of chunks.</p> Source code in <code>src/ragcrawl/chunking/heading_chunker.py</code> <pre><code>def chunk(self, document: Document) -&gt; list[Chunk]:\n    \"\"\"\n    Chunk document by headings.\n\n    Args:\n        document: Document to chunk.\n\n    Returns:\n        List of chunks.\n    \"\"\"\n    content = document.markdown\n    if not content:\n        return []\n\n    # Parse sections\n    sections = self._parse_sections(content)\n\n    if not sections:\n        # No headings found, treat as single chunk\n        return self._create_single_chunk(document, content)\n\n    # Create chunks from sections\n    chunks = []\n    section_path_stack: list[str] = []\n\n    for i, section in enumerate(sections):\n        # Update section path\n        while section_path_stack and len(section_path_stack) &gt;= section.level:\n            section_path_stack.pop()\n        section_path_stack.append(section.heading)\n        section_path = \" &gt; \".join(section_path_stack)\n\n        # Build chunk content\n        if self.include_heading_in_chunk:\n            chunk_content = f\"{'#' * section.level} {section.heading}\\n\\n{section.content}\"\n        else:\n            chunk_content = section.content\n\n        # Handle oversized sections\n        if len(chunk_content) &gt; self.max_chunk_size:\n            sub_chunks = self._split_large_section(\n                document,\n                chunk_content,\n                section,\n                section_path,\n                len(chunks),\n            )\n            chunks.extend(sub_chunks)\n        elif len(chunk_content) &gt;= self.min_chunk_size:\n            chunk = self._create_chunk(\n                document=document,\n                content=chunk_content,\n                index=len(chunks),\n                start_offset=section.start_offset,\n                end_offset=section.end_offset,\n                section_path=section_path,\n                heading=section.heading,\n                heading_level=section.level,\n            )\n            chunks.append(chunk)\n        # else: skip chunks that are too small\n\n    # Update total_chunks\n    for chunk in chunks:\n        chunk.total_chunks = len(chunks)\n\n    return chunks\n</code></pre>"},{"location":"reference/ragcrawl/chunking/heading_chunker/#ragcrawl.chunking.heading_chunker.HeadingChunker.estimate_tokens","title":"<code>estimate_tokens(text)</code>","text":"<p>Estimate token count (roughly 4 chars per token).</p> Source code in <code>src/ragcrawl/chunking/heading_chunker.py</code> <pre><code>def estimate_tokens(self, text: str) -&gt; int:\n    \"\"\"Estimate token count (roughly 4 chars per token).\"\"\"\n    return len(text) // 4\n</code></pre>"},{"location":"reference/ragcrawl/chunking/heading_chunker/#ragcrawl.chunking.heading_chunker.Section","title":"<code>Section(heading, level, content, start_offset, end_offset)</code>  <code>dataclass</code>","text":"<p>A section of content under a heading.</p>"},{"location":"reference/ragcrawl/chunking/token_chunker/","title":"Token chunker","text":""},{"location":"reference/ragcrawl/chunking/token_chunker/#ragcrawl.chunking.token_chunker","title":"<code>token_chunker</code>","text":"<p>Token-based chunker using tiktoken.</p>"},{"location":"reference/ragcrawl/chunking/token_chunker/#ragcrawl.chunking.token_chunker.TokenChunker","title":"<code>TokenChunker(chunk_size=512, chunk_overlap=50, encoding_name='cl100k_base', separators=None)</code>","text":"<p>               Bases: <code>Chunker</code></p> <p>Chunks content by token count.</p> <p>Uses tiktoken for accurate token counting and respects natural text boundaries (sentences, paragraphs).</p> <p>Initialize token chunker.</p> <p>Parameters:</p> Name Type Description Default <code>chunk_size</code> <code>int</code> <p>Target chunk size in tokens.</p> <code>512</code> <code>chunk_overlap</code> <code>int</code> <p>Token overlap between chunks.</p> <code>50</code> <code>encoding_name</code> <code>str</code> <p>Tiktoken encoding name.</p> <code>'cl100k_base'</code> <code>separators</code> <code>list[str] | None</code> <p>Text separators to try, in order.</p> <code>None</code> Source code in <code>src/ragcrawl/chunking/token_chunker.py</code> <pre><code>def __init__(\n    self,\n    chunk_size: int = 512,\n    chunk_overlap: int = 50,\n    encoding_name: str = \"cl100k_base\",\n    separators: list[str] | None = None,\n) -&gt; None:\n    \"\"\"\n    Initialize token chunker.\n\n    Args:\n        chunk_size: Target chunk size in tokens.\n        chunk_overlap: Token overlap between chunks.\n        encoding_name: Tiktoken encoding name.\n        separators: Text separators to try, in order.\n    \"\"\"\n    self.chunk_size = chunk_size\n    self.chunk_overlap = chunk_overlap\n    self.encoding_name = encoding_name\n    self.separators = separators or [\"\\n\\n\", \"\\n\", \". \", \" \"]\n\n    self._encoding = None\n</code></pre>"},{"location":"reference/ragcrawl/chunking/token_chunker/#ragcrawl.chunking.token_chunker.TokenChunker.encoding","title":"<code>encoding</code>  <code>property</code>","text":"<p>Get or create tiktoken encoding.</p>"},{"location":"reference/ragcrawl/chunking/token_chunker/#ragcrawl.chunking.token_chunker.TokenChunker.chunk","title":"<code>chunk(document)</code>","text":"<p>Chunk document by token count.</p> <p>Parameters:</p> Name Type Description Default <code>document</code> <code>Document</code> <p>Document to chunk.</p> required <p>Returns:</p> Type Description <code>list[Chunk]</code> <p>List of chunks.</p> Source code in <code>src/ragcrawl/chunking/token_chunker.py</code> <pre><code>def chunk(self, document: Document) -&gt; list[Chunk]:\n    \"\"\"\n    Chunk document by token count.\n\n    Args:\n        document: Document to chunk.\n\n    Returns:\n        List of chunks.\n    \"\"\"\n    content = document.markdown\n    if not content:\n        return []\n\n    # Split content into chunks\n    text_chunks = self._split_text(content)\n\n    # Create Chunk objects\n    chunks = []\n    current_offset = 0\n\n    for i, text in enumerate(text_chunks):\n        # Find actual offset in original content\n        start_offset = content.find(text[:50], current_offset)\n        if start_offset == -1:\n            start_offset = current_offset\n        end_offset = start_offset + len(text)\n        current_offset = end_offset - self.chunk_overlap * 4  # Approximate\n\n        chunk = Chunk(\n            chunk_id=generate_chunk_id(document.doc_id, i),\n            doc_id=document.doc_id,\n            page_id=document.page_id,\n            version_id=document.version_id,\n            content=text,\n            content_type=\"markdown\",\n            chunk_index=i,\n            total_chunks=len(text_chunks),\n            start_offset=start_offset,\n            end_offset=end_offset,\n            char_count=len(text),\n            word_count=len(text.split()),\n            token_estimate=self.estimate_tokens(text),\n            section_path=None,\n            heading=document.title,\n            heading_level=None,\n            source_url=document.source_url,\n            title=document.title,\n            chunker_type=\"token\",\n            overlap_tokens=self.chunk_overlap if i &gt; 0 else 0,\n        )\n        chunks.append(chunk)\n\n    return chunks\n</code></pre>"},{"location":"reference/ragcrawl/chunking/token_chunker/#ragcrawl.chunking.token_chunker.TokenChunker.estimate_tokens","title":"<code>estimate_tokens(text)</code>","text":"<p>Estimate token count.</p> <p>Uses tiktoken if available, otherwise approximates.</p> Source code in <code>src/ragcrawl/chunking/token_chunker.py</code> <pre><code>def estimate_tokens(self, text: str) -&gt; int:\n    \"\"\"\n    Estimate token count.\n\n    Uses tiktoken if available, otherwise approximates.\n    \"\"\"\n    if self.encoding:\n        return len(self.encoding.encode(text))\n\n    # Fallback: approximately 4 characters per token\n    return len(text) // 4\n</code></pre>"},{"location":"reference/ragcrawl/cli/","title":"Index","text":""},{"location":"reference/ragcrawl/cli/#ragcrawl.cli","title":"<code>cli</code>","text":"<p>CLI for ragcrawl.</p>"},{"location":"reference/ragcrawl/cli/#ragcrawl.cli.app","title":"<code>app()</code>","text":"<p>ragcrawl - Crawl websites and produce LLM-ready artifacts.</p> Source code in <code>src/ragcrawl/cli/main.py</code> <pre><code>@click.group()\n@click.version_option(version=__version__)\ndef app() -&gt; None:\n    \"\"\"ragcrawl - Crawl websites and produce LLM-ready artifacts.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/ragcrawl/cli/config_tui/","title":"Config tui","text":""},{"location":"reference/ragcrawl/cli/config_tui/#ragcrawl.cli.config_tui","title":"<code>config_tui</code>","text":"<p>Textual TUI for ragcrawl configuration.</p>"},{"location":"reference/ragcrawl/cli/config_tui/#ragcrawl.cli.config_tui.ConfigField","title":"<code>ConfigField(key, label, value, placeholder='')</code>","text":"<p>               Bases: <code>Horizontal</code></p> <p>A single configuration field with label and input.</p> Source code in <code>src/ragcrawl/cli/config_tui.py</code> <pre><code>def __init__(\n    self,\n    key: str,\n    label: str,\n    value: str,\n    placeholder: str = \"\",\n) -&gt; None:\n    super().__init__()\n    self.key = key\n    self.label_text = label\n    self.value = value\n    self.placeholder = placeholder or f\"Enter {label.lower()}\"\n</code></pre>"},{"location":"reference/ragcrawl/cli/config_tui/#ragcrawl.cli.config_tui.ConfigSection","title":"<code>ConfigSection(title)</code>","text":"<p>               Bases: <code>Vertical</code></p> <p>A section of configuration options.</p> Source code in <code>src/ragcrawl/cli/config_tui.py</code> <pre><code>def __init__(self, title: str) -&gt; None:\n    super().__init__()\n    self.title = title\n</code></pre>"},{"location":"reference/ragcrawl/cli/config_tui/#ragcrawl.cli.config_tui.ConfigTUI","title":"<code>ConfigTUI()</code>","text":"<p>               Bases: <code>App[None]</code></p> <p>Textual app for editing ragcrawl configuration.</p> Source code in <code>src/ragcrawl/cli/config_tui.py</code> <pre><code>def __init__(self) -&gt; None:\n    super().__init__()\n    self.config_manager = UserConfigManager()\n    self.config = self.config_manager.load()\n    self.has_changes = False\n</code></pre>"},{"location":"reference/ragcrawl/cli/config_tui/#ragcrawl.cli.config_tui.ConfigTUI.action_quit","title":"<code>action_quit()</code>","text":"<p>Quit action.</p> Source code in <code>src/ragcrawl/cli/config_tui.py</code> <pre><code>def action_quit(self) -&gt; None:\n    \"\"\"Quit action.\"\"\"\n    if self.has_changes:\n        def handle_confirm(result: bool) -&gt; None:\n            if result:\n                self.exit()\n\n        self.push_screen(\n            ConfirmDialog(\n                \"Unsaved Changes\",\n                \"Quit without saving?\"\n            ),\n            handle_confirm,\n        )\n    else:\n        self.exit()\n</code></pre>"},{"location":"reference/ragcrawl/cli/config_tui/#ragcrawl.cli.config_tui.ConfigTUI.action_reset","title":"<code>action_reset()</code>","text":"<p>Reset action (Ctrl+R).</p> Source code in <code>src/ragcrawl/cli/config_tui.py</code> <pre><code>def action_reset(self) -&gt; None:\n    \"\"\"Reset action (Ctrl+R).\"\"\"\n    def handle_confirm(result: bool) -&gt; None:\n        if result:\n            self.reset_to_defaults()\n\n    self.push_screen(\n        ConfirmDialog(\n            \"Reset Configuration\",\n            \"Reset all settings to defaults?\"\n        ),\n        handle_confirm,\n    )\n</code></pre>"},{"location":"reference/ragcrawl/cli/config_tui/#ragcrawl.cli.config_tui.ConfigTUI.action_save","title":"<code>action_save()</code>","text":"<p>Save action (Ctrl+S).</p> Source code in <code>src/ragcrawl/cli/config_tui.py</code> <pre><code>def action_save(self) -&gt; None:\n    \"\"\"Save action (Ctrl+S).\"\"\"\n    self.validate_and_save()\n</code></pre>"},{"location":"reference/ragcrawl/cli/config_tui/#ragcrawl.cli.config_tui.ConfigTUI.get_field_values","title":"<code>get_field_values()</code>","text":"<p>Get current values from all input fields.</p> Source code in <code>src/ragcrawl/cli/config_tui.py</code> <pre><code>def get_field_values(self) -&gt; dict:\n    \"\"\"Get current values from all input fields.\"\"\"\n    values = {}\n    for field in self.query(ConfigField):\n        input_widget = field.query_one(Input)\n        values[field.key] = input_widget.value\n    return values\n</code></pre>"},{"location":"reference/ragcrawl/cli/config_tui/#ragcrawl.cli.config_tui.ConfigTUI.on_input_changed","title":"<code>on_input_changed(event)</code>","text":"<p>Track when inputs change.</p> Source code in <code>src/ragcrawl/cli/config_tui.py</code> <pre><code>def on_input_changed(self, event: Input.Changed) -&gt; None:\n    \"\"\"Track when inputs change.\"\"\"\n    self.has_changes = True\n    self.update_status(\"\u25cf Unsaved changes\", \"\")\n</code></pre>"},{"location":"reference/ragcrawl/cli/config_tui/#ragcrawl.cli.config_tui.ConfigTUI.on_quit_button","title":"<code>on_quit_button()</code>","text":"<p>Handle quit button click.</p> Source code in <code>src/ragcrawl/cli/config_tui.py</code> <pre><code>@on(Button.Pressed, \"#quit-btn\")\ndef on_quit_button(self) -&gt; None:\n    \"\"\"Handle quit button click.\"\"\"\n    self.action_quit()\n</code></pre>"},{"location":"reference/ragcrawl/cli/config_tui/#ragcrawl.cli.config_tui.ConfigTUI.on_reset_button","title":"<code>on_reset_button()</code>","text":"<p>Handle reset button click.</p> Source code in <code>src/ragcrawl/cli/config_tui.py</code> <pre><code>@on(Button.Pressed, \"#reset-btn\")\ndef on_reset_button(self) -&gt; None:\n    \"\"\"Handle reset button click.\"\"\"\n    def handle_confirm(result: bool) -&gt; None:\n        if result:\n            self.reset_to_defaults()\n\n    self.push_screen(\n        ConfirmDialog(\n            \"Reset Configuration\",\n            \"Reset all settings to defaults?\"\n        ),\n        handle_confirm,\n    )\n</code></pre>"},{"location":"reference/ragcrawl/cli/config_tui/#ragcrawl.cli.config_tui.ConfigTUI.on_save_button","title":"<code>on_save_button()</code>","text":"<p>Handle save button click.</p> Source code in <code>src/ragcrawl/cli/config_tui.py</code> <pre><code>@on(Button.Pressed, \"#save-btn\")\ndef on_save_button(self) -&gt; None:\n    \"\"\"Handle save button click.\"\"\"\n    self.validate_and_save()\n</code></pre>"},{"location":"reference/ragcrawl/cli/config_tui/#ragcrawl.cli.config_tui.ConfigTUI.reset_to_defaults","title":"<code>reset_to_defaults()</code>","text":"<p>Reset all fields to default values.</p> Source code in <code>src/ragcrawl/cli/config_tui.py</code> <pre><code>def reset_to_defaults(self) -&gt; None:\n    \"\"\"Reset all fields to default values.\"\"\"\n    default_config = UserConfig()\n\n    # Update all input fields\n    field_values = {\n        \"storage_dir\": str(default_config.storage_dir),\n        \"db_name\": default_config.db_name,\n        \"user_agent\": default_config.user_agent,\n        \"timeout\": str(default_config.timeout),\n        \"max_retries\": str(default_config.max_retries),\n        \"default_max_pages\": str(default_config.default_max_pages),\n        \"default_max_depth\": str(default_config.default_max_depth),\n    }\n\n    for field in self.query(ConfigField):\n        if field.key in field_values:\n            input_widget = field.query_one(Input)\n            input_widget.value = field_values[field.key]\n\n    self.has_changes = True\n    self.update_status(\"\u25cf Reset to defaults (unsaved)\", \"\")\n</code></pre>"},{"location":"reference/ragcrawl/cli/config_tui/#ragcrawl.cli.config_tui.ConfigTUI.update_status","title":"<code>update_status(message, status_class)</code>","text":"<p>Update the status bar.</p> Source code in <code>src/ragcrawl/cli/config_tui.py</code> <pre><code>def update_status(self, message: str, status_class: str) -&gt; None:\n    \"\"\"Update the status bar.\"\"\"\n    status = self.query_one(\"#status-bar\", Static)\n    status.update(message)\n    status.remove_class(\"success\", \"error\")\n    if status_class:\n        status.add_class(status_class)\n</code></pre>"},{"location":"reference/ragcrawl/cli/config_tui/#ragcrawl.cli.config_tui.ConfigTUI.validate_and_save","title":"<code>validate_and_save()</code>","text":"<p>Validate inputs and save configuration.</p> Source code in <code>src/ragcrawl/cli/config_tui.py</code> <pre><code>def validate_and_save(self) -&gt; bool:\n    \"\"\"Validate inputs and save configuration.\"\"\"\n    values = self.get_field_values()\n\n    try:\n        # Type conversions\n        int_keys = {\"timeout\", \"max_retries\", \"default_max_pages\", \"default_max_depth\"}\n\n        for key, value in values.items():\n            if key in int_keys:\n                try:\n                    values[key] = int(value)\n                except ValueError:\n                    self.update_status(f\"\u2717 Invalid number: {key}\", \"error\")\n                    return False\n            elif key == \"storage_dir\":\n                path = Path(value).expanduser().resolve()\n                values[key] = path\n\n        # Create new config\n        new_config = UserConfig(**values)\n\n        # Ensure storage directory exists\n        new_config.ensure_storage_dir()\n\n        # Save\n        self.config_manager.save(new_config)\n        self.config = new_config\n        self.has_changes = False\n\n        self.update_status(\"\u2713 Saved successfully\", \"success\")\n        return True\n\n    except Exception as e:\n        self.update_status(f\"\u2717 Error: {e}\", \"error\")\n        return False\n</code></pre>"},{"location":"reference/ragcrawl/cli/config_tui/#ragcrawl.cli.config_tui.ConfirmDialog","title":"<code>ConfirmDialog(title, message)</code>","text":"<p>               Bases: <code>ModalScreen[bool]</code></p> <p>A confirmation dialog.</p> Source code in <code>src/ragcrawl/cli/config_tui.py</code> <pre><code>def __init__(self, title: str, message: str) -&gt; None:\n    super().__init__()\n    self.dialog_title = title\n    self.message = message\n</code></pre>"},{"location":"reference/ragcrawl/cli/config_tui/#ragcrawl.cli.config_tui.run_config_tui","title":"<code>run_config_tui()</code>","text":"<p>Run the configuration TUI.</p> Source code in <code>src/ragcrawl/cli/config_tui.py</code> <pre><code>def run_config_tui() -&gt; None:\n    \"\"\"Run the configuration TUI.\"\"\"\n    app = ConfigTUI()\n    app.run()\n</code></pre>"},{"location":"reference/ragcrawl/cli/main/","title":"Main","text":""},{"location":"reference/ragcrawl/cli/main/#ragcrawl.cli.main","title":"<code>main</code>","text":"<p>CLI entry point for ragcrawl.</p>"},{"location":"reference/ragcrawl/cli/main/#ragcrawl.cli.main.app","title":"<code>app()</code>","text":"<p>ragcrawl - Crawl websites and produce LLM-ready artifacts.</p> Source code in <code>src/ragcrawl/cli/main.py</code> <pre><code>@click.group()\n@click.version_option(version=__version__)\ndef app() -&gt; None:\n    \"\"\"ragcrawl - Crawl websites and produce LLM-ready artifacts.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/ragcrawl/cli/main/#ragcrawl.cli.main.config","title":"<code>config(ctx)</code>","text":"<p>Manage ragcrawl configuration.</p> <p>Run without subcommand to open interactive TUI editor.</p> Source code in <code>src/ragcrawl/cli/main.py</code> <pre><code>@app.group(invoke_without_command=True)\n@click.pass_context\ndef config(ctx: click.Context) -&gt; None:\n    \"\"\"Manage ragcrawl configuration.\n\n    Run without subcommand to open interactive TUI editor.\n    \"\"\"\n    if ctx.invoked_subcommand is None:\n        # Launch TUI when no subcommand is provided\n        from ragcrawl.cli.config_tui import run_config_tui\n\n        run_config_tui()\n</code></pre>"},{"location":"reference/ragcrawl/cli/main/#ragcrawl.cli.main.config_path","title":"<code>config_path()</code>","text":"<p>Show the path to the configuration file.</p> Source code in <code>src/ragcrawl/cli/main.py</code> <pre><code>@config.command(\"path\")\ndef config_path() -&gt; None:\n    \"\"\"Show the path to the configuration file.\"\"\"\n    from ragcrawl.config.user_config import get_config_manager\n\n    manager = get_config_manager()\n    click.echo(manager.config_file)\n</code></pre>"},{"location":"reference/ragcrawl/cli/main/#ragcrawl.cli.main.config_reset","title":"<code>config_reset()</code>","text":"<p>Reset configuration to defaults.</p> Source code in <code>src/ragcrawl/cli/main.py</code> <pre><code>@config.command(\"reset\")\n@click.confirmation_option(prompt=\"Are you sure you want to reset configuration to defaults?\")\ndef config_reset() -&gt; None:\n    \"\"\"Reset configuration to defaults.\"\"\"\n    from ragcrawl.config.user_config import get_config_manager\n\n    manager = get_config_manager()\n    manager.reset()\n    click.echo(\"Configuration reset to defaults.\")\n</code></pre>"},{"location":"reference/ragcrawl/cli/main/#ragcrawl.cli.main.config_set","title":"<code>config_set(key, value)</code>","text":"<p>Set a configuration value.</p> <p>KEY: Configuration key (e.g., storage_dir, user_agent, timeout) VALUE: Value to set</p> Source code in <code>src/ragcrawl/cli/main.py</code> <pre><code>@config.command(\"set\")\n@click.argument(\"key\")\n@click.argument(\"value\")\ndef config_set(key: str, value: str) -&gt; None:\n    \"\"\"Set a configuration value.\n\n    KEY: Configuration key (e.g., storage_dir, user_agent, timeout)\n    VALUE: Value to set\n    \"\"\"\n    from ragcrawl.config.user_config import get_config_manager\n\n    manager = get_config_manager()\n\n    # Handle type conversions\n    int_keys = {\"timeout\", \"max_retries\", \"default_max_pages\", \"default_max_depth\"}\n\n    try:\n        if key in int_keys:\n            typed_value: str | int | Path = int(value)\n        elif key == \"storage_dir\":\n            typed_value = Path(value).expanduser().resolve()\n        else:\n            typed_value = value\n\n        manager.set(key, typed_value)\n        click.echo(f\"Set {key} = {typed_value}\")\n\n        # Show warning if storage_dir changed\n        if key == \"storage_dir\":\n            click.echo(\n                click.style(\n                    \"\\nNote: Existing data in the old location will not be moved automatically.\",\n                    fg=\"yellow\",\n                )\n            )\n    except KeyError as e:\n        click.echo(click.style(f\"Error: {e}\", fg=\"red\"))\n        click.echo(\"\\nValid keys: storage_dir, db_name, user_agent, timeout, max_retries, \"\n                   \"default_max_pages, default_max_depth\")\n        sys.exit(1)\n    except ValueError as e:\n        click.echo(click.style(f\"Error: Invalid value - {e}\", fg=\"red\"))\n        sys.exit(1)\n</code></pre>"},{"location":"reference/ragcrawl/cli/main/#ragcrawl.cli.main.config_show","title":"<code>config_show()</code>","text":"<p>Show current configuration.</p> Source code in <code>src/ragcrawl/cli/main.py</code> <pre><code>@config.command(\"show\")\ndef config_show() -&gt; None:\n    \"\"\"Show current configuration.\"\"\"\n    from ragcrawl.config.user_config import get_config_manager\n\n    manager = get_config_manager()\n    cfg = manager.load()\n\n    click.echo(\"ragcrawl Configuration\")\n    click.echo(\"=\" * 40)\n    click.echo(f\"Config file: {manager.config_file}\")\n    click.echo(f\"Config exists: {manager.config_file.exists()}\")\n    click.echo()\n    click.echo(\"Settings:\")\n    click.echo(f\"  storage_dir:       {cfg.storage_dir}\")\n    click.echo(f\"  db_name:           {cfg.db_name}\")\n    click.echo(f\"  db_path:           {cfg.db_path}\")\n    click.echo(f\"  user_agent:        {cfg.user_agent}\")\n    click.echo(f\"  timeout:           {cfg.timeout}s\")\n    click.echo(f\"  max_retries:       {cfg.max_retries}\")\n    click.echo(f\"  default_max_pages: {cfg.default_max_pages}\")\n    click.echo(f\"  default_max_depth: {cfg.default_max_depth}\")\n</code></pre>"},{"location":"reference/ragcrawl/cli/main/#ragcrawl.cli.main.crawl","title":"<code>crawl(seeds, max_pages, max_depth, output, output_mode, storage, include, exclude, robots, js, export_json, export_jsonl, verbose)</code>","text":"<p>Crawl websites from seed URLs.</p> <p>SEEDS: One or more URLs to start crawling from.</p> Source code in <code>src/ragcrawl/cli/main.py</code> <pre><code>@app.command()\n@click.argument(\"seeds\", nargs=-1, required=True)\n@click.option(\n    \"--max-pages\",\n    \"-m\",\n    default=None,\n    type=int,\n    help=\"Maximum pages to crawl (default: from config).\",\n)\n@click.option(\n    \"--max-depth\",\n    \"-d\",\n    default=None,\n    type=int,\n    help=\"Maximum crawl depth (default: from config).\",\n)\n@click.option(\n    \"--output\",\n    \"-o\",\n    default=\"./output\",\n    help=\"Output directory.\",\n)\n@click.option(\n    \"--output-mode\",\n    type=click.Choice([\"single\", \"multi\"]),\n    default=\"multi\",\n    help=\"Output mode: single file or multi-page.\",\n)\n@click.option(\n    \"--storage\",\n    \"-s\",\n    type=click.Path(),\n    help=\"DuckDB storage path (default: ~/.ragcrawl/ragcrawl.duckdb).\",\n)\n@click.option(\n    \"--include\",\n    \"-i\",\n    multiple=True,\n    help=\"Include URL patterns (regex).\",\n)\n@click.option(\n    \"--exclude\",\n    \"-e\",\n    multiple=True,\n    help=\"Exclude URL patterns (regex).\",\n)\n@click.option(\n    \"--robots/--no-robots\",\n    default=True,\n    help=\"Respect robots.txt.\",\n)\n@click.option(\n    \"--js/--no-js\",\n    default=False,\n    help=\"Enable JavaScript rendering.\",\n)\n@click.option(\n    \"--export-json\",\n    type=click.Path(),\n    help=\"Export documents to JSON file.\",\n)\n@click.option(\n    \"--export-jsonl\",\n    type=click.Path(),\n    help=\"Export documents to JSONL file.\",\n)\n@click.option(\n    \"--verbose\",\n    \"-v\",\n    is_flag=True,\n    help=\"Verbose output.\",\n)\ndef crawl(\n    seeds: tuple[str, ...],\n    max_pages: Optional[int],\n    max_depth: Optional[int],\n    output: str,\n    output_mode: str,\n    storage: Optional[str],\n    include: tuple[str, ...],\n    exclude: tuple[str, ...],\n    robots: bool,\n    js: bool,\n    export_json: Optional[str],\n    export_jsonl: Optional[str],\n    verbose: bool,\n) -&gt; None:\n    \"\"\"\n    Crawl websites from seed URLs.\n\n    SEEDS: One or more URLs to start crawling from.\n    \"\"\"\n    from ragcrawl.config.crawler_config import CrawlerConfig, FetchMode, RobotsMode\n    from ragcrawl.config.output_config import OutputConfig, OutputMode\n    from ragcrawl.config.storage_config import DuckDBConfig, StorageConfig\n    from ragcrawl.config.user_config import get_user_config\n    from ragcrawl.core.crawl_job import CrawlJob\n    from ragcrawl.utils.logging import setup_logging\n\n    import logging\n\n    setup_logging(level=logging.DEBUG if verbose else logging.INFO)\n\n    # Get user config for defaults\n    user_cfg = get_user_config()\n\n    # Use defaults from user config if not specified\n    if max_pages is None:\n        max_pages = user_cfg.default_max_pages\n    if max_depth is None:\n        max_depth = user_cfg.default_max_depth\n\n    # Use centralized storage by default\n    storage_path = Path(storage) if storage else user_cfg.db_path\n\n    # Ensure storage directory exists\n    storage_path.parent.mkdir(parents=True, exist_ok=True)\n\n    # Build config\n    config = CrawlerConfig(\n        seeds=list(seeds),\n        max_pages=max_pages,\n        max_depth=max_depth,\n        include_patterns=list(include),\n        exclude_patterns=list(exclude),\n        robots_mode=RobotsMode.STRICT if robots else RobotsMode.OFF,\n        fetch_mode=FetchMode.BROWSER if js else FetchMode.HTTP,\n        storage=StorageConfig(backend=DuckDBConfig(path=storage_path)),\n        output=OutputConfig(\n            mode=OutputMode.SINGLE if output_mode == \"single\" else OutputMode.MULTI,\n            root_dir=output,\n        ),\n    )\n\n    click.echo(f\"Starting crawl of {len(seeds)} seed URL(s)...\")\n    click.echo(f\"  Max pages: {max_pages}\")\n    click.echo(f\"  Max depth: {max_depth}\")\n    click.echo(f\"  Output: {output} ({output_mode} mode)\")\n    click.echo(f\"  Storage: {storage_path}\")\n\n    # Run crawl\n    job = CrawlJob(config)\n    result = asyncio.run(job.run())\n\n    if result.success:\n        click.echo(click.style(\"\\nCrawl completed successfully!\", fg=\"green\"))\n        click.echo(f\"  Pages crawled: {result.stats.pages_crawled}\")\n        click.echo(f\"  Pages failed: {result.stats.pages_failed}\")\n        click.echo(f\"  Duration: {result.duration_seconds:.1f}s\")\n\n        # Publish output\n        if result.documents:\n            from ragcrawl.output.multi_page import MultiPagePublisher\n            from ragcrawl.output.single_page import SinglePagePublisher\n\n            if output_mode == \"single\":\n                publisher = SinglePagePublisher(config.output)\n            else:\n                publisher = MultiPagePublisher(config.output)\n\n            files = publisher.publish(result.documents)\n            click.echo(f\"  Output files: {len(files)}\")\n\n        # Export if requested\n        if export_json or export_jsonl:\n            from ragcrawl.export.json_exporter import JSONExporter, JSONLExporter\n\n            if export_json:\n                exporter = JSONExporter()\n                exporter.export_documents(result.documents, Path(export_json))\n                click.echo(f\"  Exported JSON: {export_json}\")\n\n            if export_jsonl:\n                exporter = JSONLExporter()\n                exporter.export_documents(result.documents, Path(export_jsonl))\n                click.echo(f\"  Exported JSONL: {export_jsonl}\")\n\n    else:\n        click.echo(click.style(f\"\\nCrawl failed: {result.error}\", fg=\"red\"))\n        sys.exit(1)\n</code></pre>"},{"location":"reference/ragcrawl/cli/main/#ragcrawl.cli.main.get_storage_path","title":"<code>get_storage_path()</code>","text":"<p>Get the default storage path from user config.</p> Source code in <code>src/ragcrawl/cli/main.py</code> <pre><code>def get_storage_path() -&gt; Path:\n    \"\"\"Get the default storage path from user config.\"\"\"\n    from ragcrawl.config.user_config import get_config_manager\n\n    manager = get_config_manager()\n    config = manager.ensure_initialized()\n    return config.db_path\n</code></pre>"},{"location":"reference/ragcrawl/cli/main/#ragcrawl.cli.main.list_runs","title":"<code>list_runs(storage, limit, site, status)</code>","text":"<p>List all crawl runs.</p> <p>Shows a summary of all crawls recorded in the database with useful information like site/seed, run ID, status, timing, and pages crawled.</p> Source code in <code>src/ragcrawl/cli/main.py</code> <pre><code>@app.command(\"list\")\n@click.option(\n    \"--storage\",\n    \"-s\",\n    type=click.Path(),\n    help=\"DuckDB storage path (default: ~/.ragcrawl/ragcrawl.duckdb).\",\n)\n@click.option(\n    \"--limit\",\n    \"-l\",\n    default=20,\n    help=\"Maximum number of runs to show.\",\n)\n@click.option(\n    \"--site\",\n    help=\"Filter by site ID.\",\n)\n@click.option(\n    \"--status\",\n    type=click.Choice([\"running\", \"completed\", \"partial\", \"failed\"]),\n    help=\"Filter by status.\",\n)\ndef list_runs(\n    storage: Optional[str],\n    limit: int,\n    site: Optional[str],\n    status: Optional[str],\n) -&gt; None:\n    \"\"\"List all crawl runs.\n\n    Shows a summary of all crawls recorded in the database with useful\n    information like site/seed, run ID, status, timing, and pages crawled.\n    \"\"\"\n    from ragcrawl.config.storage_config import DuckDBConfig, StorageConfig\n    from ragcrawl.storage.backend import create_storage_backend\n\n    storage_path = Path(storage) if storage else get_storage_path()\n\n    if not storage_path.exists():\n        click.echo(f\"No database found at: {storage_path}\")\n        click.echo(\"Run 'ragcrawl crawl &lt;url&gt;' to start crawling.\")\n        return\n\n    config = StorageConfig(backend=DuckDBConfig(path=storage_path))\n    backend = create_storage_backend(config)\n    backend.initialize()\n\n    # Get all sites first\n    site_list = backend.list_sites()\n\n    if not site_list:\n        click.echo(\"No crawl data found.\")\n        backend.close()\n        return\n\n    # Build site lookup\n    sites_by_id = {s.site_id: s for s in site_list}\n\n    # Collect all runs\n    all_runs = []\n    for s in site_list:\n        if site and s.site_id != site:\n            continue\n        runs = backend.list_runs(s.site_id, limit=limit)\n        for run in runs:\n            if status and run.status.value != status:\n                continue\n            all_runs.append((s, run))\n\n    if not all_runs:\n        click.echo(\"No runs found matching criteria.\")\n        backend.close()\n        return\n\n    # Sort by start time (newest first)\n    all_runs.sort(key=lambda x: x[1].started_at or datetime.min, reverse=True)\n    all_runs = all_runs[:limit]\n\n    # Display header\n    click.echo()\n    click.echo(\n        f\"{'RUN ID':&lt;36} {'STATUS':&lt;10} {'SITE':&lt;20} {'PAGES':&lt;8} {'DURATION':&lt;10} {'STARTED':&lt;20}\"\n    )\n    click.echo(\"-\" * 110)\n\n    for site_info, run in all_runs:\n        # Format status with color\n        status_colors = {\n            \"completed\": \"green\",\n            \"partial\": \"yellow\",\n            \"failed\": \"red\",\n            \"running\": \"blue\",\n        }\n        status_str = click.style(\n            run.status.value.ljust(10),\n            fg=status_colors.get(run.status.value, \"white\"),\n        )\n\n        # Format site (truncate if needed)\n        site_name = site_info.name or site_info.site_id\n        if len(site_name) &gt; 20:\n            site_name = site_name[:17] + \"...\"\n\n        # Format pages\n        pages_str = f\"{run.stats.pages_crawled}/{run.stats.pages_failed}\"\n\n        # Format duration\n        if run.duration_seconds:\n            if run.duration_seconds &lt; 60:\n                duration_str = f\"{run.duration_seconds:.1f}s\"\n            elif run.duration_seconds &lt; 3600:\n                duration_str = f\"{run.duration_seconds / 60:.1f}m\"\n            else:\n                duration_str = f\"{run.duration_seconds / 3600:.1f}h\"\n        else:\n            duration_str = \"-\"\n\n        # Format started time\n        if run.started_at:\n            started_str = run.started_at.strftime(\"%Y-%m-%d %H:%M\")\n        else:\n            started_str = \"-\"\n\n        click.echo(\n            f\"{run.run_id:&lt;36} {status_str} {site_name:&lt;20} {pages_str:&lt;8} {duration_str:&lt;10} {started_str:&lt;20}\"\n        )\n\n    click.echo()\n    click.echo(f\"Total: {len(all_runs)} run(s)\")\n\n    # Show seeds for context\n    if len(site_list) &lt;= 5:\n        click.echo()\n        click.echo(\"Sites:\")\n        for s in site_list:\n            seeds_str = \", \".join(s.seeds[:2])\n            if len(s.seeds) &gt; 2:\n                seeds_str += f\" (+{len(s.seeds) - 2} more)\"\n            click.echo(f\"  {s.site_id}: {seeds_str}\")\n\n    backend.close()\n</code></pre>"},{"location":"reference/ragcrawl/cli/main/#ragcrawl.cli.main.runs","title":"<code>runs(site_id, storage, limit)</code>","text":"<p>List crawl runs for a site.</p> Source code in <code>src/ragcrawl/cli/main.py</code> <pre><code>@app.command()\n@click.argument(\"site_id\")\n@click.option(\n    \"--storage\",\n    \"-s\",\n    type=click.Path(),\n    help=\"DuckDB storage path (default: ~/.ragcrawl/ragcrawl.duckdb).\",\n)\n@click.option(\n    \"--limit\",\n    \"-l\",\n    default=10,\n    help=\"Number of runs to show.\",\n)\ndef runs(site_id: str, storage: Optional[str], limit: int) -&gt; None:\n    \"\"\"List crawl runs for a site.\"\"\"\n    from ragcrawl.config.storage_config import DuckDBConfig, StorageConfig\n    from ragcrawl.storage.backend import create_storage_backend\n\n    storage_path = Path(storage) if storage else get_storage_path()\n\n    if not storage_path.exists():\n        click.echo(f\"No database found at: {storage_path}\")\n        return\n\n    config = StorageConfig(backend=DuckDBConfig(path=storage_path))\n    backend = create_storage_backend(config)\n    backend.initialize()\n\n    run_list = backend.list_runs(site_id, limit=limit)\n\n    if not run_list:\n        click.echo(f\"No runs found for site: {site_id}\")\n        backend.close()\n        return\n\n    click.echo(f\"Runs for site {site_id}:\\n\")\n    for run in run_list:\n        status_color = {\n            \"completed\": \"green\",\n            \"partial\": \"yellow\",\n            \"failed\": \"red\",\n            \"running\": \"blue\",\n        }.get(run.status.value, \"white\")\n\n        click.echo(f\"Run: {run.run_id}\")\n        click.echo(f\"  Status: \" + click.style(run.status.value, fg=status_color))\n        click.echo(f\"  Started: {run.started_at.isoformat() if run.started_at else 'N/A'}\")\n        if run.duration_seconds:\n            click.echo(f\"  Duration: {run.duration_seconds:.1f}s\")\n        click.echo(f\"  Pages: {run.stats.pages_crawled} crawled, {run.stats.pages_failed} failed\")\n        click.echo()\n\n    backend.close()\n</code></pre>"},{"location":"reference/ragcrawl/cli/main/#ragcrawl.cli.main.sites","title":"<code>sites(storage)</code>","text":"<p>List all crawled sites.</p> Source code in <code>src/ragcrawl/cli/main.py</code> <pre><code>@app.command()\n@click.option(\n    \"--storage\",\n    \"-s\",\n    type=click.Path(),\n    help=\"DuckDB storage path (default: ~/.ragcrawl/ragcrawl.duckdb).\",\n)\ndef sites(storage: Optional[str]) -&gt; None:\n    \"\"\"List all crawled sites.\"\"\"\n    from ragcrawl.config.storage_config import DuckDBConfig, StorageConfig\n    from ragcrawl.storage.backend import create_storage_backend\n\n    storage_path = Path(storage) if storage else get_storage_path()\n\n    if not storage_path.exists():\n        click.echo(f\"No database found at: {storage_path}\")\n        click.echo(\"Run 'ragcrawl crawl &lt;url&gt;' to start crawling.\")\n        return\n\n    config = StorageConfig(backend=DuckDBConfig(path=storage_path))\n    backend = create_storage_backend(config)\n    backend.initialize()\n\n    site_list = backend.list_sites()\n\n    if not site_list:\n        click.echo(\"No sites found.\")\n        backend.close()\n        return\n\n    click.echo(f\"Found {len(site_list)} site(s):\\n\")\n    for site in site_list:\n        click.echo(f\"ID: {site.site_id}\")\n        click.echo(f\"  Name: {site.name}\")\n        click.echo(f\"  Seeds: {', '.join(site.seeds[:3])}\")\n        click.echo(f\"  Pages: {site.total_pages}\")\n        click.echo(f\"  Runs: {site.total_runs}\")\n        if site.last_crawl_at:\n            click.echo(f\"  Last crawl: {site.last_crawl_at.isoformat()}\")\n        click.echo()\n\n    backend.close()\n</code></pre>"},{"location":"reference/ragcrawl/cli/main/#ragcrawl.cli.main.sync","title":"<code>sync(site_id, storage, max_pages, max_age, output, verbose)</code>","text":"<p>Sync a previously crawled site for changes.</p> <p>SITE_ID: ID of the site to sync.</p> Source code in <code>src/ragcrawl/cli/main.py</code> <pre><code>@app.command()\n@click.argument(\"site_id\")\n@click.option(\n    \"--storage\",\n    \"-s\",\n    type=click.Path(),\n    help=\"DuckDB storage path (default: ~/.ragcrawl/ragcrawl.duckdb).\",\n)\n@click.option(\n    \"--max-pages\",\n    \"-m\",\n    type=int,\n    help=\"Maximum pages to sync.\",\n)\n@click.option(\n    \"--max-age\",\n    type=float,\n    help=\"Only check pages older than N hours.\",\n)\n@click.option(\n    \"--output\",\n    \"-o\",\n    help=\"Output directory for updates.\",\n)\n@click.option(\n    \"--verbose\",\n    \"-v\",\n    is_flag=True,\n    help=\"Verbose output.\",\n)\ndef sync(\n    site_id: str,\n    storage: Optional[str],\n    max_pages: Optional[int],\n    max_age: Optional[float],\n    output: Optional[str],\n    verbose: bool,\n) -&gt; None:\n    \"\"\"\n    Sync a previously crawled site for changes.\n\n    SITE_ID: ID of the site to sync.\n    \"\"\"\n    from ragcrawl.config.output_config import OutputConfig\n    from ragcrawl.config.storage_config import DuckDBConfig, StorageConfig\n    from ragcrawl.config.sync_config import SyncConfig\n    from ragcrawl.config.user_config import get_user_config\n    from ragcrawl.core.sync_job import SyncJob\n    from ragcrawl.utils.logging import setup_logging\n\n    import logging\n\n    setup_logging(level=logging.DEBUG if verbose else logging.INFO)\n\n    # Use centralized storage by default\n    user_cfg = get_user_config()\n    storage_path = Path(storage) if storage else user_cfg.db_path\n\n    # Build config\n    sync_config = SyncConfig(\n        site_id=site_id,\n        max_pages=max_pages,\n        max_age_hours=max_age,\n        storage=StorageConfig(backend=DuckDBConfig(path=storage_path)),\n        output=OutputConfig(root_dir=output) if output else None,\n    )\n\n    click.echo(f\"Starting sync for site: {site_id}\")\n    click.echo(f\"  Storage: {storage_path}\")\n\n    # Run sync\n    job = SyncJob(sync_config)\n    result = asyncio.run(job.run())\n\n    if result.success:\n        click.echo(click.style(\"\\nSync completed successfully!\", fg=\"green\"))\n        click.echo(f\"  Pages checked: {result.stats.pages_crawled}\")\n        click.echo(f\"  Pages changed: {result.stats.pages_changed}\")\n        click.echo(f\"  Pages deleted: {result.stats.pages_deleted}\")\n        click.echo(f\"  Duration: {result.duration_seconds:.1f}s\")\n\n        if result.changed_pages:\n            click.echo(\"\\nChanged pages:\")\n            for url in result.changed_pages[:10]:\n                click.echo(f\"  - {url}\")\n            if len(result.changed_pages) &gt; 10:\n                click.echo(f\"  ... and {len(result.changed_pages) - 10} more\")\n\n    else:\n        click.echo(click.style(f\"\\nSync failed: {result.error}\", fg=\"red\"))\n        sys.exit(1)\n</code></pre>"},{"location":"reference/ragcrawl/config/","title":"Index","text":""},{"location":"reference/ragcrawl/config/#ragcrawl.config","title":"<code>config</code>","text":"<p>Configuration classes for ragcrawl.</p>"},{"location":"reference/ragcrawl/config/#ragcrawl.config.CrawlerConfig","title":"<code>CrawlerConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Main configuration for the ragcrawl.</p> <p>This is the primary configuration object that controls all aspects of crawling behavior.</p>"},{"location":"reference/ragcrawl/config/#ragcrawl.config.CrawlerConfig.get_allowed_domains","title":"<code>get_allowed_domains()</code>","text":"<p>Get the set of allowed domains including seed domains.</p> Source code in <code>src/ragcrawl/config/crawler_config.py</code> <pre><code>def get_allowed_domains(self) -&gt; set[str]:\n    \"\"\"Get the set of allowed domains including seed domains.\"\"\"\n    from urllib.parse import urlparse\n\n    domains = set(self.allowed_domains)\n    for seed in self.seeds:\n        parsed = urlparse(seed)\n        if parsed.netloc:\n            domains.add(parsed.netloc)\n    return domains\n</code></pre>"},{"location":"reference/ragcrawl/config/#ragcrawl.config.DeletionHandling","title":"<code>DeletionHandling</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>How to handle deleted pages in multi-page mode.</p>"},{"location":"reference/ragcrawl/config/#ragcrawl.config.DuckDBConfig","title":"<code>DuckDBConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for DuckDB storage backend.</p>"},{"location":"reference/ragcrawl/config/#ragcrawl.config.DynamoDBConfig","title":"<code>DynamoDBConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for DynamoDB storage backend.</p>"},{"location":"reference/ragcrawl/config/#ragcrawl.config.FetchMode","title":"<code>FetchMode</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Fetching mode for pages.</p>"},{"location":"reference/ragcrawl/config/#ragcrawl.config.OutputConfig","title":"<code>OutputConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for Markdown output/publishing.</p> <p>Supports single-page (concatenated) and multi-page (folder structure) modes.</p>"},{"location":"reference/ragcrawl/config/#ragcrawl.config.OutputConfig.output_path","title":"<code>output_path</code>  <code>property</code>","text":"<p>Get the output root path.</p>"},{"location":"reference/ragcrawl/config/#ragcrawl.config.OutputMode","title":"<code>OutputMode</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Markdown output mode.</p>"},{"location":"reference/ragcrawl/config/#ragcrawl.config.RobotsMode","title":"<code>RobotsMode</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Robots.txt compliance mode.</p>"},{"location":"reference/ragcrawl/config/#ragcrawl.config.StorageConfig","title":"<code>StorageConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Storage configuration supporting multiple backends.</p> <p>DuckDB is the default when no configuration is provided. DynamoDB is enabled only when explicitly configured.</p>"},{"location":"reference/ragcrawl/config/#ragcrawl.config.StorageConfig.storage_type","title":"<code>storage_type</code>  <code>property</code>","text":"<p>Get the storage type.</p>"},{"location":"reference/ragcrawl/config/#ragcrawl.config.StorageConfig.duckdb","title":"<code>duckdb(path=None)</code>  <code>classmethod</code>","text":"<p>Create a DuckDB storage configuration.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path | None</code> <p>Path to database file. If None, uses default (~/.ragcrawl/ragcrawl.duckdb).</p> <code>None</code> Source code in <code>src/ragcrawl/config/storage_config.py</code> <pre><code>@classmethod\ndef duckdb(cls, path: str | Path | None = None) -&gt; \"StorageConfig\":\n    \"\"\"Create a DuckDB storage configuration.\n\n    Args:\n        path: Path to database file. If None, uses default (~/.ragcrawl/ragcrawl.duckdb).\n    \"\"\"\n    if path is None:\n        return cls(backend=DuckDBConfig())\n    return cls(backend=DuckDBConfig(path=path))\n</code></pre>"},{"location":"reference/ragcrawl/config/#ragcrawl.config.StorageConfig.dynamodb","title":"<code>dynamodb(region='us-east-1', table_prefix='ragcrawl', **kwargs)</code>  <code>classmethod</code>","text":"<p>Create a DynamoDB storage configuration.</p> Source code in <code>src/ragcrawl/config/storage_config.py</code> <pre><code>@classmethod\ndef dynamodb(\n    cls,\n    region: str = \"us-east-1\",\n    table_prefix: str = \"ragcrawl\",\n    **kwargs: str | int | None,\n) -&gt; \"StorageConfig\":\n    \"\"\"Create a DynamoDB storage configuration.\"\"\"\n    return cls(backend=DynamoDBConfig(region=region, table_prefix=table_prefix, **kwargs))\n</code></pre>"},{"location":"reference/ragcrawl/config/#ragcrawl.config.StorageType","title":"<code>StorageType</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Supported storage backend types.</p>"},{"location":"reference/ragcrawl/config/#ragcrawl.config.SyncConfig","title":"<code>SyncConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for incremental sync/update operations.</p> <p>Sync operations detect and process only changed content, minimizing redundant work and API calls.</p>"},{"location":"reference/ragcrawl/config/#ragcrawl.config.SyncStrategy","title":"<code>SyncStrategy</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Strategy for detecting content changes.</p>"},{"location":"reference/ragcrawl/config/#ragcrawl.config.UserConfig","title":"<code>UserConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>User configuration settings.</p>"},{"location":"reference/ragcrawl/config/#ragcrawl.config.UserConfig.config_file","title":"<code>config_file</code>  <code>property</code>","text":"<p>Get the path to the config file.</p>"},{"location":"reference/ragcrawl/config/#ragcrawl.config.UserConfig.db_path","title":"<code>db_path</code>  <code>property</code>","text":"<p>Get the full path to the DuckDB database.</p>"},{"location":"reference/ragcrawl/config/#ragcrawl.config.UserConfig.ensure_storage_dir","title":"<code>ensure_storage_dir()</code>","text":"<p>Ensure the storage directory exists.</p> Source code in <code>src/ragcrawl/config/user_config.py</code> <pre><code>def ensure_storage_dir(self) -&gt; None:\n    \"\"\"Ensure the storage directory exists.\"\"\"\n    self.storage_dir.mkdir(parents=True, exist_ok=True)\n</code></pre>"},{"location":"reference/ragcrawl/config/#ragcrawl.config.UserConfigManager","title":"<code>UserConfigManager(config_dir=None)</code>","text":"<p>Manages loading and saving user configuration.</p> <p>Initialize the config manager.</p> <p>Parameters:</p> Name Type Description Default <code>config_dir</code> <code>Path | None</code> <p>Override the default config directory.</p> <code>None</code> Source code in <code>src/ragcrawl/config/user_config.py</code> <pre><code>def __init__(self, config_dir: Path | None = None) -&gt; None:\n    \"\"\"Initialize the config manager.\n\n    Args:\n        config_dir: Override the default config directory.\n    \"\"\"\n    self._config_dir = config_dir or get_default_data_dir()\n    self._config_file = self._config_dir / \"config.json\"\n    self._config: UserConfig | None = None\n</code></pre>"},{"location":"reference/ragcrawl/config/#ragcrawl.config.UserConfigManager.config_dir","title":"<code>config_dir</code>  <code>property</code>","text":"<p>Get the config directory.</p>"},{"location":"reference/ragcrawl/config/#ragcrawl.config.UserConfigManager.config_file","title":"<code>config_file</code>  <code>property</code>","text":"<p>Get the path to the config file.</p>"},{"location":"reference/ragcrawl/config/#ragcrawl.config.UserConfigManager.ensure_initialized","title":"<code>ensure_initialized()</code>","text":"<p>Ensure config is loaded and storage directory exists.</p> <p>Returns:</p> Type Description <code>UserConfig</code> <p>The loaded configuration.</p> Source code in <code>src/ragcrawl/config/user_config.py</code> <pre><code>def ensure_initialized(self) -&gt; UserConfig:\n    \"\"\"Ensure config is loaded and storage directory exists.\n\n    Returns:\n        The loaded configuration.\n    \"\"\"\n    config = self.load()\n    config.ensure_storage_dir()\n    return config\n</code></pre>"},{"location":"reference/ragcrawl/config/#ragcrawl.config.UserConfigManager.get","title":"<code>get(key)</code>","text":"<p>Get a configuration value.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Configuration key to retrieve.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The configuration value.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If key doesn't exist.</p> Source code in <code>src/ragcrawl/config/user_config.py</code> <pre><code>def get(self, key: str) -&gt; Any:\n    \"\"\"Get a configuration value.\n\n    Args:\n        key: Configuration key to retrieve.\n\n    Returns:\n        The configuration value.\n\n    Raises:\n        KeyError: If key doesn't exist.\n    \"\"\"\n    config = self.load()\n    if hasattr(config, key):\n        return getattr(config, key)\n    raise KeyError(f\"Unknown configuration key: {key}\")\n</code></pre>"},{"location":"reference/ragcrawl/config/#ragcrawl.config.UserConfigManager.load","title":"<code>load()</code>","text":"<p>Load configuration from file, or return defaults.</p> Source code in <code>src/ragcrawl/config/user_config.py</code> <pre><code>def load(self) -&gt; UserConfig:\n    \"\"\"Load configuration from file, or return defaults.\"\"\"\n    if self._config is not None:\n        return self._config\n\n    if self._config_file.exists():\n        try:\n            with open(self._config_file) as f:\n                data = json.load(f)\n            # Convert storage_dir back to Path\n            if \"storage_dir\" in data:\n                data[\"storage_dir\"] = Path(data[\"storage_dir\"])\n            self._config = UserConfig(**data)\n        except (json.JSONDecodeError, ValueError):\n            # Invalid config, use defaults\n            self._config = UserConfig()\n    else:\n        self._config = UserConfig()\n\n    return self._config\n</code></pre>"},{"location":"reference/ragcrawl/config/#ragcrawl.config.UserConfigManager.reset","title":"<code>reset()</code>","text":"<p>Reset configuration to defaults.</p> Source code in <code>src/ragcrawl/config/user_config.py</code> <pre><code>def reset(self) -&gt; None:\n    \"\"\"Reset configuration to defaults.\"\"\"\n    self._config = UserConfig()\n    self.save()\n</code></pre>"},{"location":"reference/ragcrawl/config/#ragcrawl.config.UserConfigManager.save","title":"<code>save(config=None)</code>","text":"<p>Save configuration to file.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>UserConfig | None</code> <p>Configuration to save. If None, saves current config.</p> <code>None</code> Source code in <code>src/ragcrawl/config/user_config.py</code> <pre><code>def save(self, config: UserConfig | None = None) -&gt; None:\n    \"\"\"Save configuration to file.\n\n    Args:\n        config: Configuration to save. If None, saves current config.\n    \"\"\"\n    if config is not None:\n        self._config = config\n\n    if self._config is None:\n        self._config = UserConfig()\n\n    # Ensure directory exists\n    self._config_dir.mkdir(parents=True, exist_ok=True)\n\n    # Serialize config\n    data = self._config.model_dump()\n    # Convert Path to string for JSON serialization\n    data[\"storage_dir\"] = str(data[\"storage_dir\"])\n\n    with open(self._config_file, \"w\") as f:\n        json.dump(data, f, indent=2)\n</code></pre>"},{"location":"reference/ragcrawl/config/#ragcrawl.config.UserConfigManager.set","title":"<code>set(key, value)</code>","text":"<p>Set a configuration value.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Configuration key to set.</p> required <code>value</code> <code>Any</code> <p>Value to set.</p> required <p>Raises:</p> Type Description <code>KeyError</code> <p>If key doesn't exist.</p> <code>ValueError</code> <p>If value is invalid.</p> Source code in <code>src/ragcrawl/config/user_config.py</code> <pre><code>def set(self, key: str, value: Any) -&gt; None:\n    \"\"\"Set a configuration value.\n\n    Args:\n        key: Configuration key to set.\n        value: Value to set.\n\n    Raises:\n        KeyError: If key doesn't exist.\n        ValueError: If value is invalid.\n    \"\"\"\n    config = self.load()\n    if not hasattr(config, key):\n        raise KeyError(f\"Unknown configuration key: {key}\")\n\n    # Handle Path conversion for storage_dir\n    if key == \"storage_dir\":\n        value = Path(value)\n\n    # Create new config with updated value\n    data = config.model_dump()\n    data[key] = value\n    self._config = UserConfig(**data)\n    self.save()\n</code></pre>"},{"location":"reference/ragcrawl/config/#ragcrawl.config.get_config_manager","title":"<code>get_config_manager()</code>","text":"<p>Get the global config manager instance.</p> Source code in <code>src/ragcrawl/config/user_config.py</code> <pre><code>def get_config_manager() -&gt; UserConfigManager:\n    \"\"\"Get the global config manager instance.\"\"\"\n    global _config_manager\n    if _config_manager is None:\n        _config_manager = UserConfigManager()\n    return _config_manager\n</code></pre>"},{"location":"reference/ragcrawl/config/#ragcrawl.config.get_default_storage_path","title":"<code>get_default_storage_path()</code>","text":"<p>Get the default storage path from user config.</p> Source code in <code>src/ragcrawl/config/user_config.py</code> <pre><code>def get_default_storage_path() -&gt; Path:\n    \"\"\"Get the default storage path from user config.\"\"\"\n    return get_user_config().db_path\n</code></pre>"},{"location":"reference/ragcrawl/config/#ragcrawl.config.get_user_config","title":"<code>get_user_config()</code>","text":"<p>Get the current user configuration.</p> Source code in <code>src/ragcrawl/config/user_config.py</code> <pre><code>def get_user_config() -&gt; UserConfig:\n    \"\"\"Get the current user configuration.\"\"\"\n    return get_config_manager().load()\n</code></pre>"},{"location":"reference/ragcrawl/config/crawler_config/","title":"Crawler config","text":""},{"location":"reference/ragcrawl/config/crawler_config/#ragcrawl.config.crawler_config","title":"<code>crawler_config</code>","text":"<p>Main crawler configuration.</p>"},{"location":"reference/ragcrawl/config/crawler_config/#ragcrawl.config.crawler_config.CrawlerConfig","title":"<code>CrawlerConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Main configuration for the ragcrawl.</p> <p>This is the primary configuration object that controls all aspects of crawling behavior.</p>"},{"location":"reference/ragcrawl/config/crawler_config/#ragcrawl.config.crawler_config.CrawlerConfig.get_allowed_domains","title":"<code>get_allowed_domains()</code>","text":"<p>Get the set of allowed domains including seed domains.</p> Source code in <code>src/ragcrawl/config/crawler_config.py</code> <pre><code>def get_allowed_domains(self) -&gt; set[str]:\n    \"\"\"Get the set of allowed domains including seed domains.\"\"\"\n    from urllib.parse import urlparse\n\n    domains = set(self.allowed_domains)\n    for seed in self.seeds:\n        parsed = urlparse(seed)\n        if parsed.netloc:\n            domains.add(parsed.netloc)\n    return domains\n</code></pre>"},{"location":"reference/ragcrawl/config/crawler_config/#ragcrawl.config.crawler_config.FetchMode","title":"<code>FetchMode</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Fetching mode for pages.</p>"},{"location":"reference/ragcrawl/config/crawler_config/#ragcrawl.config.crawler_config.QualityGateConfig","title":"<code>QualityGateConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for content quality gates.</p>"},{"location":"reference/ragcrawl/config/crawler_config/#ragcrawl.config.crawler_config.RateLimitConfig","title":"<code>RateLimitConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for rate limiting.</p>"},{"location":"reference/ragcrawl/config/crawler_config/#ragcrawl.config.crawler_config.RetryConfig","title":"<code>RetryConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for retry behavior.</p>"},{"location":"reference/ragcrawl/config/crawler_config/#ragcrawl.config.crawler_config.RobotsMode","title":"<code>RobotsMode</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Robots.txt compliance mode.</p>"},{"location":"reference/ragcrawl/config/output_config/","title":"Output config","text":""},{"location":"reference/ragcrawl/config/output_config/#ragcrawl.config.output_config","title":"<code>output_config</code>","text":"<p>Output/publishing configuration for Markdown export.</p>"},{"location":"reference/ragcrawl/config/output_config/#ragcrawl.config.output_config.DeletionHandling","title":"<code>DeletionHandling</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>How to handle deleted pages in multi-page mode.</p>"},{"location":"reference/ragcrawl/config/output_config/#ragcrawl.config.output_config.OutputConfig","title":"<code>OutputConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for Markdown output/publishing.</p> <p>Supports single-page (concatenated) and multi-page (folder structure) modes.</p>"},{"location":"reference/ragcrawl/config/output_config/#ragcrawl.config.output_config.OutputConfig.output_path","title":"<code>output_path</code>  <code>property</code>","text":"<p>Get the output root path.</p>"},{"location":"reference/ragcrawl/config/output_config/#ragcrawl.config.output_config.OutputMode","title":"<code>OutputMode</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Markdown output mode.</p>"},{"location":"reference/ragcrawl/config/storage_config/","title":"Storage config","text":""},{"location":"reference/ragcrawl/config/storage_config/#ragcrawl.config.storage_config","title":"<code>storage_config</code>","text":"<p>Storage configuration for different backends.</p>"},{"location":"reference/ragcrawl/config/storage_config/#ragcrawl.config.storage_config.DuckDBConfig","title":"<code>DuckDBConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for DuckDB storage backend.</p>"},{"location":"reference/ragcrawl/config/storage_config/#ragcrawl.config.storage_config.DynamoDBConfig","title":"<code>DynamoDBConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for DynamoDB storage backend.</p>"},{"location":"reference/ragcrawl/config/storage_config/#ragcrawl.config.storage_config.StorageConfig","title":"<code>StorageConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Storage configuration supporting multiple backends.</p> <p>DuckDB is the default when no configuration is provided. DynamoDB is enabled only when explicitly configured.</p>"},{"location":"reference/ragcrawl/config/storage_config/#ragcrawl.config.storage_config.StorageConfig.storage_type","title":"<code>storage_type</code>  <code>property</code>","text":"<p>Get the storage type.</p>"},{"location":"reference/ragcrawl/config/storage_config/#ragcrawl.config.storage_config.StorageConfig.duckdb","title":"<code>duckdb(path=None)</code>  <code>classmethod</code>","text":"<p>Create a DuckDB storage configuration.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path | None</code> <p>Path to database file. If None, uses default (~/.ragcrawl/ragcrawl.duckdb).</p> <code>None</code> Source code in <code>src/ragcrawl/config/storage_config.py</code> <pre><code>@classmethod\ndef duckdb(cls, path: str | Path | None = None) -&gt; \"StorageConfig\":\n    \"\"\"Create a DuckDB storage configuration.\n\n    Args:\n        path: Path to database file. If None, uses default (~/.ragcrawl/ragcrawl.duckdb).\n    \"\"\"\n    if path is None:\n        return cls(backend=DuckDBConfig())\n    return cls(backend=DuckDBConfig(path=path))\n</code></pre>"},{"location":"reference/ragcrawl/config/storage_config/#ragcrawl.config.storage_config.StorageConfig.dynamodb","title":"<code>dynamodb(region='us-east-1', table_prefix='ragcrawl', **kwargs)</code>  <code>classmethod</code>","text":"<p>Create a DynamoDB storage configuration.</p> Source code in <code>src/ragcrawl/config/storage_config.py</code> <pre><code>@classmethod\ndef dynamodb(\n    cls,\n    region: str = \"us-east-1\",\n    table_prefix: str = \"ragcrawl\",\n    **kwargs: str | int | None,\n) -&gt; \"StorageConfig\":\n    \"\"\"Create a DynamoDB storage configuration.\"\"\"\n    return cls(backend=DynamoDBConfig(region=region, table_prefix=table_prefix, **kwargs))\n</code></pre>"},{"location":"reference/ragcrawl/config/storage_config/#ragcrawl.config.storage_config.StorageType","title":"<code>StorageType</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Supported storage backend types.</p>"},{"location":"reference/ragcrawl/config/sync_config/","title":"Sync config","text":""},{"location":"reference/ragcrawl/config/sync_config/#ragcrawl.config.sync_config","title":"<code>sync_config</code>","text":"<p>Configuration for incremental sync operations.</p>"},{"location":"reference/ragcrawl/config/sync_config/#ragcrawl.config.sync_config.SyncConfig","title":"<code>SyncConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for incremental sync/update operations.</p> <p>Sync operations detect and process only changed content, minimizing redundant work and API calls.</p>"},{"location":"reference/ragcrawl/config/sync_config/#ragcrawl.config.sync_config.SyncStrategy","title":"<code>SyncStrategy</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Strategy for detecting content changes.</p>"},{"location":"reference/ragcrawl/config/user_config/","title":"User config","text":""},{"location":"reference/ragcrawl/config/user_config/#ragcrawl.config.user_config","title":"<code>user_config</code>","text":"<p>User configuration management for ragcrawl.</p> <p>Manages persistent user settings stored in ~/.ragcrawl/config.json</p>"},{"location":"reference/ragcrawl/config/user_config/#ragcrawl.config.user_config.UserConfig","title":"<code>UserConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>User configuration settings.</p>"},{"location":"reference/ragcrawl/config/user_config/#ragcrawl.config.user_config.UserConfig.config_file","title":"<code>config_file</code>  <code>property</code>","text":"<p>Get the path to the config file.</p>"},{"location":"reference/ragcrawl/config/user_config/#ragcrawl.config.user_config.UserConfig.db_path","title":"<code>db_path</code>  <code>property</code>","text":"<p>Get the full path to the DuckDB database.</p>"},{"location":"reference/ragcrawl/config/user_config/#ragcrawl.config.user_config.UserConfig.ensure_storage_dir","title":"<code>ensure_storage_dir()</code>","text":"<p>Ensure the storage directory exists.</p> Source code in <code>src/ragcrawl/config/user_config.py</code> <pre><code>def ensure_storage_dir(self) -&gt; None:\n    \"\"\"Ensure the storage directory exists.\"\"\"\n    self.storage_dir.mkdir(parents=True, exist_ok=True)\n</code></pre>"},{"location":"reference/ragcrawl/config/user_config/#ragcrawl.config.user_config.UserConfigManager","title":"<code>UserConfigManager(config_dir=None)</code>","text":"<p>Manages loading and saving user configuration.</p> <p>Initialize the config manager.</p> <p>Parameters:</p> Name Type Description Default <code>config_dir</code> <code>Path | None</code> <p>Override the default config directory.</p> <code>None</code> Source code in <code>src/ragcrawl/config/user_config.py</code> <pre><code>def __init__(self, config_dir: Path | None = None) -&gt; None:\n    \"\"\"Initialize the config manager.\n\n    Args:\n        config_dir: Override the default config directory.\n    \"\"\"\n    self._config_dir = config_dir or get_default_data_dir()\n    self._config_file = self._config_dir / \"config.json\"\n    self._config: UserConfig | None = None\n</code></pre>"},{"location":"reference/ragcrawl/config/user_config/#ragcrawl.config.user_config.UserConfigManager.config_dir","title":"<code>config_dir</code>  <code>property</code>","text":"<p>Get the config directory.</p>"},{"location":"reference/ragcrawl/config/user_config/#ragcrawl.config.user_config.UserConfigManager.config_file","title":"<code>config_file</code>  <code>property</code>","text":"<p>Get the path to the config file.</p>"},{"location":"reference/ragcrawl/config/user_config/#ragcrawl.config.user_config.UserConfigManager.ensure_initialized","title":"<code>ensure_initialized()</code>","text":"<p>Ensure config is loaded and storage directory exists.</p> <p>Returns:</p> Type Description <code>UserConfig</code> <p>The loaded configuration.</p> Source code in <code>src/ragcrawl/config/user_config.py</code> <pre><code>def ensure_initialized(self) -&gt; UserConfig:\n    \"\"\"Ensure config is loaded and storage directory exists.\n\n    Returns:\n        The loaded configuration.\n    \"\"\"\n    config = self.load()\n    config.ensure_storage_dir()\n    return config\n</code></pre>"},{"location":"reference/ragcrawl/config/user_config/#ragcrawl.config.user_config.UserConfigManager.get","title":"<code>get(key)</code>","text":"<p>Get a configuration value.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Configuration key to retrieve.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The configuration value.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If key doesn't exist.</p> Source code in <code>src/ragcrawl/config/user_config.py</code> <pre><code>def get(self, key: str) -&gt; Any:\n    \"\"\"Get a configuration value.\n\n    Args:\n        key: Configuration key to retrieve.\n\n    Returns:\n        The configuration value.\n\n    Raises:\n        KeyError: If key doesn't exist.\n    \"\"\"\n    config = self.load()\n    if hasattr(config, key):\n        return getattr(config, key)\n    raise KeyError(f\"Unknown configuration key: {key}\")\n</code></pre>"},{"location":"reference/ragcrawl/config/user_config/#ragcrawl.config.user_config.UserConfigManager.load","title":"<code>load()</code>","text":"<p>Load configuration from file, or return defaults.</p> Source code in <code>src/ragcrawl/config/user_config.py</code> <pre><code>def load(self) -&gt; UserConfig:\n    \"\"\"Load configuration from file, or return defaults.\"\"\"\n    if self._config is not None:\n        return self._config\n\n    if self._config_file.exists():\n        try:\n            with open(self._config_file) as f:\n                data = json.load(f)\n            # Convert storage_dir back to Path\n            if \"storage_dir\" in data:\n                data[\"storage_dir\"] = Path(data[\"storage_dir\"])\n            self._config = UserConfig(**data)\n        except (json.JSONDecodeError, ValueError):\n            # Invalid config, use defaults\n            self._config = UserConfig()\n    else:\n        self._config = UserConfig()\n\n    return self._config\n</code></pre>"},{"location":"reference/ragcrawl/config/user_config/#ragcrawl.config.user_config.UserConfigManager.reset","title":"<code>reset()</code>","text":"<p>Reset configuration to defaults.</p> Source code in <code>src/ragcrawl/config/user_config.py</code> <pre><code>def reset(self) -&gt; None:\n    \"\"\"Reset configuration to defaults.\"\"\"\n    self._config = UserConfig()\n    self.save()\n</code></pre>"},{"location":"reference/ragcrawl/config/user_config/#ragcrawl.config.user_config.UserConfigManager.save","title":"<code>save(config=None)</code>","text":"<p>Save configuration to file.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>UserConfig | None</code> <p>Configuration to save. If None, saves current config.</p> <code>None</code> Source code in <code>src/ragcrawl/config/user_config.py</code> <pre><code>def save(self, config: UserConfig | None = None) -&gt; None:\n    \"\"\"Save configuration to file.\n\n    Args:\n        config: Configuration to save. If None, saves current config.\n    \"\"\"\n    if config is not None:\n        self._config = config\n\n    if self._config is None:\n        self._config = UserConfig()\n\n    # Ensure directory exists\n    self._config_dir.mkdir(parents=True, exist_ok=True)\n\n    # Serialize config\n    data = self._config.model_dump()\n    # Convert Path to string for JSON serialization\n    data[\"storage_dir\"] = str(data[\"storage_dir\"])\n\n    with open(self._config_file, \"w\") as f:\n        json.dump(data, f, indent=2)\n</code></pre>"},{"location":"reference/ragcrawl/config/user_config/#ragcrawl.config.user_config.UserConfigManager.set","title":"<code>set(key, value)</code>","text":"<p>Set a configuration value.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Configuration key to set.</p> required <code>value</code> <code>Any</code> <p>Value to set.</p> required <p>Raises:</p> Type Description <code>KeyError</code> <p>If key doesn't exist.</p> <code>ValueError</code> <p>If value is invalid.</p> Source code in <code>src/ragcrawl/config/user_config.py</code> <pre><code>def set(self, key: str, value: Any) -&gt; None:\n    \"\"\"Set a configuration value.\n\n    Args:\n        key: Configuration key to set.\n        value: Value to set.\n\n    Raises:\n        KeyError: If key doesn't exist.\n        ValueError: If value is invalid.\n    \"\"\"\n    config = self.load()\n    if not hasattr(config, key):\n        raise KeyError(f\"Unknown configuration key: {key}\")\n\n    # Handle Path conversion for storage_dir\n    if key == \"storage_dir\":\n        value = Path(value)\n\n    # Create new config with updated value\n    data = config.model_dump()\n    data[key] = value\n    self._config = UserConfig(**data)\n    self.save()\n</code></pre>"},{"location":"reference/ragcrawl/config/user_config/#ragcrawl.config.user_config.get_config_manager","title":"<code>get_config_manager()</code>","text":"<p>Get the global config manager instance.</p> Source code in <code>src/ragcrawl/config/user_config.py</code> <pre><code>def get_config_manager() -&gt; UserConfigManager:\n    \"\"\"Get the global config manager instance.\"\"\"\n    global _config_manager\n    if _config_manager is None:\n        _config_manager = UserConfigManager()\n    return _config_manager\n</code></pre>"},{"location":"reference/ragcrawl/config/user_config/#ragcrawl.config.user_config.get_default_data_dir","title":"<code>get_default_data_dir()</code>","text":"<p>Get the default ragcrawl data directory.</p> Source code in <code>src/ragcrawl/config/user_config.py</code> <pre><code>def get_default_data_dir() -&gt; Path:\n    \"\"\"Get the default ragcrawl data directory.\"\"\"\n    return Path.home() / \".ragcrawl\"\n</code></pre>"},{"location":"reference/ragcrawl/config/user_config/#ragcrawl.config.user_config.get_default_db_path","title":"<code>get_default_db_path()</code>","text":"<p>Get the default DuckDB database path.</p> Source code in <code>src/ragcrawl/config/user_config.py</code> <pre><code>def get_default_db_path() -&gt; Path:\n    \"\"\"Get the default DuckDB database path.\"\"\"\n    return get_default_data_dir() / \"ragcrawl.duckdb\"\n</code></pre>"},{"location":"reference/ragcrawl/config/user_config/#ragcrawl.config.user_config.get_default_storage_path","title":"<code>get_default_storage_path()</code>","text":"<p>Get the default storage path from user config.</p> Source code in <code>src/ragcrawl/config/user_config.py</code> <pre><code>def get_default_storage_path() -&gt; Path:\n    \"\"\"Get the default storage path from user config.\"\"\"\n    return get_user_config().db_path\n</code></pre>"},{"location":"reference/ragcrawl/config/user_config/#ragcrawl.config.user_config.get_user_config","title":"<code>get_user_config()</code>","text":"<p>Get the current user configuration.</p> Source code in <code>src/ragcrawl/config/user_config.py</code> <pre><code>def get_user_config() -&gt; UserConfig:\n    \"\"\"Get the current user configuration.\"\"\"\n    return get_config_manager().load()\n</code></pre>"},{"location":"reference/ragcrawl/core/","title":"Index","text":""},{"location":"reference/ragcrawl/core/#ragcrawl.core","title":"<code>core</code>","text":"<p>Core crawling logic for ragcrawl.</p>"},{"location":"reference/ragcrawl/core/#ragcrawl.core.CrawlJob","title":"<code>CrawlJob(config)</code>","text":"<p>Main crawl job orchestrator.</p> <p>Coordinates the frontier, fetcher, extractor, and storage to perform a complete crawl.</p> <p>Initialize a crawl job.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>CrawlerConfig</code> <p>Crawler configuration.</p> required Source code in <code>src/ragcrawl/core/crawl_job.py</code> <pre><code>def __init__(self, config: CrawlerConfig) -&gt; None:\n    \"\"\"\n    Initialize a crawl job.\n\n    Args:\n        config: Crawler configuration.\n    \"\"\"\n    self.config = config\n\n    # Generate IDs\n    self.site_id = config.site_id or generate_site_id(config.seeds)\n    self.run_id = generate_run_id()\n\n    # Initialize components (lazy)\n    self._storage: StorageBackend | None = None\n    self._fetcher: Crawl4AIFetcher | None = None\n    self._robots: RobotsChecker | None = None\n    self._frontier: Frontier | None = None\n    self._scheduler: DomainScheduler | None = None\n    self._extractor: ContentExtractor | None = None\n    self._quality_gate: QualityGate | None = None\n    self._link_filter: LinkFilter | None = None\n\n    # Tracking\n    self._metrics = MetricsCollector()\n    self._logger = CrawlLoggerAdapter(self.run_id, self.site_id)\n    self._crawl_run: CrawlRun | None = None\n    self._documents: list[Document] = []\n</code></pre>"},{"location":"reference/ragcrawl/core/#ragcrawl.core.CrawlJob.run","title":"<code>run()</code>  <code>async</code>","text":"<p>Execute the crawl job.</p> <p>Returns:</p> Type Description <code>CrawlResult</code> <p>CrawlResult with statistics and documents.</p> Source code in <code>src/ragcrawl/core/crawl_job.py</code> <pre><code>async def run(self) -&gt; CrawlResult:\n    \"\"\"\n    Execute the crawl job.\n\n    Returns:\n        CrawlResult with statistics and documents.\n    \"\"\"\n    start_time = datetime.now()\n\n    try:\n        # Initialize\n        self._init_components()\n\n        # Create/update site record\n        await self._save_site()\n\n        # Create crawl run record\n        self._crawl_run = CrawlRun(\n            run_id=self.run_id,\n            site_id=self.site_id,\n            config_snapshot=self.config.model_dump(exclude={\"on_page\", \"on_error\", \"on_change_detected\", \"redaction_hook\"}),\n            seeds=self.config.seeds,\n        )\n        self._crawl_run.mark_started()\n        self._storage.save_run(self._crawl_run)\n\n        self._logger.run_started(\n            self.config.seeds,\n            {\"max_pages\": self.config.max_pages, \"max_depth\": self.config.max_depth},\n        )\n\n        # Add seeds to frontier\n        await self._frontier.add_seeds(self.config.seeds)\n\n        # Main crawl loop\n        await self._crawl_loop()\n\n        # Finalize\n        metrics = self._metrics.finalize()\n        self._crawl_run.stats = CrawlStats(\n            pages_discovered=metrics.pages_discovered,\n            pages_crawled=metrics.pages_crawled,\n            pages_failed=metrics.pages_failed,\n            pages_skipped=metrics.pages_skipped,\n            pages_changed=metrics.pages_changed,\n            pages_new=metrics.pages_new,\n            total_bytes_downloaded=metrics.total_bytes,\n            total_fetch_time_ms=metrics.total_fetch_time_ms,\n            total_extraction_time_ms=metrics.total_extraction_time_ms,\n            avg_fetch_latency_ms=metrics.avg_fetch_latency_ms,\n            status_codes=dict(metrics.status_codes),\n            errors_by_type=dict(metrics.errors_by_type),\n        )\n        self._crawl_run.frontier_size = self._frontier.size\n        self._crawl_run.max_depth_reached = self._frontier.max_depth_reached\n\n        partial = metrics.pages_failed &gt; 0\n        self._crawl_run.mark_completed(partial=partial)\n        self._storage.save_run(self._crawl_run)\n\n        duration = (datetime.now() - start_time).total_seconds()\n        self._logger.run_completed(metrics.to_dict(), duration)\n\n        return CrawlResult(\n            run_id=self.run_id,\n            site_id=self.site_id,\n            success=True,\n            stats=self._crawl_run.stats,\n            documents=self._documents,\n            duration_seconds=duration,\n        )\n\n    except Exception as e:\n        logger.error(\"Crawl job failed\", error=str(e))\n\n        if self._crawl_run:\n            self._crawl_run.mark_failed(str(e))\n            self._storage.save_run(self._crawl_run)\n\n        self._logger.run_failed(str(e))\n\n        return CrawlResult(\n            run_id=self.run_id,\n            site_id=self.site_id,\n            success=False,\n            error=str(e),\n            duration_seconds=(datetime.now() - start_time).total_seconds(),\n        )\n\n    finally:\n        # Cleanup\n        if self._fetcher:\n            await self._fetcher.close()\n        if self._storage:\n            self._storage.close()\n</code></pre>"},{"location":"reference/ragcrawl/core/#ragcrawl.core.CrawlResult","title":"<code>CrawlResult(run_id, site_id, success, stats=CrawlStats(), documents=list(), error=None, duration_seconds=0.0)</code>  <code>dataclass</code>","text":"<p>Result of a crawl job.</p>"},{"location":"reference/ragcrawl/core/#ragcrawl.core.DomainScheduler","title":"<code>DomainScheduler(config, max_concurrency=10)</code>","text":"<p>Manages per-domain rate limiting and scheduling.</p> <p>Features: - Per-domain request rate limiting - Per-domain concurrency limits - Circuit breaker for failing domains - Global rate limiting</p> <p>Initialize the scheduler.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>RateLimitConfig</code> <p>Rate limit configuration.</p> required <code>max_concurrency</code> <code>int</code> <p>Global max concurrent requests.</p> <code>10</code> Source code in <code>src/ragcrawl/core/scheduler.py</code> <pre><code>def __init__(\n    self,\n    config: RateLimitConfig,\n    max_concurrency: int = 10,\n) -&gt; None:\n    \"\"\"\n    Initialize the scheduler.\n\n    Args:\n        config: Rate limit configuration.\n        max_concurrency: Global max concurrent requests.\n    \"\"\"\n    self.config = config\n    self.max_concurrency = max_concurrency\n\n    # Per-domain state\n    self._domain_states: dict[str, DomainState] = defaultdict(DomainState)\n\n    # Global state\n    self._last_global_request = 0.0\n    self._active_requests = 0\n\n    # Semaphores\n    self._global_semaphore = asyncio.Semaphore(max_concurrency)\n    self._domain_semaphores: dict[str, asyncio.Semaphore] = {}\n\n    # Circuit breaker config\n    self._error_threshold = 5  # Consecutive errors to open circuit\n    self._circuit_timeout = 60.0  # Seconds to keep circuit open\n</code></pre>"},{"location":"reference/ragcrawl/core/#ragcrawl.core.DomainScheduler.active_requests","title":"<code>active_requests</code>  <code>property</code>","text":"<p>Number of active requests.</p>"},{"location":"reference/ragcrawl/core/#ragcrawl.core.DomainScheduler.acquire","title":"<code>acquire(domain)</code>  <code>async</code>","text":"<p>Acquire permission to make a request to a domain.</p> <p>Parameters:</p> Name Type Description Default <code>domain</code> <code>str</code> <p>Target domain.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if request can proceed, False if blocked.</p> Source code in <code>src/ragcrawl/core/scheduler.py</code> <pre><code>async def acquire(self, domain: str) -&gt; bool:\n    \"\"\"\n    Acquire permission to make a request to a domain.\n\n    Args:\n        domain: Target domain.\n\n    Returns:\n        True if request can proceed, False if blocked.\n    \"\"\"\n    # Check circuit breaker\n    state = self._domain_states[domain]\n    if state.circuit_open:\n        if time.time() &lt; state.circuit_open_until:\n            logger.debug(\"Circuit open for domain\", domain=domain)\n            return False\n        else:\n            # Half-open: allow one request\n            state.circuit_open = False\n\n    # Global rate limit\n    await self._wait_global_rate()\n\n    # Domain rate limit\n    await self._wait_domain_rate(domain)\n\n    # Acquire semaphores\n    await self._global_semaphore.acquire()\n\n    domain_sem = self._get_domain_semaphore(domain)\n    await domain_sem.acquire()\n\n    # Update state\n    state.active_requests += 1\n    state.request_count += 1\n    state.last_request_time = time.time()\n    self._active_requests += 1\n    self._last_global_request = time.time()\n\n    return True\n</code></pre>"},{"location":"reference/ragcrawl/core/#ragcrawl.core.DomainScheduler.get_domain_stats","title":"<code>get_domain_stats(domain)</code>","text":"<p>Get statistics for a domain.</p> Source code in <code>src/ragcrawl/core/scheduler.py</code> <pre><code>def get_domain_stats(self, domain: str) -&gt; dict[str, Any]:\n    \"\"\"Get statistics for a domain.\"\"\"\n    state = self._domain_states[domain]\n    return {\n        \"request_count\": state.request_count,\n        \"error_count\": state.error_count,\n        \"active_requests\": state.active_requests,\n        \"circuit_open\": state.circuit_open,\n        \"consecutive_errors\": state.consecutive_errors,\n    }\n</code></pre>"},{"location":"reference/ragcrawl/core/#ragcrawl.core.DomainScheduler.get_stats","title":"<code>get_stats()</code>","text":"<p>Get overall scheduler statistics.</p> Source code in <code>src/ragcrawl/core/scheduler.py</code> <pre><code>def get_stats(self) -&gt; dict[str, Any]:\n    \"\"\"Get overall scheduler statistics.\"\"\"\n    return {\n        \"active_requests\": self._active_requests,\n        \"domains_tracked\": len(self._domain_states),\n        \"circuits_open\": sum(\n            1 for s in self._domain_states.values() if s.circuit_open\n        ),\n    }\n</code></pre>"},{"location":"reference/ragcrawl/core/#ragcrawl.core.DomainScheduler.release","title":"<code>release(domain, success=True)</code>","text":"<p>Release a request slot.</p> <p>Parameters:</p> Name Type Description Default <code>domain</code> <code>str</code> <p>Target domain.</p> required <code>success</code> <code>bool</code> <p>Whether the request succeeded.</p> <code>True</code> Source code in <code>src/ragcrawl/core/scheduler.py</code> <pre><code>def release(self, domain: str, success: bool = True) -&gt; None:\n    \"\"\"\n    Release a request slot.\n\n    Args:\n        domain: Target domain.\n        success: Whether the request succeeded.\n    \"\"\"\n    state = self._domain_states[domain]\n    state.active_requests = max(0, state.active_requests - 1)\n    self._active_requests = max(0, self._active_requests - 1)\n\n    # Update circuit breaker\n    if success:\n        state.consecutive_errors = 0\n    else:\n        state.error_count += 1\n        state.consecutive_errors += 1\n\n        if state.consecutive_errors &gt;= self._error_threshold:\n            state.circuit_open = True\n            state.circuit_open_until = time.time() + self._circuit_timeout\n            logger.warning(\n                \"Circuit opened for domain\",\n                domain=domain,\n                consecutive_errors=state.consecutive_errors,\n            )\n\n    # Release semaphores\n    self._global_semaphore.release()\n\n    domain_sem = self._get_domain_semaphore(domain)\n    domain_sem.release()\n</code></pre>"},{"location":"reference/ragcrawl/core/#ragcrawl.core.DomainScheduler.set_crawl_delay","title":"<code>set_crawl_delay(domain, delay)</code>","text":"<p>Set custom crawl delay for a domain (from robots.txt).</p> <p>Parameters:</p> Name Type Description Default <code>domain</code> <code>str</code> <p>Target domain.</p> required <code>delay</code> <code>float</code> <p>Delay in seconds.</p> required Source code in <code>src/ragcrawl/core/scheduler.py</code> <pre><code>def set_crawl_delay(self, domain: str, delay: float) -&gt; None:\n    \"\"\"\n    Set custom crawl delay for a domain (from robots.txt).\n\n    Args:\n        domain: Target domain.\n        delay: Delay in seconds.\n    \"\"\"\n    # Store as effective per-domain rate\n    # This will be used in _wait_domain_rate\n    # For now, just log it\n    logger.debug(\"Crawl delay set\", domain=domain, delay=delay)\n</code></pre>"},{"location":"reference/ragcrawl/core/#ragcrawl.core.Frontier","title":"<code>Frontier(run_id, site_id, link_filter, max_depth=10, max_pages=1000)</code>","text":"<p>Crawl frontier managing the URL queue.</p> <p>Features: - Priority-based ordering - Deduplication - Depth tracking - Per-domain bucketing</p> <p>Initialize the frontier.</p> <p>Parameters:</p> Name Type Description Default <code>run_id</code> <code>str</code> <p>Current crawl run ID.</p> required <code>site_id</code> <code>str</code> <p>Site being crawled.</p> required <code>link_filter</code> <code>LinkFilter</code> <p>Filter for URL validation.</p> required <code>max_depth</code> <code>int</code> <p>Maximum crawl depth.</p> <code>10</code> <code>max_pages</code> <code>int</code> <p>Maximum pages to crawl.</p> <code>1000</code> Source code in <code>src/ragcrawl/core/frontier.py</code> <pre><code>def __init__(\n    self,\n    run_id: str,\n    site_id: str,\n    link_filter: LinkFilter,\n    max_depth: int = 10,\n    max_pages: int = 1000,\n) -&gt; None:\n    \"\"\"\n    Initialize the frontier.\n\n    Args:\n        run_id: Current crawl run ID.\n        site_id: Site being crawled.\n        link_filter: Filter for URL validation.\n        max_depth: Maximum crawl depth.\n        max_pages: Maximum pages to crawl.\n    \"\"\"\n    self.run_id = run_id\n    self.site_id = site_id\n    self.link_filter = link_filter\n    self.max_depth = max_depth\n    self.max_pages = max_pages\n\n    self.normalizer = URLNormalizer()\n\n    # Priority queue\n    self._queue: list[PrioritizedItem] = []\n\n    # Tracking sets\n    self._seen_urls: set[str] = set()\n    self._in_progress: set[str] = set()\n    self._completed: set[str] = set()\n    self._failed: set[str] = set()\n\n    # Per-domain tracking\n    self._domain_counts: dict[str, int] = {}\n\n    # Stats\n    self._discovered_count = 0\n    self._max_depth_seen = 0\n\n    # Lock for thread safety\n    self._lock = asyncio.Lock()\n</code></pre>"},{"location":"reference/ragcrawl/core/#ragcrawl.core.Frontier.completed_count","title":"<code>completed_count</code>  <code>property</code>","text":"<p>Total URLs completed.</p>"},{"location":"reference/ragcrawl/core/#ragcrawl.core.Frontier.discovered_count","title":"<code>discovered_count</code>  <code>property</code>","text":"<p>Total URLs discovered.</p>"},{"location":"reference/ragcrawl/core/#ragcrawl.core.Frontier.failed_count","title":"<code>failed_count</code>  <code>property</code>","text":"<p>Total URLs failed.</p>"},{"location":"reference/ragcrawl/core/#ragcrawl.core.Frontier.in_progress_count","title":"<code>in_progress_count</code>  <code>property</code>","text":"<p>URLs currently in progress.</p>"},{"location":"reference/ragcrawl/core/#ragcrawl.core.Frontier.is_empty","title":"<code>is_empty</code>  <code>property</code>","text":"<p>Check if frontier is empty.</p>"},{"location":"reference/ragcrawl/core/#ragcrawl.core.Frontier.max_depth_reached","title":"<code>max_depth_reached</code>  <code>property</code>","text":"<p>Maximum depth seen.</p>"},{"location":"reference/ragcrawl/core/#ragcrawl.core.Frontier.size","title":"<code>size</code>  <code>property</code>","text":"<p>Current queue size.</p>"},{"location":"reference/ragcrawl/core/#ragcrawl.core.Frontier.add","title":"<code>add(url, depth=0, referrer_url=None, priority=None)</code>  <code>async</code>","text":"<p>Add a URL to the frontier.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>URL to add.</p> required <code>depth</code> <code>int</code> <p>Depth from seed.</p> <code>0</code> <code>referrer_url</code> <code>str | None</code> <p>URL that linked to this.</p> <code>None</code> <code>priority</code> <code>float | None</code> <p>Custom priority (higher = sooner).</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if URL was added, False if filtered/duplicate.</p> Source code in <code>src/ragcrawl/core/frontier.py</code> <pre><code>async def add(\n    self,\n    url: str,\n    depth: int = 0,\n    referrer_url: str | None = None,\n    priority: float | None = None,\n) -&gt; bool:\n    \"\"\"\n    Add a URL to the frontier.\n\n    Args:\n        url: URL to add.\n        depth: Depth from seed.\n        referrer_url: URL that linked to this.\n        priority: Custom priority (higher = sooner).\n\n    Returns:\n        True if URL was added, False if filtered/duplicate.\n    \"\"\"\n    async with self._lock:\n        # Check limits\n        if self._discovered_count &gt;= self.max_pages:\n            return False\n\n        if depth &gt; self.max_depth:\n            return False\n\n        # Filter and normalize\n        result = self.link_filter.filter(\n            url,\n            check_seen=False,\n            current_depth=depth,\n            max_depth=self.max_depth,\n        )\n\n        if not result.allowed:\n            return False\n\n        normalized_url = result.normalized_url or self.normalizer.normalize(url)\n\n        # Deduplication\n        if normalized_url in self._seen_urls:\n            return False\n\n        self._seen_urls.add(normalized_url)\n        self._discovered_count += 1\n        self._max_depth_seen = max(self._max_depth_seen, depth)\n\n        # Calculate priority\n        if priority is None:\n            priority = self._calculate_priority(depth, normalized_url)\n\n        # Create frontier item\n        domain = self._get_domain(normalized_url)\n        url_hash = compute_doc_id(normalized_url)\n\n        item = FrontierItem(\n            item_id=f\"{self.run_id}_{url_hash}\",\n            run_id=self.run_id,\n            site_id=self.site_id,\n            url=url,\n            normalized_url=normalized_url,\n            url_hash=url_hash,\n            depth=depth,\n            referrer_url=referrer_url,\n            priority=priority,\n            status=FrontierStatus.PENDING,\n            discovered_at=datetime.now(),\n            domain=domain,\n        )\n\n        # Add to priority queue (negative priority for max-heap behavior)\n        heapq.heappush(self._queue, PrioritizedItem(-priority, item))\n\n        # Track domain\n        self._domain_counts[domain] = self._domain_counts.get(domain, 0) + 1\n\n        return True\n</code></pre>"},{"location":"reference/ragcrawl/core/#ragcrawl.core.Frontier.add_batch","title":"<code>add_batch(urls, depth, referrer_url=None)</code>  <code>async</code>","text":"<p>Add multiple URLs to the frontier.</p> <p>Parameters:</p> Name Type Description Default <code>urls</code> <code>list[str]</code> <p>URLs to add.</p> required <code>depth</code> <code>int</code> <p>Depth for all URLs.</p> required <code>referrer_url</code> <code>str | None</code> <p>Referrer URL.</p> <code>None</code> <p>Returns:</p> Type Description <code>int</code> <p>Number of URLs added.</p> Source code in <code>src/ragcrawl/core/frontier.py</code> <pre><code>async def add_batch(\n    self,\n    urls: list[str],\n    depth: int,\n    referrer_url: str | None = None,\n) -&gt; int:\n    \"\"\"\n    Add multiple URLs to the frontier.\n\n    Args:\n        urls: URLs to add.\n        depth: Depth for all URLs.\n        referrer_url: Referrer URL.\n\n    Returns:\n        Number of URLs added.\n    \"\"\"\n    added = 0\n    for url in urls:\n        if await self.add(url, depth, referrer_url):\n            added += 1\n    return added\n</code></pre>"},{"location":"reference/ragcrawl/core/#ragcrawl.core.Frontier.add_seeds","title":"<code>add_seeds(seeds)</code>  <code>async</code>","text":"<p>Add seed URLs to the frontier.</p> <p>Parameters:</p> Name Type Description Default <code>seeds</code> <code>list[str]</code> <p>List of seed URLs.</p> required <p>Returns:</p> Type Description <code>int</code> <p>Number of seeds added.</p> Source code in <code>src/ragcrawl/core/frontier.py</code> <pre><code>async def add_seeds(self, seeds: list[str]) -&gt; int:\n    \"\"\"\n    Add seed URLs to the frontier.\n\n    Args:\n        seeds: List of seed URLs.\n\n    Returns:\n        Number of seeds added.\n    \"\"\"\n    added = 0\n    for seed in seeds:\n        if await self.add(seed, depth=0, priority=1000.0):  # High priority for seeds\n            added += 1\n    return added\n</code></pre>"},{"location":"reference/ragcrawl/core/#ragcrawl.core.Frontier.get_batch","title":"<code>get_batch(count=10)</code>  <code>async</code>","text":"<p>Get multiple URLs to crawl.</p> <p>Parameters:</p> Name Type Description Default <code>count</code> <code>int</code> <p>Number of URLs to get.</p> <code>10</code> <p>Returns:</p> Type Description <code>list[FrontierItem]</code> <p>List of FrontierItems.</p> Source code in <code>src/ragcrawl/core/frontier.py</code> <pre><code>async def get_batch(self, count: int = 10) -&gt; list[FrontierItem]:\n    \"\"\"\n    Get multiple URLs to crawl.\n\n    Args:\n        count: Number of URLs to get.\n\n    Returns:\n        List of FrontierItems.\n    \"\"\"\n    items = []\n    for _ in range(count):\n        item = await self.get_next()\n        if item is None:\n            break\n        items.append(item)\n    return items\n</code></pre>"},{"location":"reference/ragcrawl/core/#ragcrawl.core.Frontier.get_next","title":"<code>get_next()</code>  <code>async</code>","text":"<p>Get the next URL to crawl.</p> <p>Returns:</p> Type Description <code>FrontierItem | None</code> <p>Next FrontierItem or None if empty.</p> Source code in <code>src/ragcrawl/core/frontier.py</code> <pre><code>async def get_next(self) -&gt; FrontierItem | None:\n    \"\"\"\n    Get the next URL to crawl.\n\n    Returns:\n        Next FrontierItem or None if empty.\n    \"\"\"\n    async with self._lock:\n        while self._queue:\n            prioritized = heapq.heappop(self._queue)\n            item = prioritized.item\n\n            # Skip if already processed\n            if item.normalized_url in self._completed:\n                continue\n            if item.normalized_url in self._failed:\n                continue\n            if item.normalized_url in self._in_progress:\n                continue\n\n            # Mark in progress\n            self._in_progress.add(item.normalized_url)\n            item.mark_in_progress()\n\n            return item\n\n        return None\n</code></pre>"},{"location":"reference/ragcrawl/core/#ragcrawl.core.Frontier.get_stats","title":"<code>get_stats()</code>","text":"<p>Get frontier statistics.</p> Source code in <code>src/ragcrawl/core/frontier.py</code> <pre><code>def get_stats(self) -&gt; dict[str, Any]:\n    \"\"\"Get frontier statistics.\"\"\"\n    return {\n        \"queue_size\": self.size,\n        \"discovered\": self._discovered_count,\n        \"completed\": len(self._completed),\n        \"failed\": len(self._failed),\n        \"in_progress\": len(self._in_progress),\n        \"max_depth_reached\": self._max_depth_seen,\n        \"domains\": len(self._domain_counts),\n    }\n</code></pre>"},{"location":"reference/ragcrawl/core/#ragcrawl.core.Frontier.mark_completed","title":"<code>mark_completed(url)</code>  <code>async</code>","text":"<p>Mark a URL as completed.</p> Source code in <code>src/ragcrawl/core/frontier.py</code> <pre><code>async def mark_completed(self, url: str) -&gt; None:\n    \"\"\"Mark a URL as completed.\"\"\"\n    async with self._lock:\n        normalized = self.normalizer.normalize(url)\n        self._in_progress.discard(normalized)\n        self._completed.add(normalized)\n</code></pre>"},{"location":"reference/ragcrawl/core/#ragcrawl.core.Frontier.mark_failed","title":"<code>mark_failed(url, error=None)</code>  <code>async</code>","text":"<p>Mark a URL as failed.</p> Source code in <code>src/ragcrawl/core/frontier.py</code> <pre><code>async def mark_failed(self, url: str, error: str | None = None) -&gt; None:\n    \"\"\"Mark a URL as failed.\"\"\"\n    async with self._lock:\n        normalized = self.normalizer.normalize(url)\n        self._in_progress.discard(normalized)\n        self._failed.add(normalized)\n</code></pre>"},{"location":"reference/ragcrawl/core/#ragcrawl.core.Frontier.return_to_queue","title":"<code>return_to_queue(item)</code>  <code>async</code>","text":"<p>Return an item to the queue for retry.</p> Source code in <code>src/ragcrawl/core/frontier.py</code> <pre><code>async def return_to_queue(self, item: FrontierItem) -&gt; None:\n    \"\"\"Return an item to the queue for retry.\"\"\"\n    async with self._lock:\n        self._in_progress.discard(item.normalized_url)\n        item.status = FrontierStatus.PENDING\n        item.retry_count += 1\n\n        # Lower priority on retry\n        new_priority = item.priority * 0.5\n        heapq.heappush(self._queue, PrioritizedItem(-new_priority, item))\n</code></pre>"},{"location":"reference/ragcrawl/core/#ragcrawl.core.SyncJob","title":"<code>SyncJob(config)</code>","text":"<p>Incremental sync job for detecting content changes.</p> <p>Uses multiple strategies: 1. Sitemap lastmod (if available) 2. HTTP conditional requests (ETag/Last-Modified) 3. Content hash diffing (fallback)</p> <p>Initialize sync job.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>SyncConfig</code> <p>Sync configuration.</p> required Source code in <code>src/ragcrawl/core/sync_job.py</code> <pre><code>def __init__(self, config: SyncConfig) -&gt; None:\n    \"\"\"\n    Initialize sync job.\n\n    Args:\n        config: Sync configuration.\n    \"\"\"\n    self.config = config\n    self.site_id = config.site_id\n    self.run_id = generate_run_id()\n\n    # Components\n    self._storage: StorageBackend | None = None\n    self._fetcher: Crawl4AIFetcher | None = None\n    self._extractor: ContentExtractor | None = None\n    self._sitemap_parser: SitemapParser | None = None\n    self._change_detector: ChangeDetector | None = None\n    self._revalidator: Revalidator | None = None\n\n    # Tracking\n    self._metrics = MetricsCollector()\n    self._logger = CrawlLoggerAdapter(self.run_id, self.site_id)\n    self._changed_pages: list[str] = []\n    self._deleted_pages: list[str] = []\n</code></pre>"},{"location":"reference/ragcrawl/core/#ragcrawl.core.SyncJob.run","title":"<code>run()</code>  <code>async</code>","text":"<p>Execute the sync job.</p> <p>Returns:</p> Type Description <code>SyncResult</code> <p>SyncResult with changed and deleted pages.</p> Source code in <code>src/ragcrawl/core/sync_job.py</code> <pre><code>async def run(self) -&gt; SyncResult:\n    \"\"\"\n    Execute the sync job.\n\n    Returns:\n        SyncResult with changed and deleted pages.\n    \"\"\"\n    start_time = datetime.now()\n\n    try:\n        self._init_components()\n\n        # Verify site exists\n        site = self._storage.get_site(self.site_id)\n        if not site:\n            raise ValueError(f\"Site not found: {self.site_id}\")\n\n        # Create sync run record\n        crawl_run = CrawlRun(\n            run_id=self.run_id,\n            site_id=self.site_id,\n            is_sync=True,\n            config_snapshot=self.config.model_dump(\n                exclude={\"on_page\", \"on_change_detected\", \"on_deletion_detected\", \"on_error\"}\n            ),\n        )\n        crawl_run.mark_started()\n        self._storage.save_run(crawl_run)\n\n        # Get pages to check\n        pages = await self._get_pages_to_check()\n\n        logger.info(\"Starting sync\", site_id=self.site_id, pages_to_check=len(pages))\n\n        # Process pages\n        for page in pages:\n            await self._process_page(page)\n\n            # Check limit\n            if self.config.max_pages and self._metrics.metrics.pages_crawled &gt;= self.config.max_pages:\n                break\n\n        # Finalize\n        metrics = self._metrics.finalize()\n        crawl_run.stats = CrawlStats(\n            pages_crawled=metrics.pages_crawled,\n            pages_changed=metrics.pages_changed,\n            pages_unchanged=metrics.pages_unchanged,\n            pages_deleted=metrics.pages_deleted,\n            pages_failed=metrics.pages_failed,\n        )\n        crawl_run.mark_completed(partial=metrics.pages_failed &gt; 0)\n        self._storage.save_run(crawl_run)\n\n        # Update site\n        site.last_sync_at = datetime.now()\n        self._storage.save_site(site)\n\n        duration = (datetime.now() - start_time).total_seconds()\n\n        return SyncResult(\n            run_id=self.run_id,\n            site_id=self.site_id,\n            success=True,\n            stats=crawl_run.stats,\n            changed_pages=self._changed_pages,\n            deleted_pages=self._deleted_pages,\n            duration_seconds=duration,\n        )\n\n    except Exception as e:\n        logger.error(\"Sync job failed\", error=str(e))\n        return SyncResult(\n            run_id=self.run_id,\n            site_id=self.site_id,\n            success=False,\n            error=str(e),\n            duration_seconds=(datetime.now() - start_time).total_seconds(),\n        )\n\n    finally:\n        if self._fetcher:\n            await self._fetcher.close()\n        if self._storage:\n            self._storage.close()\n</code></pre>"},{"location":"reference/ragcrawl/core/#ragcrawl.core.SyncResult","title":"<code>SyncResult(run_id, site_id, success, stats=CrawlStats(), changed_pages=list(), deleted_pages=list(), error=None, duration_seconds=0.0)</code>  <code>dataclass</code>","text":"<p>Result of a sync job.</p>"},{"location":"reference/ragcrawl/core/crawl_job/","title":"Crawl job","text":""},{"location":"reference/ragcrawl/core/crawl_job/#ragcrawl.core.crawl_job","title":"<code>crawl_job</code>","text":"<p>Main crawl job orchestration.</p>"},{"location":"reference/ragcrawl/core/crawl_job/#ragcrawl.core.crawl_job.CrawlJob","title":"<code>CrawlJob(config)</code>","text":"<p>Main crawl job orchestrator.</p> <p>Coordinates the frontier, fetcher, extractor, and storage to perform a complete crawl.</p> <p>Initialize a crawl job.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>CrawlerConfig</code> <p>Crawler configuration.</p> required Source code in <code>src/ragcrawl/core/crawl_job.py</code> <pre><code>def __init__(self, config: CrawlerConfig) -&gt; None:\n    \"\"\"\n    Initialize a crawl job.\n\n    Args:\n        config: Crawler configuration.\n    \"\"\"\n    self.config = config\n\n    # Generate IDs\n    self.site_id = config.site_id or generate_site_id(config.seeds)\n    self.run_id = generate_run_id()\n\n    # Initialize components (lazy)\n    self._storage: StorageBackend | None = None\n    self._fetcher: Crawl4AIFetcher | None = None\n    self._robots: RobotsChecker | None = None\n    self._frontier: Frontier | None = None\n    self._scheduler: DomainScheduler | None = None\n    self._extractor: ContentExtractor | None = None\n    self._quality_gate: QualityGate | None = None\n    self._link_filter: LinkFilter | None = None\n\n    # Tracking\n    self._metrics = MetricsCollector()\n    self._logger = CrawlLoggerAdapter(self.run_id, self.site_id)\n    self._crawl_run: CrawlRun | None = None\n    self._documents: list[Document] = []\n</code></pre>"},{"location":"reference/ragcrawl/core/crawl_job/#ragcrawl.core.crawl_job.CrawlJob.run","title":"<code>run()</code>  <code>async</code>","text":"<p>Execute the crawl job.</p> <p>Returns:</p> Type Description <code>CrawlResult</code> <p>CrawlResult with statistics and documents.</p> Source code in <code>src/ragcrawl/core/crawl_job.py</code> <pre><code>async def run(self) -&gt; CrawlResult:\n    \"\"\"\n    Execute the crawl job.\n\n    Returns:\n        CrawlResult with statistics and documents.\n    \"\"\"\n    start_time = datetime.now()\n\n    try:\n        # Initialize\n        self._init_components()\n\n        # Create/update site record\n        await self._save_site()\n\n        # Create crawl run record\n        self._crawl_run = CrawlRun(\n            run_id=self.run_id,\n            site_id=self.site_id,\n            config_snapshot=self.config.model_dump(exclude={\"on_page\", \"on_error\", \"on_change_detected\", \"redaction_hook\"}),\n            seeds=self.config.seeds,\n        )\n        self._crawl_run.mark_started()\n        self._storage.save_run(self._crawl_run)\n\n        self._logger.run_started(\n            self.config.seeds,\n            {\"max_pages\": self.config.max_pages, \"max_depth\": self.config.max_depth},\n        )\n\n        # Add seeds to frontier\n        await self._frontier.add_seeds(self.config.seeds)\n\n        # Main crawl loop\n        await self._crawl_loop()\n\n        # Finalize\n        metrics = self._metrics.finalize()\n        self._crawl_run.stats = CrawlStats(\n            pages_discovered=metrics.pages_discovered,\n            pages_crawled=metrics.pages_crawled,\n            pages_failed=metrics.pages_failed,\n            pages_skipped=metrics.pages_skipped,\n            pages_changed=metrics.pages_changed,\n            pages_new=metrics.pages_new,\n            total_bytes_downloaded=metrics.total_bytes,\n            total_fetch_time_ms=metrics.total_fetch_time_ms,\n            total_extraction_time_ms=metrics.total_extraction_time_ms,\n            avg_fetch_latency_ms=metrics.avg_fetch_latency_ms,\n            status_codes=dict(metrics.status_codes),\n            errors_by_type=dict(metrics.errors_by_type),\n        )\n        self._crawl_run.frontier_size = self._frontier.size\n        self._crawl_run.max_depth_reached = self._frontier.max_depth_reached\n\n        partial = metrics.pages_failed &gt; 0\n        self._crawl_run.mark_completed(partial=partial)\n        self._storage.save_run(self._crawl_run)\n\n        duration = (datetime.now() - start_time).total_seconds()\n        self._logger.run_completed(metrics.to_dict(), duration)\n\n        return CrawlResult(\n            run_id=self.run_id,\n            site_id=self.site_id,\n            success=True,\n            stats=self._crawl_run.stats,\n            documents=self._documents,\n            duration_seconds=duration,\n        )\n\n    except Exception as e:\n        logger.error(\"Crawl job failed\", error=str(e))\n\n        if self._crawl_run:\n            self._crawl_run.mark_failed(str(e))\n            self._storage.save_run(self._crawl_run)\n\n        self._logger.run_failed(str(e))\n\n        return CrawlResult(\n            run_id=self.run_id,\n            site_id=self.site_id,\n            success=False,\n            error=str(e),\n            duration_seconds=(datetime.now() - start_time).total_seconds(),\n        )\n\n    finally:\n        # Cleanup\n        if self._fetcher:\n            await self._fetcher.close()\n        if self._storage:\n            self._storage.close()\n</code></pre>"},{"location":"reference/ragcrawl/core/crawl_job/#ragcrawl.core.crawl_job.CrawlResult","title":"<code>CrawlResult(run_id, site_id, success, stats=CrawlStats(), documents=list(), error=None, duration_seconds=0.0)</code>  <code>dataclass</code>","text":"<p>Result of a crawl job.</p>"},{"location":"reference/ragcrawl/core/frontier/","title":"Frontier","text":""},{"location":"reference/ragcrawl/core/frontier/#ragcrawl.core.frontier","title":"<code>frontier</code>","text":"<p>Crawl frontier for URL queue management.</p>"},{"location":"reference/ragcrawl/core/frontier/#ragcrawl.core.frontier.Frontier","title":"<code>Frontier(run_id, site_id, link_filter, max_depth=10, max_pages=1000)</code>","text":"<p>Crawl frontier managing the URL queue.</p> <p>Features: - Priority-based ordering - Deduplication - Depth tracking - Per-domain bucketing</p> <p>Initialize the frontier.</p> <p>Parameters:</p> Name Type Description Default <code>run_id</code> <code>str</code> <p>Current crawl run ID.</p> required <code>site_id</code> <code>str</code> <p>Site being crawled.</p> required <code>link_filter</code> <code>LinkFilter</code> <p>Filter for URL validation.</p> required <code>max_depth</code> <code>int</code> <p>Maximum crawl depth.</p> <code>10</code> <code>max_pages</code> <code>int</code> <p>Maximum pages to crawl.</p> <code>1000</code> Source code in <code>src/ragcrawl/core/frontier.py</code> <pre><code>def __init__(\n    self,\n    run_id: str,\n    site_id: str,\n    link_filter: LinkFilter,\n    max_depth: int = 10,\n    max_pages: int = 1000,\n) -&gt; None:\n    \"\"\"\n    Initialize the frontier.\n\n    Args:\n        run_id: Current crawl run ID.\n        site_id: Site being crawled.\n        link_filter: Filter for URL validation.\n        max_depth: Maximum crawl depth.\n        max_pages: Maximum pages to crawl.\n    \"\"\"\n    self.run_id = run_id\n    self.site_id = site_id\n    self.link_filter = link_filter\n    self.max_depth = max_depth\n    self.max_pages = max_pages\n\n    self.normalizer = URLNormalizer()\n\n    # Priority queue\n    self._queue: list[PrioritizedItem] = []\n\n    # Tracking sets\n    self._seen_urls: set[str] = set()\n    self._in_progress: set[str] = set()\n    self._completed: set[str] = set()\n    self._failed: set[str] = set()\n\n    # Per-domain tracking\n    self._domain_counts: dict[str, int] = {}\n\n    # Stats\n    self._discovered_count = 0\n    self._max_depth_seen = 0\n\n    # Lock for thread safety\n    self._lock = asyncio.Lock()\n</code></pre>"},{"location":"reference/ragcrawl/core/frontier/#ragcrawl.core.frontier.Frontier.completed_count","title":"<code>completed_count</code>  <code>property</code>","text":"<p>Total URLs completed.</p>"},{"location":"reference/ragcrawl/core/frontier/#ragcrawl.core.frontier.Frontier.discovered_count","title":"<code>discovered_count</code>  <code>property</code>","text":"<p>Total URLs discovered.</p>"},{"location":"reference/ragcrawl/core/frontier/#ragcrawl.core.frontier.Frontier.failed_count","title":"<code>failed_count</code>  <code>property</code>","text":"<p>Total URLs failed.</p>"},{"location":"reference/ragcrawl/core/frontier/#ragcrawl.core.frontier.Frontier.in_progress_count","title":"<code>in_progress_count</code>  <code>property</code>","text":"<p>URLs currently in progress.</p>"},{"location":"reference/ragcrawl/core/frontier/#ragcrawl.core.frontier.Frontier.is_empty","title":"<code>is_empty</code>  <code>property</code>","text":"<p>Check if frontier is empty.</p>"},{"location":"reference/ragcrawl/core/frontier/#ragcrawl.core.frontier.Frontier.max_depth_reached","title":"<code>max_depth_reached</code>  <code>property</code>","text":"<p>Maximum depth seen.</p>"},{"location":"reference/ragcrawl/core/frontier/#ragcrawl.core.frontier.Frontier.size","title":"<code>size</code>  <code>property</code>","text":"<p>Current queue size.</p>"},{"location":"reference/ragcrawl/core/frontier/#ragcrawl.core.frontier.Frontier.add","title":"<code>add(url, depth=0, referrer_url=None, priority=None)</code>  <code>async</code>","text":"<p>Add a URL to the frontier.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>URL to add.</p> required <code>depth</code> <code>int</code> <p>Depth from seed.</p> <code>0</code> <code>referrer_url</code> <code>str | None</code> <p>URL that linked to this.</p> <code>None</code> <code>priority</code> <code>float | None</code> <p>Custom priority (higher = sooner).</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if URL was added, False if filtered/duplicate.</p> Source code in <code>src/ragcrawl/core/frontier.py</code> <pre><code>async def add(\n    self,\n    url: str,\n    depth: int = 0,\n    referrer_url: str | None = None,\n    priority: float | None = None,\n) -&gt; bool:\n    \"\"\"\n    Add a URL to the frontier.\n\n    Args:\n        url: URL to add.\n        depth: Depth from seed.\n        referrer_url: URL that linked to this.\n        priority: Custom priority (higher = sooner).\n\n    Returns:\n        True if URL was added, False if filtered/duplicate.\n    \"\"\"\n    async with self._lock:\n        # Check limits\n        if self._discovered_count &gt;= self.max_pages:\n            return False\n\n        if depth &gt; self.max_depth:\n            return False\n\n        # Filter and normalize\n        result = self.link_filter.filter(\n            url,\n            check_seen=False,\n            current_depth=depth,\n            max_depth=self.max_depth,\n        )\n\n        if not result.allowed:\n            return False\n\n        normalized_url = result.normalized_url or self.normalizer.normalize(url)\n\n        # Deduplication\n        if normalized_url in self._seen_urls:\n            return False\n\n        self._seen_urls.add(normalized_url)\n        self._discovered_count += 1\n        self._max_depth_seen = max(self._max_depth_seen, depth)\n\n        # Calculate priority\n        if priority is None:\n            priority = self._calculate_priority(depth, normalized_url)\n\n        # Create frontier item\n        domain = self._get_domain(normalized_url)\n        url_hash = compute_doc_id(normalized_url)\n\n        item = FrontierItem(\n            item_id=f\"{self.run_id}_{url_hash}\",\n            run_id=self.run_id,\n            site_id=self.site_id,\n            url=url,\n            normalized_url=normalized_url,\n            url_hash=url_hash,\n            depth=depth,\n            referrer_url=referrer_url,\n            priority=priority,\n            status=FrontierStatus.PENDING,\n            discovered_at=datetime.now(),\n            domain=domain,\n        )\n\n        # Add to priority queue (negative priority for max-heap behavior)\n        heapq.heappush(self._queue, PrioritizedItem(-priority, item))\n\n        # Track domain\n        self._domain_counts[domain] = self._domain_counts.get(domain, 0) + 1\n\n        return True\n</code></pre>"},{"location":"reference/ragcrawl/core/frontier/#ragcrawl.core.frontier.Frontier.add_batch","title":"<code>add_batch(urls, depth, referrer_url=None)</code>  <code>async</code>","text":"<p>Add multiple URLs to the frontier.</p> <p>Parameters:</p> Name Type Description Default <code>urls</code> <code>list[str]</code> <p>URLs to add.</p> required <code>depth</code> <code>int</code> <p>Depth for all URLs.</p> required <code>referrer_url</code> <code>str | None</code> <p>Referrer URL.</p> <code>None</code> <p>Returns:</p> Type Description <code>int</code> <p>Number of URLs added.</p> Source code in <code>src/ragcrawl/core/frontier.py</code> <pre><code>async def add_batch(\n    self,\n    urls: list[str],\n    depth: int,\n    referrer_url: str | None = None,\n) -&gt; int:\n    \"\"\"\n    Add multiple URLs to the frontier.\n\n    Args:\n        urls: URLs to add.\n        depth: Depth for all URLs.\n        referrer_url: Referrer URL.\n\n    Returns:\n        Number of URLs added.\n    \"\"\"\n    added = 0\n    for url in urls:\n        if await self.add(url, depth, referrer_url):\n            added += 1\n    return added\n</code></pre>"},{"location":"reference/ragcrawl/core/frontier/#ragcrawl.core.frontier.Frontier.add_seeds","title":"<code>add_seeds(seeds)</code>  <code>async</code>","text":"<p>Add seed URLs to the frontier.</p> <p>Parameters:</p> Name Type Description Default <code>seeds</code> <code>list[str]</code> <p>List of seed URLs.</p> required <p>Returns:</p> Type Description <code>int</code> <p>Number of seeds added.</p> Source code in <code>src/ragcrawl/core/frontier.py</code> <pre><code>async def add_seeds(self, seeds: list[str]) -&gt; int:\n    \"\"\"\n    Add seed URLs to the frontier.\n\n    Args:\n        seeds: List of seed URLs.\n\n    Returns:\n        Number of seeds added.\n    \"\"\"\n    added = 0\n    for seed in seeds:\n        if await self.add(seed, depth=0, priority=1000.0):  # High priority for seeds\n            added += 1\n    return added\n</code></pre>"},{"location":"reference/ragcrawl/core/frontier/#ragcrawl.core.frontier.Frontier.get_batch","title":"<code>get_batch(count=10)</code>  <code>async</code>","text":"<p>Get multiple URLs to crawl.</p> <p>Parameters:</p> Name Type Description Default <code>count</code> <code>int</code> <p>Number of URLs to get.</p> <code>10</code> <p>Returns:</p> Type Description <code>list[FrontierItem]</code> <p>List of FrontierItems.</p> Source code in <code>src/ragcrawl/core/frontier.py</code> <pre><code>async def get_batch(self, count: int = 10) -&gt; list[FrontierItem]:\n    \"\"\"\n    Get multiple URLs to crawl.\n\n    Args:\n        count: Number of URLs to get.\n\n    Returns:\n        List of FrontierItems.\n    \"\"\"\n    items = []\n    for _ in range(count):\n        item = await self.get_next()\n        if item is None:\n            break\n        items.append(item)\n    return items\n</code></pre>"},{"location":"reference/ragcrawl/core/frontier/#ragcrawl.core.frontier.Frontier.get_next","title":"<code>get_next()</code>  <code>async</code>","text":"<p>Get the next URL to crawl.</p> <p>Returns:</p> Type Description <code>FrontierItem | None</code> <p>Next FrontierItem or None if empty.</p> Source code in <code>src/ragcrawl/core/frontier.py</code> <pre><code>async def get_next(self) -&gt; FrontierItem | None:\n    \"\"\"\n    Get the next URL to crawl.\n\n    Returns:\n        Next FrontierItem or None if empty.\n    \"\"\"\n    async with self._lock:\n        while self._queue:\n            prioritized = heapq.heappop(self._queue)\n            item = prioritized.item\n\n            # Skip if already processed\n            if item.normalized_url in self._completed:\n                continue\n            if item.normalized_url in self._failed:\n                continue\n            if item.normalized_url in self._in_progress:\n                continue\n\n            # Mark in progress\n            self._in_progress.add(item.normalized_url)\n            item.mark_in_progress()\n\n            return item\n\n        return None\n</code></pre>"},{"location":"reference/ragcrawl/core/frontier/#ragcrawl.core.frontier.Frontier.get_stats","title":"<code>get_stats()</code>","text":"<p>Get frontier statistics.</p> Source code in <code>src/ragcrawl/core/frontier.py</code> <pre><code>def get_stats(self) -&gt; dict[str, Any]:\n    \"\"\"Get frontier statistics.\"\"\"\n    return {\n        \"queue_size\": self.size,\n        \"discovered\": self._discovered_count,\n        \"completed\": len(self._completed),\n        \"failed\": len(self._failed),\n        \"in_progress\": len(self._in_progress),\n        \"max_depth_reached\": self._max_depth_seen,\n        \"domains\": len(self._domain_counts),\n    }\n</code></pre>"},{"location":"reference/ragcrawl/core/frontier/#ragcrawl.core.frontier.Frontier.mark_completed","title":"<code>mark_completed(url)</code>  <code>async</code>","text":"<p>Mark a URL as completed.</p> Source code in <code>src/ragcrawl/core/frontier.py</code> <pre><code>async def mark_completed(self, url: str) -&gt; None:\n    \"\"\"Mark a URL as completed.\"\"\"\n    async with self._lock:\n        normalized = self.normalizer.normalize(url)\n        self._in_progress.discard(normalized)\n        self._completed.add(normalized)\n</code></pre>"},{"location":"reference/ragcrawl/core/frontier/#ragcrawl.core.frontier.Frontier.mark_failed","title":"<code>mark_failed(url, error=None)</code>  <code>async</code>","text":"<p>Mark a URL as failed.</p> Source code in <code>src/ragcrawl/core/frontier.py</code> <pre><code>async def mark_failed(self, url: str, error: str | None = None) -&gt; None:\n    \"\"\"Mark a URL as failed.\"\"\"\n    async with self._lock:\n        normalized = self.normalizer.normalize(url)\n        self._in_progress.discard(normalized)\n        self._failed.add(normalized)\n</code></pre>"},{"location":"reference/ragcrawl/core/frontier/#ragcrawl.core.frontier.Frontier.return_to_queue","title":"<code>return_to_queue(item)</code>  <code>async</code>","text":"<p>Return an item to the queue for retry.</p> Source code in <code>src/ragcrawl/core/frontier.py</code> <pre><code>async def return_to_queue(self, item: FrontierItem) -&gt; None:\n    \"\"\"Return an item to the queue for retry.\"\"\"\n    async with self._lock:\n        self._in_progress.discard(item.normalized_url)\n        item.status = FrontierStatus.PENDING\n        item.retry_count += 1\n\n        # Lower priority on retry\n        new_priority = item.priority * 0.5\n        heapq.heappush(self._queue, PrioritizedItem(-new_priority, item))\n</code></pre>"},{"location":"reference/ragcrawl/core/frontier/#ragcrawl.core.frontier.PrioritizedItem","title":"<code>PrioritizedItem(priority, item)</code>  <code>dataclass</code>","text":"<p>Item in the priority queue.</p>"},{"location":"reference/ragcrawl/core/scheduler/","title":"Scheduler","text":""},{"location":"reference/ragcrawl/core/scheduler/#ragcrawl.core.scheduler","title":"<code>scheduler</code>","text":"<p>Domain-based scheduling and rate limiting.</p>"},{"location":"reference/ragcrawl/core/scheduler/#ragcrawl.core.scheduler.DomainScheduler","title":"<code>DomainScheduler(config, max_concurrency=10)</code>","text":"<p>Manages per-domain rate limiting and scheduling.</p> <p>Features: - Per-domain request rate limiting - Per-domain concurrency limits - Circuit breaker for failing domains - Global rate limiting</p> <p>Initialize the scheduler.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>RateLimitConfig</code> <p>Rate limit configuration.</p> required <code>max_concurrency</code> <code>int</code> <p>Global max concurrent requests.</p> <code>10</code> Source code in <code>src/ragcrawl/core/scheduler.py</code> <pre><code>def __init__(\n    self,\n    config: RateLimitConfig,\n    max_concurrency: int = 10,\n) -&gt; None:\n    \"\"\"\n    Initialize the scheduler.\n\n    Args:\n        config: Rate limit configuration.\n        max_concurrency: Global max concurrent requests.\n    \"\"\"\n    self.config = config\n    self.max_concurrency = max_concurrency\n\n    # Per-domain state\n    self._domain_states: dict[str, DomainState] = defaultdict(DomainState)\n\n    # Global state\n    self._last_global_request = 0.0\n    self._active_requests = 0\n\n    # Semaphores\n    self._global_semaphore = asyncio.Semaphore(max_concurrency)\n    self._domain_semaphores: dict[str, asyncio.Semaphore] = {}\n\n    # Circuit breaker config\n    self._error_threshold = 5  # Consecutive errors to open circuit\n    self._circuit_timeout = 60.0  # Seconds to keep circuit open\n</code></pre>"},{"location":"reference/ragcrawl/core/scheduler/#ragcrawl.core.scheduler.DomainScheduler.active_requests","title":"<code>active_requests</code>  <code>property</code>","text":"<p>Number of active requests.</p>"},{"location":"reference/ragcrawl/core/scheduler/#ragcrawl.core.scheduler.DomainScheduler.acquire","title":"<code>acquire(domain)</code>  <code>async</code>","text":"<p>Acquire permission to make a request to a domain.</p> <p>Parameters:</p> Name Type Description Default <code>domain</code> <code>str</code> <p>Target domain.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if request can proceed, False if blocked.</p> Source code in <code>src/ragcrawl/core/scheduler.py</code> <pre><code>async def acquire(self, domain: str) -&gt; bool:\n    \"\"\"\n    Acquire permission to make a request to a domain.\n\n    Args:\n        domain: Target domain.\n\n    Returns:\n        True if request can proceed, False if blocked.\n    \"\"\"\n    # Check circuit breaker\n    state = self._domain_states[domain]\n    if state.circuit_open:\n        if time.time() &lt; state.circuit_open_until:\n            logger.debug(\"Circuit open for domain\", domain=domain)\n            return False\n        else:\n            # Half-open: allow one request\n            state.circuit_open = False\n\n    # Global rate limit\n    await self._wait_global_rate()\n\n    # Domain rate limit\n    await self._wait_domain_rate(domain)\n\n    # Acquire semaphores\n    await self._global_semaphore.acquire()\n\n    domain_sem = self._get_domain_semaphore(domain)\n    await domain_sem.acquire()\n\n    # Update state\n    state.active_requests += 1\n    state.request_count += 1\n    state.last_request_time = time.time()\n    self._active_requests += 1\n    self._last_global_request = time.time()\n\n    return True\n</code></pre>"},{"location":"reference/ragcrawl/core/scheduler/#ragcrawl.core.scheduler.DomainScheduler.get_domain_stats","title":"<code>get_domain_stats(domain)</code>","text":"<p>Get statistics for a domain.</p> Source code in <code>src/ragcrawl/core/scheduler.py</code> <pre><code>def get_domain_stats(self, domain: str) -&gt; dict[str, Any]:\n    \"\"\"Get statistics for a domain.\"\"\"\n    state = self._domain_states[domain]\n    return {\n        \"request_count\": state.request_count,\n        \"error_count\": state.error_count,\n        \"active_requests\": state.active_requests,\n        \"circuit_open\": state.circuit_open,\n        \"consecutive_errors\": state.consecutive_errors,\n    }\n</code></pre>"},{"location":"reference/ragcrawl/core/scheduler/#ragcrawl.core.scheduler.DomainScheduler.get_stats","title":"<code>get_stats()</code>","text":"<p>Get overall scheduler statistics.</p> Source code in <code>src/ragcrawl/core/scheduler.py</code> <pre><code>def get_stats(self) -&gt; dict[str, Any]:\n    \"\"\"Get overall scheduler statistics.\"\"\"\n    return {\n        \"active_requests\": self._active_requests,\n        \"domains_tracked\": len(self._domain_states),\n        \"circuits_open\": sum(\n            1 for s in self._domain_states.values() if s.circuit_open\n        ),\n    }\n</code></pre>"},{"location":"reference/ragcrawl/core/scheduler/#ragcrawl.core.scheduler.DomainScheduler.release","title":"<code>release(domain, success=True)</code>","text":"<p>Release a request slot.</p> <p>Parameters:</p> Name Type Description Default <code>domain</code> <code>str</code> <p>Target domain.</p> required <code>success</code> <code>bool</code> <p>Whether the request succeeded.</p> <code>True</code> Source code in <code>src/ragcrawl/core/scheduler.py</code> <pre><code>def release(self, domain: str, success: bool = True) -&gt; None:\n    \"\"\"\n    Release a request slot.\n\n    Args:\n        domain: Target domain.\n        success: Whether the request succeeded.\n    \"\"\"\n    state = self._domain_states[domain]\n    state.active_requests = max(0, state.active_requests - 1)\n    self._active_requests = max(0, self._active_requests - 1)\n\n    # Update circuit breaker\n    if success:\n        state.consecutive_errors = 0\n    else:\n        state.error_count += 1\n        state.consecutive_errors += 1\n\n        if state.consecutive_errors &gt;= self._error_threshold:\n            state.circuit_open = True\n            state.circuit_open_until = time.time() + self._circuit_timeout\n            logger.warning(\n                \"Circuit opened for domain\",\n                domain=domain,\n                consecutive_errors=state.consecutive_errors,\n            )\n\n    # Release semaphores\n    self._global_semaphore.release()\n\n    domain_sem = self._get_domain_semaphore(domain)\n    domain_sem.release()\n</code></pre>"},{"location":"reference/ragcrawl/core/scheduler/#ragcrawl.core.scheduler.DomainScheduler.set_crawl_delay","title":"<code>set_crawl_delay(domain, delay)</code>","text":"<p>Set custom crawl delay for a domain (from robots.txt).</p> <p>Parameters:</p> Name Type Description Default <code>domain</code> <code>str</code> <p>Target domain.</p> required <code>delay</code> <code>float</code> <p>Delay in seconds.</p> required Source code in <code>src/ragcrawl/core/scheduler.py</code> <pre><code>def set_crawl_delay(self, domain: str, delay: float) -&gt; None:\n    \"\"\"\n    Set custom crawl delay for a domain (from robots.txt).\n\n    Args:\n        domain: Target domain.\n        delay: Delay in seconds.\n    \"\"\"\n    # Store as effective per-domain rate\n    # This will be used in _wait_domain_rate\n    # For now, just log it\n    logger.debug(\"Crawl delay set\", domain=domain, delay=delay)\n</code></pre>"},{"location":"reference/ragcrawl/core/scheduler/#ragcrawl.core.scheduler.DomainState","title":"<code>DomainState(last_request_time=0.0, request_count=0, active_requests=0, error_count=0, consecutive_errors=0, circuit_open=False, circuit_open_until=0.0)</code>  <code>dataclass</code>","text":"<p>State for a single domain.</p>"},{"location":"reference/ragcrawl/core/sync_job/","title":"Sync job","text":""},{"location":"reference/ragcrawl/core/sync_job/#ragcrawl.core.sync_job","title":"<code>sync_job</code>","text":"<p>Incremental sync job for detecting and processing changes.</p>"},{"location":"reference/ragcrawl/core/sync_job/#ragcrawl.core.sync_job.SyncJob","title":"<code>SyncJob(config)</code>","text":"<p>Incremental sync job for detecting content changes.</p> <p>Uses multiple strategies: 1. Sitemap lastmod (if available) 2. HTTP conditional requests (ETag/Last-Modified) 3. Content hash diffing (fallback)</p> <p>Initialize sync job.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>SyncConfig</code> <p>Sync configuration.</p> required Source code in <code>src/ragcrawl/core/sync_job.py</code> <pre><code>def __init__(self, config: SyncConfig) -&gt; None:\n    \"\"\"\n    Initialize sync job.\n\n    Args:\n        config: Sync configuration.\n    \"\"\"\n    self.config = config\n    self.site_id = config.site_id\n    self.run_id = generate_run_id()\n\n    # Components\n    self._storage: StorageBackend | None = None\n    self._fetcher: Crawl4AIFetcher | None = None\n    self._extractor: ContentExtractor | None = None\n    self._sitemap_parser: SitemapParser | None = None\n    self._change_detector: ChangeDetector | None = None\n    self._revalidator: Revalidator | None = None\n\n    # Tracking\n    self._metrics = MetricsCollector()\n    self._logger = CrawlLoggerAdapter(self.run_id, self.site_id)\n    self._changed_pages: list[str] = []\n    self._deleted_pages: list[str] = []\n</code></pre>"},{"location":"reference/ragcrawl/core/sync_job/#ragcrawl.core.sync_job.SyncJob.run","title":"<code>run()</code>  <code>async</code>","text":"<p>Execute the sync job.</p> <p>Returns:</p> Type Description <code>SyncResult</code> <p>SyncResult with changed and deleted pages.</p> Source code in <code>src/ragcrawl/core/sync_job.py</code> <pre><code>async def run(self) -&gt; SyncResult:\n    \"\"\"\n    Execute the sync job.\n\n    Returns:\n        SyncResult with changed and deleted pages.\n    \"\"\"\n    start_time = datetime.now()\n\n    try:\n        self._init_components()\n\n        # Verify site exists\n        site = self._storage.get_site(self.site_id)\n        if not site:\n            raise ValueError(f\"Site not found: {self.site_id}\")\n\n        # Create sync run record\n        crawl_run = CrawlRun(\n            run_id=self.run_id,\n            site_id=self.site_id,\n            is_sync=True,\n            config_snapshot=self.config.model_dump(\n                exclude={\"on_page\", \"on_change_detected\", \"on_deletion_detected\", \"on_error\"}\n            ),\n        )\n        crawl_run.mark_started()\n        self._storage.save_run(crawl_run)\n\n        # Get pages to check\n        pages = await self._get_pages_to_check()\n\n        logger.info(\"Starting sync\", site_id=self.site_id, pages_to_check=len(pages))\n\n        # Process pages\n        for page in pages:\n            await self._process_page(page)\n\n            # Check limit\n            if self.config.max_pages and self._metrics.metrics.pages_crawled &gt;= self.config.max_pages:\n                break\n\n        # Finalize\n        metrics = self._metrics.finalize()\n        crawl_run.stats = CrawlStats(\n            pages_crawled=metrics.pages_crawled,\n            pages_changed=metrics.pages_changed,\n            pages_unchanged=metrics.pages_unchanged,\n            pages_deleted=metrics.pages_deleted,\n            pages_failed=metrics.pages_failed,\n        )\n        crawl_run.mark_completed(partial=metrics.pages_failed &gt; 0)\n        self._storage.save_run(crawl_run)\n\n        # Update site\n        site.last_sync_at = datetime.now()\n        self._storage.save_site(site)\n\n        duration = (datetime.now() - start_time).total_seconds()\n\n        return SyncResult(\n            run_id=self.run_id,\n            site_id=self.site_id,\n            success=True,\n            stats=crawl_run.stats,\n            changed_pages=self._changed_pages,\n            deleted_pages=self._deleted_pages,\n            duration_seconds=duration,\n        )\n\n    except Exception as e:\n        logger.error(\"Sync job failed\", error=str(e))\n        return SyncResult(\n            run_id=self.run_id,\n            site_id=self.site_id,\n            success=False,\n            error=str(e),\n            duration_seconds=(datetime.now() - start_time).total_seconds(),\n        )\n\n    finally:\n        if self._fetcher:\n            await self._fetcher.close()\n        if self._storage:\n            self._storage.close()\n</code></pre>"},{"location":"reference/ragcrawl/core/sync_job/#ragcrawl.core.sync_job.SyncResult","title":"<code>SyncResult(run_id, site_id, success, stats=CrawlStats(), changed_pages=list(), deleted_pages=list(), error=None, duration_seconds=0.0)</code>  <code>dataclass</code>","text":"<p>Result of a sync job.</p>"},{"location":"reference/ragcrawl/export/","title":"Index","text":""},{"location":"reference/ragcrawl/export/#ragcrawl.export","title":"<code>export</code>","text":"<p>Export functionality for ragcrawl.</p>"},{"location":"reference/ragcrawl/export/#ragcrawl.export.ChangeEvent","title":"<code>ChangeEvent(event_type, page_id, url, site_id, run_id, timestamp, version_id=None, old_version_id=None, content_hash=None, old_content_hash=None, metadata=None)</code>  <code>dataclass</code>","text":"<p>Event representing a change to a page.</p> <p>Used for notifying downstream systems of KB updates.</p>"},{"location":"reference/ragcrawl/export/#ragcrawl.export.ChangeEvent.to_dict","title":"<code>to_dict()</code>","text":"<p>Convert to dictionary.</p> Source code in <code>src/ragcrawl/export/events.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Convert to dictionary.\"\"\"\n    return {\n        \"event_type\": self.event_type.value,\n        \"page_id\": self.page_id,\n        \"url\": self.url,\n        \"site_id\": self.site_id,\n        \"run_id\": self.run_id,\n        \"timestamp\": self.timestamp.isoformat(),\n        \"version_id\": self.version_id,\n        \"old_version_id\": self.old_version_id,\n        \"content_hash\": self.content_hash,\n        \"old_content_hash\": self.old_content_hash,\n        \"metadata\": self.metadata,\n    }\n</code></pre>"},{"location":"reference/ragcrawl/export/#ragcrawl.export.EventType","title":"<code>EventType</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Types of change events.</p>"},{"location":"reference/ragcrawl/export/#ragcrawl.export.Exporter","title":"<code>Exporter</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for content exporters.</p> <p>Exporters serialize documents and chunks for downstream pipelines.</p>"},{"location":"reference/ragcrawl/export/#ragcrawl.export.Exporter.export_chunk","title":"<code>export_chunk(chunk, path=None)</code>  <code>abstractmethod</code>","text":"<p>Export a single chunk.</p> <p>Parameters:</p> Name Type Description Default <code>chunk</code> <code>Chunk</code> <p>Chunk to export.</p> required <code>path</code> <code>Path | None</code> <p>Optional file path to write to.</p> <code>None</code> <p>Returns:</p> Type Description <code>str | None</code> <p>Serialized chunk string, or None if written to file.</p> Source code in <code>src/ragcrawl/export/exporter.py</code> <pre><code>@abstractmethod\ndef export_chunk(self, chunk: Chunk, path: Path | None = None) -&gt; str | None:\n    \"\"\"\n    Export a single chunk.\n\n    Args:\n        chunk: Chunk to export.\n        path: Optional file path to write to.\n\n    Returns:\n        Serialized chunk string, or None if written to file.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/ragcrawl/export/#ragcrawl.export.Exporter.export_chunks","title":"<code>export_chunks(chunks, path)</code>  <code>abstractmethod</code>","text":"<p>Export multiple chunks to a file.</p> <p>Parameters:</p> Name Type Description Default <code>chunks</code> <code>list[Chunk]</code> <p>Chunks to export.</p> required <code>path</code> <code>Path</code> <p>File path to write to.</p> required Source code in <code>src/ragcrawl/export/exporter.py</code> <pre><code>@abstractmethod\ndef export_chunks(self, chunks: list[Chunk], path: Path) -&gt; None:\n    \"\"\"\n    Export multiple chunks to a file.\n\n    Args:\n        chunks: Chunks to export.\n        path: File path to write to.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/ragcrawl/export/#ragcrawl.export.Exporter.export_document","title":"<code>export_document(document, path=None)</code>  <code>abstractmethod</code>","text":"<p>Export a single document.</p> <p>Parameters:</p> Name Type Description Default <code>document</code> <code>Document</code> <p>Document to export.</p> required <code>path</code> <code>Path | None</code> <p>Optional file path to write to.</p> <code>None</code> <p>Returns:</p> Type Description <code>str | None</code> <p>Serialized document string, or None if written to file.</p> Source code in <code>src/ragcrawl/export/exporter.py</code> <pre><code>@abstractmethod\ndef export_document(self, document: Document, path: Path | None = None) -&gt; str | None:\n    \"\"\"\n    Export a single document.\n\n    Args:\n        document: Document to export.\n        path: Optional file path to write to.\n\n    Returns:\n        Serialized document string, or None if written to file.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/ragcrawl/export/#ragcrawl.export.Exporter.export_documents","title":"<code>export_documents(documents, path)</code>  <code>abstractmethod</code>","text":"<p>Export multiple documents to a file.</p> <p>Parameters:</p> Name Type Description Default <code>documents</code> <code>list[Document]</code> <p>Documents to export.</p> required <code>path</code> <code>Path</code> <p>File path to write to.</p> required Source code in <code>src/ragcrawl/export/exporter.py</code> <pre><code>@abstractmethod\ndef export_documents(\n    self, documents: list[Document], path: Path\n) -&gt; None:\n    \"\"\"\n    Export multiple documents to a file.\n\n    Args:\n        documents: Documents to export.\n        path: File path to write to.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/ragcrawl/export/#ragcrawl.export.JSONExporter","title":"<code>JSONExporter(indent=2, include_html=False, include_diagnostics=True)</code>","text":"<p>               Bases: <code>Exporter</code></p> <p>Exports documents and chunks as JSON.</p> <p>Initialize JSON exporter.</p> <p>Parameters:</p> Name Type Description Default <code>indent</code> <code>int | None</code> <p>JSON indentation (None for compact).</p> <code>2</code> <code>include_html</code> <code>bool</code> <p>Include HTML content in export.</p> <code>False</code> <code>include_diagnostics</code> <code>bool</code> <p>Include diagnostic info.</p> <code>True</code> Source code in <code>src/ragcrawl/export/json_exporter.py</code> <pre><code>def __init__(\n    self,\n    indent: int | None = 2,\n    include_html: bool = False,\n    include_diagnostics: bool = True,\n) -&gt; None:\n    \"\"\"\n    Initialize JSON exporter.\n\n    Args:\n        indent: JSON indentation (None for compact).\n        include_html: Include HTML content in export.\n        include_diagnostics: Include diagnostic info.\n    \"\"\"\n    self.indent = indent\n    self.include_html = include_html\n    self.include_diagnostics = include_diagnostics\n</code></pre>"},{"location":"reference/ragcrawl/export/#ragcrawl.export.JSONExporter.export_chunk","title":"<code>export_chunk(chunk, path=None)</code>","text":"<p>Export a chunk as JSON.</p> Source code in <code>src/ragcrawl/export/json_exporter.py</code> <pre><code>def export_chunk(self, chunk: Chunk, path: Path | None = None) -&gt; str | None:\n    \"\"\"Export a chunk as JSON.\"\"\"\n    data = self._chunk_to_dict(chunk)\n    json_str = json.dumps(data, indent=self.indent, default=self._json_serializer)\n\n    if path:\n        path.parent.mkdir(parents=True, exist_ok=True)\n        path.write_text(json_str)\n        return None\n\n    return json_str\n</code></pre>"},{"location":"reference/ragcrawl/export/#ragcrawl.export.JSONExporter.export_chunks","title":"<code>export_chunks(chunks, path)</code>","text":"<p>Export chunks as JSON array.</p> Source code in <code>src/ragcrawl/export/json_exporter.py</code> <pre><code>def export_chunks(self, chunks: list[Chunk], path: Path) -&gt; None:\n    \"\"\"Export chunks as JSON array.\"\"\"\n    data = [self._chunk_to_dict(chunk) for chunk in chunks]\n    json_str = json.dumps(data, indent=self.indent, default=self._json_serializer)\n\n    path.parent.mkdir(parents=True, exist_ok=True)\n    path.write_text(json_str)\n</code></pre>"},{"location":"reference/ragcrawl/export/#ragcrawl.export.JSONExporter.export_document","title":"<code>export_document(document, path=None)</code>","text":"<p>Export a document as JSON.</p> Source code in <code>src/ragcrawl/export/json_exporter.py</code> <pre><code>def export_document(\n    self, document: Document, path: Path | None = None\n) -&gt; str | None:\n    \"\"\"Export a document as JSON.\"\"\"\n    data = self._document_to_dict(document)\n    json_str = json.dumps(data, indent=self.indent, default=self._json_serializer)\n\n    if path:\n        path.parent.mkdir(parents=True, exist_ok=True)\n        path.write_text(json_str)\n        return None\n\n    return json_str\n</code></pre>"},{"location":"reference/ragcrawl/export/#ragcrawl.export.JSONExporter.export_documents","title":"<code>export_documents(documents, path)</code>","text":"<p>Export documents as JSON array.</p> Source code in <code>src/ragcrawl/export/json_exporter.py</code> <pre><code>def export_documents(self, documents: list[Document], path: Path) -&gt; None:\n    \"\"\"Export documents as JSON array.\"\"\"\n    data = [self._document_to_dict(doc) for doc in documents]\n    json_str = json.dumps(data, indent=self.indent, default=self._json_serializer)\n\n    path.parent.mkdir(parents=True, exist_ok=True)\n    path.write_text(json_str)\n</code></pre>"},{"location":"reference/ragcrawl/export/#ragcrawl.export.JSONLExporter","title":"<code>JSONLExporter(include_html=False, include_diagnostics=True)</code>","text":"<p>               Bases: <code>Exporter</code></p> <p>Exports documents and chunks as JSONL (one JSON object per line).</p> <p>JSONL is better for streaming and large datasets.</p> <p>Initialize JSONL exporter.</p> <p>Parameters:</p> Name Type Description Default <code>include_html</code> <code>bool</code> <p>Include HTML content.</p> <code>False</code> <code>include_diagnostics</code> <code>bool</code> <p>Include diagnostics.</p> <code>True</code> Source code in <code>src/ragcrawl/export/json_exporter.py</code> <pre><code>def __init__(\n    self,\n    include_html: bool = False,\n    include_diagnostics: bool = True,\n) -&gt; None:\n    \"\"\"\n    Initialize JSONL exporter.\n\n    Args:\n        include_html: Include HTML content.\n        include_diagnostics: Include diagnostics.\n    \"\"\"\n    self.include_html = include_html\n    self.include_diagnostics = include_diagnostics\n    self._json_exporter = JSONExporter(\n        indent=None,\n        include_html=include_html,\n        include_diagnostics=include_diagnostics,\n    )\n</code></pre>"},{"location":"reference/ragcrawl/export/#ragcrawl.export.JSONLExporter.export_chunk","title":"<code>export_chunk(chunk, path=None)</code>","text":"<p>Export a chunk as JSONL line.</p> Source code in <code>src/ragcrawl/export/json_exporter.py</code> <pre><code>def export_chunk(self, chunk: Chunk, path: Path | None = None) -&gt; str | None:\n    \"\"\"Export a chunk as JSONL line.\"\"\"\n    return self._json_exporter.export_chunk(chunk, path)\n</code></pre>"},{"location":"reference/ragcrawl/export/#ragcrawl.export.JSONLExporter.export_chunks","title":"<code>export_chunks(chunks, path)</code>","text":"<p>Export chunks as JSONL file.</p> Source code in <code>src/ragcrawl/export/json_exporter.py</code> <pre><code>def export_chunks(self, chunks: list[Chunk], path: Path) -&gt; None:\n    \"\"\"Export chunks as JSONL file.\"\"\"\n    path.parent.mkdir(parents=True, exist_ok=True)\n\n    with path.open(\"w\") as f:\n        for chunk in chunks:\n            line = self._json_exporter.export_chunk(chunk)\n            f.write(line + \"\\n\")\n</code></pre>"},{"location":"reference/ragcrawl/export/#ragcrawl.export.JSONLExporter.export_document","title":"<code>export_document(document, path=None)</code>","text":"<p>Export a document as JSONL line.</p> Source code in <code>src/ragcrawl/export/json_exporter.py</code> <pre><code>def export_document(\n    self, document: Document, path: Path | None = None\n) -&gt; str | None:\n    \"\"\"Export a document as JSONL line.\"\"\"\n    return self._json_exporter.export_document(document, path)\n</code></pre>"},{"location":"reference/ragcrawl/export/#ragcrawl.export.JSONLExporter.export_documents","title":"<code>export_documents(documents, path)</code>","text":"<p>Export documents as JSONL file.</p> Source code in <code>src/ragcrawl/export/json_exporter.py</code> <pre><code>def export_documents(self, documents: list[Document], path: Path) -&gt; None:\n    \"\"\"Export documents as JSONL file.\"\"\"\n    path.parent.mkdir(parents=True, exist_ok=True)\n\n    with path.open(\"w\") as f:\n        for doc in documents:\n            line = self._json_exporter.export_document(doc)\n            f.write(line + \"\\n\")\n</code></pre>"},{"location":"reference/ragcrawl/export/events/","title":"Events","text":""},{"location":"reference/ragcrawl/export/events/#ragcrawl.export.events","title":"<code>events</code>","text":"<p>Change events for downstream consumers.</p>"},{"location":"reference/ragcrawl/export/events/#ragcrawl.export.events.ChangeEvent","title":"<code>ChangeEvent(event_type, page_id, url, site_id, run_id, timestamp, version_id=None, old_version_id=None, content_hash=None, old_content_hash=None, metadata=None)</code>  <code>dataclass</code>","text":"<p>Event representing a change to a page.</p> <p>Used for notifying downstream systems of KB updates.</p>"},{"location":"reference/ragcrawl/export/events/#ragcrawl.export.events.ChangeEvent.to_dict","title":"<code>to_dict()</code>","text":"<p>Convert to dictionary.</p> Source code in <code>src/ragcrawl/export/events.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Convert to dictionary.\"\"\"\n    return {\n        \"event_type\": self.event_type.value,\n        \"page_id\": self.page_id,\n        \"url\": self.url,\n        \"site_id\": self.site_id,\n        \"run_id\": self.run_id,\n        \"timestamp\": self.timestamp.isoformat(),\n        \"version_id\": self.version_id,\n        \"old_version_id\": self.old_version_id,\n        \"content_hash\": self.content_hash,\n        \"old_content_hash\": self.old_content_hash,\n        \"metadata\": self.metadata,\n    }\n</code></pre>"},{"location":"reference/ragcrawl/export/events/#ragcrawl.export.events.EventEmitter","title":"<code>EventEmitter()</code>","text":"<p>Emits change events to registered handlers.</p> <p>Initialize event emitter.</p> Source code in <code>src/ragcrawl/export/events.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize event emitter.\"\"\"\n    self._handlers: list[callable] = []\n</code></pre>"},{"location":"reference/ragcrawl/export/events/#ragcrawl.export.events.EventEmitter.emit","title":"<code>emit(event)</code>","text":"<p>Emit an event to all handlers.</p> Source code in <code>src/ragcrawl/export/events.py</code> <pre><code>def emit(self, event: ChangeEvent) -&gt; None:\n    \"\"\"Emit an event to all handlers.\"\"\"\n    for handler in self._handlers:\n        try:\n            handler(event)\n        except Exception:\n            pass  # Don't let handler errors break the flow\n</code></pre>"},{"location":"reference/ragcrawl/export/events/#ragcrawl.export.events.EventEmitter.emit_changed","title":"<code>emit_changed(page_id, url, site_id, run_id, version_id, old_version_id, content_hash, old_content_hash)</code>","text":"<p>Emit page changed event.</p> Source code in <code>src/ragcrawl/export/events.py</code> <pre><code>def emit_changed(\n    self,\n    page_id: str,\n    url: str,\n    site_id: str,\n    run_id: str,\n    version_id: str,\n    old_version_id: str | None,\n    content_hash: str,\n    old_content_hash: str | None,\n) -&gt; None:\n    \"\"\"Emit page changed event.\"\"\"\n    self.emit(ChangeEvent(\n        event_type=EventType.PAGE_CHANGED,\n        page_id=page_id,\n        url=url,\n        site_id=site_id,\n        run_id=run_id,\n        timestamp=datetime.now(),\n        version_id=version_id,\n        old_version_id=old_version_id,\n        content_hash=content_hash,\n        old_content_hash=old_content_hash,\n    ))\n</code></pre>"},{"location":"reference/ragcrawl/export/events/#ragcrawl.export.events.EventEmitter.emit_created","title":"<code>emit_created(page_id, url, site_id, run_id, version_id, content_hash)</code>","text":"<p>Emit page created event.</p> Source code in <code>src/ragcrawl/export/events.py</code> <pre><code>def emit_created(\n    self,\n    page_id: str,\n    url: str,\n    site_id: str,\n    run_id: str,\n    version_id: str,\n    content_hash: str,\n) -&gt; None:\n    \"\"\"Emit page created event.\"\"\"\n    self.emit(ChangeEvent(\n        event_type=EventType.PAGE_CREATED,\n        page_id=page_id,\n        url=url,\n        site_id=site_id,\n        run_id=run_id,\n        timestamp=datetime.now(),\n        version_id=version_id,\n        content_hash=content_hash,\n    ))\n</code></pre>"},{"location":"reference/ragcrawl/export/events/#ragcrawl.export.events.EventEmitter.emit_deleted","title":"<code>emit_deleted(page_id, url, site_id, run_id)</code>","text":"<p>Emit page deleted event.</p> Source code in <code>src/ragcrawl/export/events.py</code> <pre><code>def emit_deleted(\n    self,\n    page_id: str,\n    url: str,\n    site_id: str,\n    run_id: str,\n) -&gt; None:\n    \"\"\"Emit page deleted event.\"\"\"\n    self.emit(ChangeEvent(\n        event_type=EventType.PAGE_DELETED,\n        page_id=page_id,\n        url=url,\n        site_id=site_id,\n        run_id=run_id,\n        timestamp=datetime.now(),\n    ))\n</code></pre>"},{"location":"reference/ragcrawl/export/events/#ragcrawl.export.events.EventEmitter.register","title":"<code>register(handler)</code>","text":"<p>Register an event handler.</p> Source code in <code>src/ragcrawl/export/events.py</code> <pre><code>def register(self, handler: callable) -&gt; None:\n    \"\"\"Register an event handler.\"\"\"\n    self._handlers.append(handler)\n</code></pre>"},{"location":"reference/ragcrawl/export/events/#ragcrawl.export.events.EventEmitter.unregister","title":"<code>unregister(handler)</code>","text":"<p>Unregister an event handler.</p> Source code in <code>src/ragcrawl/export/events.py</code> <pre><code>def unregister(self, handler: callable) -&gt; None:\n    \"\"\"Unregister an event handler.\"\"\"\n    if handler in self._handlers:\n        self._handlers.remove(handler)\n</code></pre>"},{"location":"reference/ragcrawl/export/events/#ragcrawl.export.events.EventType","title":"<code>EventType</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Types of change events.</p>"},{"location":"reference/ragcrawl/export/exporter/","title":"Exporter","text":""},{"location":"reference/ragcrawl/export/exporter/#ragcrawl.export.exporter","title":"<code>exporter</code>","text":"<p>Base exporter protocol.</p>"},{"location":"reference/ragcrawl/export/exporter/#ragcrawl.export.exporter.Exporter","title":"<code>Exporter</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for content exporters.</p> <p>Exporters serialize documents and chunks for downstream pipelines.</p>"},{"location":"reference/ragcrawl/export/exporter/#ragcrawl.export.exporter.Exporter.export_chunk","title":"<code>export_chunk(chunk, path=None)</code>  <code>abstractmethod</code>","text":"<p>Export a single chunk.</p> <p>Parameters:</p> Name Type Description Default <code>chunk</code> <code>Chunk</code> <p>Chunk to export.</p> required <code>path</code> <code>Path | None</code> <p>Optional file path to write to.</p> <code>None</code> <p>Returns:</p> Type Description <code>str | None</code> <p>Serialized chunk string, or None if written to file.</p> Source code in <code>src/ragcrawl/export/exporter.py</code> <pre><code>@abstractmethod\ndef export_chunk(self, chunk: Chunk, path: Path | None = None) -&gt; str | None:\n    \"\"\"\n    Export a single chunk.\n\n    Args:\n        chunk: Chunk to export.\n        path: Optional file path to write to.\n\n    Returns:\n        Serialized chunk string, or None if written to file.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/ragcrawl/export/exporter/#ragcrawl.export.exporter.Exporter.export_chunks","title":"<code>export_chunks(chunks, path)</code>  <code>abstractmethod</code>","text":"<p>Export multiple chunks to a file.</p> <p>Parameters:</p> Name Type Description Default <code>chunks</code> <code>list[Chunk]</code> <p>Chunks to export.</p> required <code>path</code> <code>Path</code> <p>File path to write to.</p> required Source code in <code>src/ragcrawl/export/exporter.py</code> <pre><code>@abstractmethod\ndef export_chunks(self, chunks: list[Chunk], path: Path) -&gt; None:\n    \"\"\"\n    Export multiple chunks to a file.\n\n    Args:\n        chunks: Chunks to export.\n        path: File path to write to.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/ragcrawl/export/exporter/#ragcrawl.export.exporter.Exporter.export_document","title":"<code>export_document(document, path=None)</code>  <code>abstractmethod</code>","text":"<p>Export a single document.</p> <p>Parameters:</p> Name Type Description Default <code>document</code> <code>Document</code> <p>Document to export.</p> required <code>path</code> <code>Path | None</code> <p>Optional file path to write to.</p> <code>None</code> <p>Returns:</p> Type Description <code>str | None</code> <p>Serialized document string, or None if written to file.</p> Source code in <code>src/ragcrawl/export/exporter.py</code> <pre><code>@abstractmethod\ndef export_document(self, document: Document, path: Path | None = None) -&gt; str | None:\n    \"\"\"\n    Export a single document.\n\n    Args:\n        document: Document to export.\n        path: Optional file path to write to.\n\n    Returns:\n        Serialized document string, or None if written to file.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/ragcrawl/export/exporter/#ragcrawl.export.exporter.Exporter.export_documents","title":"<code>export_documents(documents, path)</code>  <code>abstractmethod</code>","text":"<p>Export multiple documents to a file.</p> <p>Parameters:</p> Name Type Description Default <code>documents</code> <code>list[Document]</code> <p>Documents to export.</p> required <code>path</code> <code>Path</code> <p>File path to write to.</p> required Source code in <code>src/ragcrawl/export/exporter.py</code> <pre><code>@abstractmethod\ndef export_documents(\n    self, documents: list[Document], path: Path\n) -&gt; None:\n    \"\"\"\n    Export multiple documents to a file.\n\n    Args:\n        documents: Documents to export.\n        path: File path to write to.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/ragcrawl/export/json_exporter/","title":"Json exporter","text":""},{"location":"reference/ragcrawl/export/json_exporter/#ragcrawl.export.json_exporter","title":"<code>json_exporter</code>","text":"<p>JSON and JSONL exporters.</p>"},{"location":"reference/ragcrawl/export/json_exporter/#ragcrawl.export.json_exporter.JSONExporter","title":"<code>JSONExporter(indent=2, include_html=False, include_diagnostics=True)</code>","text":"<p>               Bases: <code>Exporter</code></p> <p>Exports documents and chunks as JSON.</p> <p>Initialize JSON exporter.</p> <p>Parameters:</p> Name Type Description Default <code>indent</code> <code>int | None</code> <p>JSON indentation (None for compact).</p> <code>2</code> <code>include_html</code> <code>bool</code> <p>Include HTML content in export.</p> <code>False</code> <code>include_diagnostics</code> <code>bool</code> <p>Include diagnostic info.</p> <code>True</code> Source code in <code>src/ragcrawl/export/json_exporter.py</code> <pre><code>def __init__(\n    self,\n    indent: int | None = 2,\n    include_html: bool = False,\n    include_diagnostics: bool = True,\n) -&gt; None:\n    \"\"\"\n    Initialize JSON exporter.\n\n    Args:\n        indent: JSON indentation (None for compact).\n        include_html: Include HTML content in export.\n        include_diagnostics: Include diagnostic info.\n    \"\"\"\n    self.indent = indent\n    self.include_html = include_html\n    self.include_diagnostics = include_diagnostics\n</code></pre>"},{"location":"reference/ragcrawl/export/json_exporter/#ragcrawl.export.json_exporter.JSONExporter.export_chunk","title":"<code>export_chunk(chunk, path=None)</code>","text":"<p>Export a chunk as JSON.</p> Source code in <code>src/ragcrawl/export/json_exporter.py</code> <pre><code>def export_chunk(self, chunk: Chunk, path: Path | None = None) -&gt; str | None:\n    \"\"\"Export a chunk as JSON.\"\"\"\n    data = self._chunk_to_dict(chunk)\n    json_str = json.dumps(data, indent=self.indent, default=self._json_serializer)\n\n    if path:\n        path.parent.mkdir(parents=True, exist_ok=True)\n        path.write_text(json_str)\n        return None\n\n    return json_str\n</code></pre>"},{"location":"reference/ragcrawl/export/json_exporter/#ragcrawl.export.json_exporter.JSONExporter.export_chunks","title":"<code>export_chunks(chunks, path)</code>","text":"<p>Export chunks as JSON array.</p> Source code in <code>src/ragcrawl/export/json_exporter.py</code> <pre><code>def export_chunks(self, chunks: list[Chunk], path: Path) -&gt; None:\n    \"\"\"Export chunks as JSON array.\"\"\"\n    data = [self._chunk_to_dict(chunk) for chunk in chunks]\n    json_str = json.dumps(data, indent=self.indent, default=self._json_serializer)\n\n    path.parent.mkdir(parents=True, exist_ok=True)\n    path.write_text(json_str)\n</code></pre>"},{"location":"reference/ragcrawl/export/json_exporter/#ragcrawl.export.json_exporter.JSONExporter.export_document","title":"<code>export_document(document, path=None)</code>","text":"<p>Export a document as JSON.</p> Source code in <code>src/ragcrawl/export/json_exporter.py</code> <pre><code>def export_document(\n    self, document: Document, path: Path | None = None\n) -&gt; str | None:\n    \"\"\"Export a document as JSON.\"\"\"\n    data = self._document_to_dict(document)\n    json_str = json.dumps(data, indent=self.indent, default=self._json_serializer)\n\n    if path:\n        path.parent.mkdir(parents=True, exist_ok=True)\n        path.write_text(json_str)\n        return None\n\n    return json_str\n</code></pre>"},{"location":"reference/ragcrawl/export/json_exporter/#ragcrawl.export.json_exporter.JSONExporter.export_documents","title":"<code>export_documents(documents, path)</code>","text":"<p>Export documents as JSON array.</p> Source code in <code>src/ragcrawl/export/json_exporter.py</code> <pre><code>def export_documents(self, documents: list[Document], path: Path) -&gt; None:\n    \"\"\"Export documents as JSON array.\"\"\"\n    data = [self._document_to_dict(doc) for doc in documents]\n    json_str = json.dumps(data, indent=self.indent, default=self._json_serializer)\n\n    path.parent.mkdir(parents=True, exist_ok=True)\n    path.write_text(json_str)\n</code></pre>"},{"location":"reference/ragcrawl/export/json_exporter/#ragcrawl.export.json_exporter.JSONLExporter","title":"<code>JSONLExporter(include_html=False, include_diagnostics=True)</code>","text":"<p>               Bases: <code>Exporter</code></p> <p>Exports documents and chunks as JSONL (one JSON object per line).</p> <p>JSONL is better for streaming and large datasets.</p> <p>Initialize JSONL exporter.</p> <p>Parameters:</p> Name Type Description Default <code>include_html</code> <code>bool</code> <p>Include HTML content.</p> <code>False</code> <code>include_diagnostics</code> <code>bool</code> <p>Include diagnostics.</p> <code>True</code> Source code in <code>src/ragcrawl/export/json_exporter.py</code> <pre><code>def __init__(\n    self,\n    include_html: bool = False,\n    include_diagnostics: bool = True,\n) -&gt; None:\n    \"\"\"\n    Initialize JSONL exporter.\n\n    Args:\n        include_html: Include HTML content.\n        include_diagnostics: Include diagnostics.\n    \"\"\"\n    self.include_html = include_html\n    self.include_diagnostics = include_diagnostics\n    self._json_exporter = JSONExporter(\n        indent=None,\n        include_html=include_html,\n        include_diagnostics=include_diagnostics,\n    )\n</code></pre>"},{"location":"reference/ragcrawl/export/json_exporter/#ragcrawl.export.json_exporter.JSONLExporter.export_chunk","title":"<code>export_chunk(chunk, path=None)</code>","text":"<p>Export a chunk as JSONL line.</p> Source code in <code>src/ragcrawl/export/json_exporter.py</code> <pre><code>def export_chunk(self, chunk: Chunk, path: Path | None = None) -&gt; str | None:\n    \"\"\"Export a chunk as JSONL line.\"\"\"\n    return self._json_exporter.export_chunk(chunk, path)\n</code></pre>"},{"location":"reference/ragcrawl/export/json_exporter/#ragcrawl.export.json_exporter.JSONLExporter.export_chunks","title":"<code>export_chunks(chunks, path)</code>","text":"<p>Export chunks as JSONL file.</p> Source code in <code>src/ragcrawl/export/json_exporter.py</code> <pre><code>def export_chunks(self, chunks: list[Chunk], path: Path) -&gt; None:\n    \"\"\"Export chunks as JSONL file.\"\"\"\n    path.parent.mkdir(parents=True, exist_ok=True)\n\n    with path.open(\"w\") as f:\n        for chunk in chunks:\n            line = self._json_exporter.export_chunk(chunk)\n            f.write(line + \"\\n\")\n</code></pre>"},{"location":"reference/ragcrawl/export/json_exporter/#ragcrawl.export.json_exporter.JSONLExporter.export_document","title":"<code>export_document(document, path=None)</code>","text":"<p>Export a document as JSONL line.</p> Source code in <code>src/ragcrawl/export/json_exporter.py</code> <pre><code>def export_document(\n    self, document: Document, path: Path | None = None\n) -&gt; str | None:\n    \"\"\"Export a document as JSONL line.\"\"\"\n    return self._json_exporter.export_document(document, path)\n</code></pre>"},{"location":"reference/ragcrawl/export/json_exporter/#ragcrawl.export.json_exporter.JSONLExporter.export_documents","title":"<code>export_documents(documents, path)</code>","text":"<p>Export documents as JSONL file.</p> Source code in <code>src/ragcrawl/export/json_exporter.py</code> <pre><code>def export_documents(self, documents: list[Document], path: Path) -&gt; None:\n    \"\"\"Export documents as JSONL file.\"\"\"\n    path.parent.mkdir(parents=True, exist_ok=True)\n\n    with path.open(\"w\") as f:\n        for doc in documents:\n            line = self._json_exporter.export_document(doc)\n            f.write(line + \"\\n\")\n</code></pre>"},{"location":"reference/ragcrawl/extraction/","title":"Index","text":""},{"location":"reference/ragcrawl/extraction/#ragcrawl.extraction","title":"<code>extraction</code>","text":"<p>Content extraction for ragcrawl.</p>"},{"location":"reference/ragcrawl/extraction/#ragcrawl.extraction.ContentExtractor","title":"<code>ContentExtractor(allowed_domains=None)</code>","text":"<p>Extracts structured content from fetched pages.</p> <p>Combines markdown extraction, metadata, and link extraction.</p> <p>Initialize content extractor.</p> <p>Parameters:</p> Name Type Description Default <code>allowed_domains</code> <code>set[str] | None</code> <p>Domains to consider as internal.</p> <code>None</code> Source code in <code>src/ragcrawl/extraction/extractor.py</code> <pre><code>def __init__(\n    self,\n    allowed_domains: set[str] | None = None,\n) -&gt; None:\n    \"\"\"\n    Initialize content extractor.\n\n    Args:\n        allowed_domains: Domains to consider as internal.\n    \"\"\"\n    self.allowed_domains = allowed_domains or set()\n    self.metadata_extractor = MetadataExtractor()\n</code></pre>"},{"location":"reference/ragcrawl/extraction/#ragcrawl.extraction.ContentExtractor.extract","title":"<code>extract(fetch_result, url, extract_html=False, extract_plain_text=False)</code>","text":"<p>Extract content from a fetch result.</p> <p>Parameters:</p> Name Type Description Default <code>fetch_result</code> <code>FetchResult</code> <p>Result from fetcher.</p> required <code>url</code> <code>str</code> <p>URL of the page.</p> required <code>extract_html</code> <code>bool</code> <p>Whether to include cleaned HTML.</p> <code>False</code> <code>extract_plain_text</code> <code>bool</code> <p>Whether to include plain text.</p> <code>False</code> <p>Returns:</p> Type Description <code>ExtractionResult</code> <p>ExtractionResult with extracted content.</p> Source code in <code>src/ragcrawl/extraction/extractor.py</code> <pre><code>def extract(\n    self,\n    fetch_result: FetchResult,\n    url: str,\n    extract_html: bool = False,\n    extract_plain_text: bool = False,\n) -&gt; ExtractionResult:\n    \"\"\"\n    Extract content from a fetch result.\n\n    Args:\n        fetch_result: Result from fetcher.\n        url: URL of the page.\n        extract_html: Whether to include cleaned HTML.\n        extract_plain_text: Whether to include plain text.\n\n    Returns:\n        ExtractionResult with extracted content.\n    \"\"\"\n    import time\n\n    start_time = time.time()\n\n    try:\n        # Get markdown (already extracted by fetcher or fallback)\n        markdown = fetch_result.markdown or \"\"\n\n        # Compute content hash\n        content_hash = compute_content_hash(markdown)\n\n        # Get HTML\n        html = fetch_result.html\n        raw_hash = compute_content_hash(html) if html else None\n\n        # Extract metadata\n        if html:\n            metadata = self.metadata_extractor.extract(html, markdown)\n        else:\n            metadata = PageMetadata(\n                title=fetch_result.title,\n                description=fetch_result.description,\n                word_count=len(markdown.split()),\n                char_count=len(markdown),\n            )\n\n        # Override with fetch result if available\n        if fetch_result.title:\n            metadata.title = fetch_result.title\n        if fetch_result.description:\n            metadata.description = fetch_result.description\n\n        # Extract links\n        link_extractor = LinkExtractor(\n            base_url=url,\n            allowed_domains=self.allowed_domains,\n        )\n\n        if html:\n            links = link_extractor.extract(html)\n            outlinks = [link.href for link in links]\n            internal_links = [link.href for link in links if link.is_internal]\n            external_links = [link.href for link in links if not link.is_internal]\n        else:\n            # Use links from fetch result\n            outlinks = fetch_result.links or []\n            internal_links = [\n                link for link in outlinks if self._is_internal(link, url)\n            ]\n            external_links = [\n                link for link in outlinks if not self._is_internal(link, url)\n            ]\n\n        # Generate plain text if requested\n        plain_text = None\n        if extract_plain_text:\n            plain_text = self._markdown_to_text(markdown)\n\n        latency_ms = (time.time() - start_time) * 1000\n\n        return ExtractionResult(\n            markdown=markdown,\n            html=html if extract_html else None,\n            plain_text=plain_text,\n            content_hash=content_hash,\n            raw_hash=raw_hash,\n            metadata=metadata,\n            outlinks=outlinks,\n            internal_links=internal_links,\n            external_links=external_links,\n            extraction_latency_ms=latency_ms,\n            success=True,\n        )\n\n    except Exception as e:\n        latency_ms = (time.time() - start_time) * 1000\n        return ExtractionResult(\n            markdown=\"\",\n            extraction_latency_ms=latency_ms,\n            success=False,\n            error=str(e),\n        )\n</code></pre>"},{"location":"reference/ragcrawl/extraction/#ragcrawl.extraction.ExtractionResult","title":"<code>ExtractionResult(markdown, html=None, plain_text=None, content_hash='', raw_hash=None, metadata=PageMetadata(), outlinks=list(), internal_links=list(), external_links=list(), extraction_latency_ms=0.0, success=True, error=None)</code>  <code>dataclass</code>","text":"<p>Result of content extraction.</p>"},{"location":"reference/ragcrawl/extraction/#ragcrawl.extraction.LinkExtractor","title":"<code>LinkExtractor(base_url, allowed_domains=None)</code>","text":"<p>Extracts and categorizes links from HTML content.</p> <p>Initialize link extractor.</p> <p>Parameters:</p> Name Type Description Default <code>base_url</code> <code>str</code> <p>Base URL for resolving relative links.</p> required <code>allowed_domains</code> <code>set[str] | None</code> <p>Domains to consider as internal.</p> <code>None</code> Source code in <code>src/ragcrawl/extraction/link_extractor.py</code> <pre><code>def __init__(\n    self,\n    base_url: str,\n    allowed_domains: set[str] | None = None,\n) -&gt; None:\n    \"\"\"\n    Initialize link extractor.\n\n    Args:\n        base_url: Base URL for resolving relative links.\n        allowed_domains: Domains to consider as internal.\n    \"\"\"\n    self.base_url = base_url\n    self.base_parsed = urlparse(base_url)\n    self.base_domain = self.base_parsed.netloc.lower()\n\n    self.allowed_domains = allowed_domains or {self.base_domain}\n</code></pre>"},{"location":"reference/ragcrawl/extraction/#ragcrawl.extraction.LinkExtractor.extract","title":"<code>extract(html)</code>","text":"<p>Extract all links from HTML.</p> <p>Parameters:</p> Name Type Description Default <code>html</code> <code>str</code> <p>HTML content.</p> required <p>Returns:</p> Type Description <code>list[ExtractedLink]</code> <p>List of extracted links.</p> Source code in <code>src/ragcrawl/extraction/link_extractor.py</code> <pre><code>def extract(self, html: str) -&gt; list[ExtractedLink]:\n    \"\"\"\n    Extract all links from HTML.\n\n    Args:\n        html: HTML content.\n\n    Returns:\n        List of extracted links.\n    \"\"\"\n    links: list[ExtractedLink] = []\n    seen_hrefs: set[str] = set()\n\n    # Match anchor tags\n    pattern = r\"&lt;a\\s+([^&gt;]*)&gt;(.*?)&lt;/a&gt;\"\n    for match in re.finditer(pattern, html, re.IGNORECASE | re.DOTALL):\n        attrs = match.group(1)\n        text = self._clean_text(match.group(2))\n\n        # Extract href\n        href_match = re.search(r'href=[\"\\']([^\"\\']+)[\"\\']', attrs)\n        if not href_match:\n            continue\n\n        href = href_match.group(1).strip()\n\n        # Skip javascript:, mailto:, tel:, etc.\n        if self._is_special_scheme(href):\n            continue\n\n        # Resolve relative URLs\n        resolved = self._resolve_url(href)\n        if not resolved:\n            continue\n\n        # Deduplicate\n        normalized = self._normalize_for_dedup(resolved)\n        if normalized in seen_hrefs:\n            continue\n        seen_hrefs.add(normalized)\n\n        # Check if nofollow\n        is_nofollow = \"nofollow\" in attrs.lower()\n\n        # Check if internal\n        is_internal = self._is_internal(resolved)\n\n        # Extract anchor\n        anchor = None\n        parsed = urlparse(resolved)\n        if parsed.fragment:\n            anchor = parsed.fragment\n\n        links.append(\n            ExtractedLink(\n                href=resolved,\n                text=text if text else None,\n                is_internal=is_internal,\n                is_nofollow=is_nofollow,\n                anchor=anchor,\n            )\n        )\n\n    return links\n</code></pre>"},{"location":"reference/ragcrawl/extraction/#ragcrawl.extraction.LinkExtractor.extract_external_urls","title":"<code>extract_external_urls(html)</code>","text":"<p>Extract only external URLs.</p> <p>Parameters:</p> Name Type Description Default <code>html</code> <code>str</code> <p>HTML content.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List of external URLs.</p> Source code in <code>src/ragcrawl/extraction/link_extractor.py</code> <pre><code>def extract_external_urls(self, html: str) -&gt; list[str]:\n    \"\"\"\n    Extract only external URLs.\n\n    Args:\n        html: HTML content.\n\n    Returns:\n        List of external URLs.\n    \"\"\"\n    return [link.href for link in self.extract(html) if not link.is_internal]\n</code></pre>"},{"location":"reference/ragcrawl/extraction/#ragcrawl.extraction.LinkExtractor.extract_internal_urls","title":"<code>extract_internal_urls(html)</code>","text":"<p>Extract only internal URLs.</p> <p>Parameters:</p> Name Type Description Default <code>html</code> <code>str</code> <p>HTML content.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List of internal URLs.</p> Source code in <code>src/ragcrawl/extraction/link_extractor.py</code> <pre><code>def extract_internal_urls(self, html: str) -&gt; list[str]:\n    \"\"\"\n    Extract only internal URLs.\n\n    Args:\n        html: HTML content.\n\n    Returns:\n        List of internal URLs.\n    \"\"\"\n    return [link.href for link in self.extract(html) if link.is_internal]\n</code></pre>"},{"location":"reference/ragcrawl/extraction/#ragcrawl.extraction.LinkExtractor.extract_urls","title":"<code>extract_urls(html)</code>","text":"<p>Extract just the URLs from HTML.</p> <p>Parameters:</p> Name Type Description Default <code>html</code> <code>str</code> <p>HTML content.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List of resolved URLs.</p> Source code in <code>src/ragcrawl/extraction/link_extractor.py</code> <pre><code>def extract_urls(self, html: str) -&gt; list[str]:\n    \"\"\"\n    Extract just the URLs from HTML.\n\n    Args:\n        html: HTML content.\n\n    Returns:\n        List of resolved URLs.\n    \"\"\"\n    return [link.href for link in self.extract(html)]\n</code></pre>"},{"location":"reference/ragcrawl/extraction/#ragcrawl.extraction.MetadataExtractor","title":"<code>MetadataExtractor</code>","text":"<p>Extracts metadata from HTML pages.</p>"},{"location":"reference/ragcrawl/extraction/#ragcrawl.extraction.MetadataExtractor.extract","title":"<code>extract(html, text=None)</code>","text":"<p>Extract metadata from HTML.</p> <p>Parameters:</p> Name Type Description Default <code>html</code> <code>str</code> <p>HTML content.</p> required <code>text</code> <code>str | None</code> <p>Optional plain text for word/char counting.</p> <code>None</code> <p>Returns:</p> Type Description <code>PageMetadata</code> <p>PageMetadata with extracted values.</p> Source code in <code>src/ragcrawl/extraction/metadata.py</code> <pre><code>def extract(self, html: str, text: str | None = None) -&gt; PageMetadata:\n    \"\"\"\n    Extract metadata from HTML.\n\n    Args:\n        html: HTML content.\n        text: Optional plain text for word/char counting.\n\n    Returns:\n        PageMetadata with extracted values.\n    \"\"\"\n    metadata = PageMetadata()\n\n    # Title\n    metadata.title = self._extract_title(html)\n\n    # Meta tags\n    metadata.description = self._extract_meta(html, \"description\")\n    metadata.keywords = self._extract_keywords(html)\n    metadata.author = self._extract_meta(html, \"author\")\n    metadata.canonical_url = self._extract_canonical(html)\n    metadata.language = self._extract_language(html)\n\n    # Dates\n    metadata.published_date = self._extract_meta(\n        html, \"article:published_time\"\n    ) or self._extract_meta(html, \"datePublished\")\n    metadata.modified_date = self._extract_meta(\n        html, \"article:modified_time\"\n    ) or self._extract_meta(html, \"dateModified\")\n\n    # Open Graph\n    metadata.og_title = self._extract_meta(html, \"og:title\", property_attr=True)\n    metadata.og_description = self._extract_meta(html, \"og:description\", property_attr=True)\n    metadata.og_image = self._extract_meta(html, \"og:image\", property_attr=True)\n    metadata.og_type = self._extract_meta(html, \"og:type\", property_attr=True)\n\n    # Headings outline\n    metadata.headings_outline = self._extract_headings(html)\n\n    # Word/char count\n    if text:\n        metadata.word_count = len(text.split())\n        metadata.char_count = len(text)\n\n    return metadata\n</code></pre>"},{"location":"reference/ragcrawl/extraction/#ragcrawl.extraction.PageMetadata","title":"<code>PageMetadata(title=None, description=None, canonical_url=None, language=None, author=None, published_date=None, modified_date=None, keywords=list(), headings_outline=list(), og_title=None, og_description=None, og_image=None, og_type=None, word_count=0, char_count=0)</code>  <code>dataclass</code>","text":"<p>Extracted metadata from a page.</p>"},{"location":"reference/ragcrawl/extraction/extractor/","title":"Extractor","text":""},{"location":"reference/ragcrawl/extraction/extractor/#ragcrawl.extraction.extractor","title":"<code>extractor</code>","text":"<p>Content extraction from fetched pages.</p>"},{"location":"reference/ragcrawl/extraction/extractor/#ragcrawl.extraction.extractor.ContentExtractor","title":"<code>ContentExtractor(allowed_domains=None)</code>","text":"<p>Extracts structured content from fetched pages.</p> <p>Combines markdown extraction, metadata, and link extraction.</p> <p>Initialize content extractor.</p> <p>Parameters:</p> Name Type Description Default <code>allowed_domains</code> <code>set[str] | None</code> <p>Domains to consider as internal.</p> <code>None</code> Source code in <code>src/ragcrawl/extraction/extractor.py</code> <pre><code>def __init__(\n    self,\n    allowed_domains: set[str] | None = None,\n) -&gt; None:\n    \"\"\"\n    Initialize content extractor.\n\n    Args:\n        allowed_domains: Domains to consider as internal.\n    \"\"\"\n    self.allowed_domains = allowed_domains or set()\n    self.metadata_extractor = MetadataExtractor()\n</code></pre>"},{"location":"reference/ragcrawl/extraction/extractor/#ragcrawl.extraction.extractor.ContentExtractor.extract","title":"<code>extract(fetch_result, url, extract_html=False, extract_plain_text=False)</code>","text":"<p>Extract content from a fetch result.</p> <p>Parameters:</p> Name Type Description Default <code>fetch_result</code> <code>FetchResult</code> <p>Result from fetcher.</p> required <code>url</code> <code>str</code> <p>URL of the page.</p> required <code>extract_html</code> <code>bool</code> <p>Whether to include cleaned HTML.</p> <code>False</code> <code>extract_plain_text</code> <code>bool</code> <p>Whether to include plain text.</p> <code>False</code> <p>Returns:</p> Type Description <code>ExtractionResult</code> <p>ExtractionResult with extracted content.</p> Source code in <code>src/ragcrawl/extraction/extractor.py</code> <pre><code>def extract(\n    self,\n    fetch_result: FetchResult,\n    url: str,\n    extract_html: bool = False,\n    extract_plain_text: bool = False,\n) -&gt; ExtractionResult:\n    \"\"\"\n    Extract content from a fetch result.\n\n    Args:\n        fetch_result: Result from fetcher.\n        url: URL of the page.\n        extract_html: Whether to include cleaned HTML.\n        extract_plain_text: Whether to include plain text.\n\n    Returns:\n        ExtractionResult with extracted content.\n    \"\"\"\n    import time\n\n    start_time = time.time()\n\n    try:\n        # Get markdown (already extracted by fetcher or fallback)\n        markdown = fetch_result.markdown or \"\"\n\n        # Compute content hash\n        content_hash = compute_content_hash(markdown)\n\n        # Get HTML\n        html = fetch_result.html\n        raw_hash = compute_content_hash(html) if html else None\n\n        # Extract metadata\n        if html:\n            metadata = self.metadata_extractor.extract(html, markdown)\n        else:\n            metadata = PageMetadata(\n                title=fetch_result.title,\n                description=fetch_result.description,\n                word_count=len(markdown.split()),\n                char_count=len(markdown),\n            )\n\n        # Override with fetch result if available\n        if fetch_result.title:\n            metadata.title = fetch_result.title\n        if fetch_result.description:\n            metadata.description = fetch_result.description\n\n        # Extract links\n        link_extractor = LinkExtractor(\n            base_url=url,\n            allowed_domains=self.allowed_domains,\n        )\n\n        if html:\n            links = link_extractor.extract(html)\n            outlinks = [link.href for link in links]\n            internal_links = [link.href for link in links if link.is_internal]\n            external_links = [link.href for link in links if not link.is_internal]\n        else:\n            # Use links from fetch result\n            outlinks = fetch_result.links or []\n            internal_links = [\n                link for link in outlinks if self._is_internal(link, url)\n            ]\n            external_links = [\n                link for link in outlinks if not self._is_internal(link, url)\n            ]\n\n        # Generate plain text if requested\n        plain_text = None\n        if extract_plain_text:\n            plain_text = self._markdown_to_text(markdown)\n\n        latency_ms = (time.time() - start_time) * 1000\n\n        return ExtractionResult(\n            markdown=markdown,\n            html=html if extract_html else None,\n            plain_text=plain_text,\n            content_hash=content_hash,\n            raw_hash=raw_hash,\n            metadata=metadata,\n            outlinks=outlinks,\n            internal_links=internal_links,\n            external_links=external_links,\n            extraction_latency_ms=latency_ms,\n            success=True,\n        )\n\n    except Exception as e:\n        latency_ms = (time.time() - start_time) * 1000\n        return ExtractionResult(\n            markdown=\"\",\n            extraction_latency_ms=latency_ms,\n            success=False,\n            error=str(e),\n        )\n</code></pre>"},{"location":"reference/ragcrawl/extraction/extractor/#ragcrawl.extraction.extractor.ExtractionResult","title":"<code>ExtractionResult(markdown, html=None, plain_text=None, content_hash='', raw_hash=None, metadata=PageMetadata(), outlinks=list(), internal_links=list(), external_links=list(), extraction_latency_ms=0.0, success=True, error=None)</code>  <code>dataclass</code>","text":"<p>Result of content extraction.</p>"},{"location":"reference/ragcrawl/extraction/extractor/#ragcrawl.extraction.extractor.ExtractorProtocol","title":"<code>ExtractorProtocol</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for content extractors.</p>"},{"location":"reference/ragcrawl/extraction/extractor/#ragcrawl.extraction.extractor.ExtractorProtocol.extract","title":"<code>extract(fetch_result, url, extract_html=False, extract_plain_text=False)</code>","text":"<p>Extract content from a fetch result.</p> Source code in <code>src/ragcrawl/extraction/extractor.py</code> <pre><code>def extract(\n    self,\n    fetch_result: FetchResult,\n    url: str,\n    extract_html: bool = False,\n    extract_plain_text: bool = False,\n) -&gt; ExtractionResult:\n    \"\"\"Extract content from a fetch result.\"\"\"\n    ...\n</code></pre>"},{"location":"reference/ragcrawl/extraction/link_extractor/","title":"Link extractor","text":""},{"location":"reference/ragcrawl/extraction/link_extractor/#ragcrawl.extraction.link_extractor","title":"<code>link_extractor</code>","text":"<p>Link extraction from HTML content.</p>"},{"location":"reference/ragcrawl/extraction/link_extractor/#ragcrawl.extraction.link_extractor.ExtractedLink","title":"<code>ExtractedLink(href, text=None, is_internal=False, is_nofollow=False, anchor=None)</code>  <code>dataclass</code>","text":"<p>An extracted link with metadata.</p>"},{"location":"reference/ragcrawl/extraction/link_extractor/#ragcrawl.extraction.link_extractor.LinkExtractor","title":"<code>LinkExtractor(base_url, allowed_domains=None)</code>","text":"<p>Extracts and categorizes links from HTML content.</p> <p>Initialize link extractor.</p> <p>Parameters:</p> Name Type Description Default <code>base_url</code> <code>str</code> <p>Base URL for resolving relative links.</p> required <code>allowed_domains</code> <code>set[str] | None</code> <p>Domains to consider as internal.</p> <code>None</code> Source code in <code>src/ragcrawl/extraction/link_extractor.py</code> <pre><code>def __init__(\n    self,\n    base_url: str,\n    allowed_domains: set[str] | None = None,\n) -&gt; None:\n    \"\"\"\n    Initialize link extractor.\n\n    Args:\n        base_url: Base URL for resolving relative links.\n        allowed_domains: Domains to consider as internal.\n    \"\"\"\n    self.base_url = base_url\n    self.base_parsed = urlparse(base_url)\n    self.base_domain = self.base_parsed.netloc.lower()\n\n    self.allowed_domains = allowed_domains or {self.base_domain}\n</code></pre>"},{"location":"reference/ragcrawl/extraction/link_extractor/#ragcrawl.extraction.link_extractor.LinkExtractor.extract","title":"<code>extract(html)</code>","text":"<p>Extract all links from HTML.</p> <p>Parameters:</p> Name Type Description Default <code>html</code> <code>str</code> <p>HTML content.</p> required <p>Returns:</p> Type Description <code>list[ExtractedLink]</code> <p>List of extracted links.</p> Source code in <code>src/ragcrawl/extraction/link_extractor.py</code> <pre><code>def extract(self, html: str) -&gt; list[ExtractedLink]:\n    \"\"\"\n    Extract all links from HTML.\n\n    Args:\n        html: HTML content.\n\n    Returns:\n        List of extracted links.\n    \"\"\"\n    links: list[ExtractedLink] = []\n    seen_hrefs: set[str] = set()\n\n    # Match anchor tags\n    pattern = r\"&lt;a\\s+([^&gt;]*)&gt;(.*?)&lt;/a&gt;\"\n    for match in re.finditer(pattern, html, re.IGNORECASE | re.DOTALL):\n        attrs = match.group(1)\n        text = self._clean_text(match.group(2))\n\n        # Extract href\n        href_match = re.search(r'href=[\"\\']([^\"\\']+)[\"\\']', attrs)\n        if not href_match:\n            continue\n\n        href = href_match.group(1).strip()\n\n        # Skip javascript:, mailto:, tel:, etc.\n        if self._is_special_scheme(href):\n            continue\n\n        # Resolve relative URLs\n        resolved = self._resolve_url(href)\n        if not resolved:\n            continue\n\n        # Deduplicate\n        normalized = self._normalize_for_dedup(resolved)\n        if normalized in seen_hrefs:\n            continue\n        seen_hrefs.add(normalized)\n\n        # Check if nofollow\n        is_nofollow = \"nofollow\" in attrs.lower()\n\n        # Check if internal\n        is_internal = self._is_internal(resolved)\n\n        # Extract anchor\n        anchor = None\n        parsed = urlparse(resolved)\n        if parsed.fragment:\n            anchor = parsed.fragment\n\n        links.append(\n            ExtractedLink(\n                href=resolved,\n                text=text if text else None,\n                is_internal=is_internal,\n                is_nofollow=is_nofollow,\n                anchor=anchor,\n            )\n        )\n\n    return links\n</code></pre>"},{"location":"reference/ragcrawl/extraction/link_extractor/#ragcrawl.extraction.link_extractor.LinkExtractor.extract_external_urls","title":"<code>extract_external_urls(html)</code>","text":"<p>Extract only external URLs.</p> <p>Parameters:</p> Name Type Description Default <code>html</code> <code>str</code> <p>HTML content.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List of external URLs.</p> Source code in <code>src/ragcrawl/extraction/link_extractor.py</code> <pre><code>def extract_external_urls(self, html: str) -&gt; list[str]:\n    \"\"\"\n    Extract only external URLs.\n\n    Args:\n        html: HTML content.\n\n    Returns:\n        List of external URLs.\n    \"\"\"\n    return [link.href for link in self.extract(html) if not link.is_internal]\n</code></pre>"},{"location":"reference/ragcrawl/extraction/link_extractor/#ragcrawl.extraction.link_extractor.LinkExtractor.extract_internal_urls","title":"<code>extract_internal_urls(html)</code>","text":"<p>Extract only internal URLs.</p> <p>Parameters:</p> Name Type Description Default <code>html</code> <code>str</code> <p>HTML content.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List of internal URLs.</p> Source code in <code>src/ragcrawl/extraction/link_extractor.py</code> <pre><code>def extract_internal_urls(self, html: str) -&gt; list[str]:\n    \"\"\"\n    Extract only internal URLs.\n\n    Args:\n        html: HTML content.\n\n    Returns:\n        List of internal URLs.\n    \"\"\"\n    return [link.href for link in self.extract(html) if link.is_internal]\n</code></pre>"},{"location":"reference/ragcrawl/extraction/link_extractor/#ragcrawl.extraction.link_extractor.LinkExtractor.extract_urls","title":"<code>extract_urls(html)</code>","text":"<p>Extract just the URLs from HTML.</p> <p>Parameters:</p> Name Type Description Default <code>html</code> <code>str</code> <p>HTML content.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List of resolved URLs.</p> Source code in <code>src/ragcrawl/extraction/link_extractor.py</code> <pre><code>def extract_urls(self, html: str) -&gt; list[str]:\n    \"\"\"\n    Extract just the URLs from HTML.\n\n    Args:\n        html: HTML content.\n\n    Returns:\n        List of resolved URLs.\n    \"\"\"\n    return [link.href for link in self.extract(html)]\n</code></pre>"},{"location":"reference/ragcrawl/extraction/metadata/","title":"Metadata","text":""},{"location":"reference/ragcrawl/extraction/metadata/#ragcrawl.extraction.metadata","title":"<code>metadata</code>","text":"<p>Metadata extraction from HTML pages.</p>"},{"location":"reference/ragcrawl/extraction/metadata/#ragcrawl.extraction.metadata.HeadingInfo","title":"<code>HeadingInfo(level, text, anchor=None)</code>  <code>dataclass</code>","text":"<p>Information about a heading.</p>"},{"location":"reference/ragcrawl/extraction/metadata/#ragcrawl.extraction.metadata.MetadataExtractor","title":"<code>MetadataExtractor</code>","text":"<p>Extracts metadata from HTML pages.</p>"},{"location":"reference/ragcrawl/extraction/metadata/#ragcrawl.extraction.metadata.MetadataExtractor.extract","title":"<code>extract(html, text=None)</code>","text":"<p>Extract metadata from HTML.</p> <p>Parameters:</p> Name Type Description Default <code>html</code> <code>str</code> <p>HTML content.</p> required <code>text</code> <code>str | None</code> <p>Optional plain text for word/char counting.</p> <code>None</code> <p>Returns:</p> Type Description <code>PageMetadata</code> <p>PageMetadata with extracted values.</p> Source code in <code>src/ragcrawl/extraction/metadata.py</code> <pre><code>def extract(self, html: str, text: str | None = None) -&gt; PageMetadata:\n    \"\"\"\n    Extract metadata from HTML.\n\n    Args:\n        html: HTML content.\n        text: Optional plain text for word/char counting.\n\n    Returns:\n        PageMetadata with extracted values.\n    \"\"\"\n    metadata = PageMetadata()\n\n    # Title\n    metadata.title = self._extract_title(html)\n\n    # Meta tags\n    metadata.description = self._extract_meta(html, \"description\")\n    metadata.keywords = self._extract_keywords(html)\n    metadata.author = self._extract_meta(html, \"author\")\n    metadata.canonical_url = self._extract_canonical(html)\n    metadata.language = self._extract_language(html)\n\n    # Dates\n    metadata.published_date = self._extract_meta(\n        html, \"article:published_time\"\n    ) or self._extract_meta(html, \"datePublished\")\n    metadata.modified_date = self._extract_meta(\n        html, \"article:modified_time\"\n    ) or self._extract_meta(html, \"dateModified\")\n\n    # Open Graph\n    metadata.og_title = self._extract_meta(html, \"og:title\", property_attr=True)\n    metadata.og_description = self._extract_meta(html, \"og:description\", property_attr=True)\n    metadata.og_image = self._extract_meta(html, \"og:image\", property_attr=True)\n    metadata.og_type = self._extract_meta(html, \"og:type\", property_attr=True)\n\n    # Headings outline\n    metadata.headings_outline = self._extract_headings(html)\n\n    # Word/char count\n    if text:\n        metadata.word_count = len(text.split())\n        metadata.char_count = len(text)\n\n    return metadata\n</code></pre>"},{"location":"reference/ragcrawl/extraction/metadata/#ragcrawl.extraction.metadata.PageMetadata","title":"<code>PageMetadata(title=None, description=None, canonical_url=None, language=None, author=None, published_date=None, modified_date=None, keywords=list(), headings_outline=list(), og_title=None, og_description=None, og_image=None, og_type=None, word_count=0, char_count=0)</code>  <code>dataclass</code>","text":"<p>Extracted metadata from a page.</p>"},{"location":"reference/ragcrawl/fetcher/","title":"Index","text":""},{"location":"reference/ragcrawl/fetcher/#ragcrawl.fetcher","title":"<code>fetcher</code>","text":"<p>Fetcher module for ragcrawl.</p>"},{"location":"reference/ragcrawl/fetcher/#ragcrawl.fetcher.BaseFetcher","title":"<code>BaseFetcher</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for page fetchers.</p> <p>Fetchers are responsible for retrieving page content from URLs.</p>"},{"location":"reference/ragcrawl/fetcher/#ragcrawl.fetcher.BaseFetcher.close","title":"<code>close()</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Close any resources (browser, connections, etc.).</p> Source code in <code>src/ragcrawl/fetcher/base.py</code> <pre><code>@abstractmethod\nasync def close(self) -&gt; None:\n    \"\"\"Close any resources (browser, connections, etc.).\"\"\"\n    ...\n</code></pre>"},{"location":"reference/ragcrawl/fetcher/#ragcrawl.fetcher.BaseFetcher.fetch","title":"<code>fetch(url, etag=None, last_modified=None, **kwargs)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Fetch a URL and return the result.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL to fetch.</p> required <code>etag</code> <code>str | None</code> <p>Optional ETag for conditional request.</p> <code>None</code> <code>last_modified</code> <code>str | None</code> <p>Optional Last-Modified for conditional request.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional fetcher-specific options.</p> <code>{}</code> <p>Returns:</p> Type Description <code>FetchResult</code> <p>FetchResult with content and metadata.</p> Source code in <code>src/ragcrawl/fetcher/base.py</code> <pre><code>@abstractmethod\nasync def fetch(\n    self,\n    url: str,\n    etag: str | None = None,\n    last_modified: str | None = None,\n    **kwargs: Any,\n) -&gt; FetchResult:\n    \"\"\"\n    Fetch a URL and return the result.\n\n    Args:\n        url: The URL to fetch.\n        etag: Optional ETag for conditional request.\n        last_modified: Optional Last-Modified for conditional request.\n        **kwargs: Additional fetcher-specific options.\n\n    Returns:\n        FetchResult with content and metadata.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/ragcrawl/fetcher/#ragcrawl.fetcher.BaseFetcher.fetch_batch","title":"<code>fetch_batch(urls, **kwargs)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Fetch multiple URLs concurrently.</p> <p>Parameters:</p> Name Type Description Default <code>urls</code> <code>list[str]</code> <p>List of URLs to fetch.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional fetcher-specific options.</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[FetchResult]</code> <p>List of FetchResults in the same order as input URLs.</p> Source code in <code>src/ragcrawl/fetcher/base.py</code> <pre><code>@abstractmethod\nasync def fetch_batch(\n    self,\n    urls: list[str],\n    **kwargs: Any,\n) -&gt; list[FetchResult]:\n    \"\"\"\n    Fetch multiple URLs concurrently.\n\n    Args:\n        urls: List of URLs to fetch.\n        **kwargs: Additional fetcher-specific options.\n\n    Returns:\n        List of FetchResults in the same order as input URLs.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/ragcrawl/fetcher/#ragcrawl.fetcher.BaseFetcher.health_check","title":"<code>health_check()</code>  <code>abstractmethod</code>","text":"<p>Check if the fetcher is ready.</p> Source code in <code>src/ragcrawl/fetcher/base.py</code> <pre><code>@abstractmethod\ndef health_check(self) -&gt; bool:\n    \"\"\"Check if the fetcher is ready.\"\"\"\n    ...\n</code></pre>"},{"location":"reference/ragcrawl/fetcher/#ragcrawl.fetcher.Crawl4AIFetcher","title":"<code>Crawl4AIFetcher(fetch_mode=FetchMode.HTTP, user_agent='ragcrawl/0.1', timeout=30, browser_timeout=30000, headers=None, cookies=None, proxy=None, retry_config=None, follow_redirects=True, max_redirects=10)</code>","text":"<p>               Bases: <code>BaseFetcher</code></p> <p>Fetcher implementation using Crawl4AI.</p> <p>Supports HTTP-only, browser rendering, and hybrid modes.</p> <p>Initialize Crawl4AI fetcher.</p> <p>Parameters:</p> Name Type Description Default <code>fetch_mode</code> <code>FetchMode</code> <p>HTTP, browser, or hybrid mode.</p> <code>HTTP</code> <code>user_agent</code> <code>str</code> <p>User agent string.</p> <code>'ragcrawl/0.1'</code> <code>timeout</code> <code>int</code> <p>HTTP timeout in seconds.</p> <code>30</code> <code>browser_timeout</code> <code>int</code> <p>Browser timeout in milliseconds.</p> <code>30000</code> <code>headers</code> <code>dict[str, str] | None</code> <p>Additional HTTP headers.</p> <code>None</code> <code>cookies</code> <code>dict[str, str] | None</code> <p>Cookies to send.</p> <code>None</code> <code>proxy</code> <code>str | None</code> <p>Proxy URL.</p> <code>None</code> <code>retry_config</code> <code>RetryConfig | None</code> <p>Retry configuration.</p> <code>None</code> <code>follow_redirects</code> <code>bool</code> <p>Whether to follow redirects.</p> <code>True</code> <code>max_redirects</code> <code>int</code> <p>Maximum redirects to follow.</p> <code>10</code> Source code in <code>src/ragcrawl/fetcher/crawl4ai_fetcher.py</code> <pre><code>def __init__(\n    self,\n    fetch_mode: FetchMode = FetchMode.HTTP,\n    user_agent: str = \"ragcrawl/0.1\",\n    timeout: int = 30,\n    browser_timeout: int = 30000,\n    headers: dict[str, str] | None = None,\n    cookies: dict[str, str] | None = None,\n    proxy: str | None = None,\n    retry_config: RetryConfig | None = None,\n    follow_redirects: bool = True,\n    max_redirects: int = 10,\n) -&gt; None:\n    \"\"\"\n    Initialize Crawl4AI fetcher.\n\n    Args:\n        fetch_mode: HTTP, browser, or hybrid mode.\n        user_agent: User agent string.\n        timeout: HTTP timeout in seconds.\n        browser_timeout: Browser timeout in milliseconds.\n        headers: Additional HTTP headers.\n        cookies: Cookies to send.\n        proxy: Proxy URL.\n        retry_config: Retry configuration.\n        follow_redirects: Whether to follow redirects.\n        max_redirects: Maximum redirects to follow.\n    \"\"\"\n    self.fetch_mode = fetch_mode\n    self.user_agent = user_agent\n    self.timeout = timeout\n    self.browser_timeout = browser_timeout\n    self.headers = headers or {}\n    self.cookies = cookies or {}\n    self.proxy = proxy\n    self.retry_config = retry_config or RetryConfig()\n    self.follow_redirects = follow_redirects\n    self.max_redirects = max_redirects\n\n    self.revalidator = Revalidator()\n    self._crawler: Any = None\n    self._initialized = False\n</code></pre>"},{"location":"reference/ragcrawl/fetcher/#ragcrawl.fetcher.Crawl4AIFetcher.close","title":"<code>close()</code>  <code>async</code>","text":"<p>Close Crawl4AI resources.</p> Source code in <code>src/ragcrawl/fetcher/crawl4ai_fetcher.py</code> <pre><code>async def close(self) -&gt; None:\n    \"\"\"Close Crawl4AI resources.\"\"\"\n    if self._crawler is not None:\n        try:\n            await self._crawler.aclose()\n        except Exception:\n            pass\n        self._crawler = None\n        self._initialized = False\n</code></pre>"},{"location":"reference/ragcrawl/fetcher/#ragcrawl.fetcher.Crawl4AIFetcher.fetch","title":"<code>fetch(url, etag=None, last_modified=None, **kwargs)</code>  <code>async</code>","text":"<p>Fetch a URL using Crawl4AI.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>URL to fetch.</p> required <code>etag</code> <code>str | None</code> <p>Optional ETag for conditional request.</p> <code>None</code> <code>last_modified</code> <code>str | None</code> <p>Optional Last-Modified for conditional request.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional options.</p> <code>{}</code> <p>Returns:</p> Type Description <code>FetchResult</code> <p>FetchResult with content and metadata.</p> Source code in <code>src/ragcrawl/fetcher/crawl4ai_fetcher.py</code> <pre><code>async def fetch(\n    self,\n    url: str,\n    etag: str | None = None,\n    last_modified: str | None = None,\n    **kwargs: Any,\n) -&gt; FetchResult:\n    \"\"\"\n    Fetch a URL using Crawl4AI.\n\n    Args:\n        url: URL to fetch.\n        etag: Optional ETag for conditional request.\n        last_modified: Optional Last-Modified for conditional request.\n        **kwargs: Additional options.\n\n    Returns:\n        FetchResult with content and metadata.\n    \"\"\"\n    await self._ensure_initialized()\n\n    start_time = time.time()\n    fetch_started = datetime.now()\n\n    try:\n        # Try HTTP mode first if hybrid or HTTP\n        if self.fetch_mode in (FetchMode.HTTP, FetchMode.HYBRID):\n            result = await self._fetch_http(url, etag, last_modified)\n\n            # If hybrid and content looks incomplete, try browser\n            if (\n                self.fetch_mode == FetchMode.HYBRID\n                and result.is_success\n                and self._needs_browser_rendering(result)\n            ):\n                logger.debug(\"Falling back to browser rendering\", url=url)\n                result = await self._fetch_browser(url)\n                result.used_browser = True\n\n        else:  # Browser mode\n            result = await self._fetch_browser(url)\n            result.used_browser = True\n\n        # Set timing\n        result.fetch_started_at = fetch_started\n        result.fetch_completed_at = datetime.now()\n        result.latency_ms = (time.time() - start_time) * 1000\n\n        return result\n\n    except Exception as e:\n        logger.error(\"Fetch failed\", url=url, error=str(e))\n        return FetchResult(\n            status=FetchStatus.ERROR,\n            error=str(e),\n            fetch_started_at=fetch_started,\n            fetch_completed_at=datetime.now(),\n            latency_ms=(time.time() - start_time) * 1000,\n        )\n</code></pre>"},{"location":"reference/ragcrawl/fetcher/#ragcrawl.fetcher.Crawl4AIFetcher.fetch_batch","title":"<code>fetch_batch(urls, **kwargs)</code>  <code>async</code>","text":"<p>Fetch multiple URLs concurrently.</p> Source code in <code>src/ragcrawl/fetcher/crawl4ai_fetcher.py</code> <pre><code>async def fetch_batch(\n    self,\n    urls: list[str],\n    **kwargs: Any,\n) -&gt; list[FetchResult]:\n    \"\"\"Fetch multiple URLs concurrently.\"\"\"\n    tasks = [self.fetch(url, **kwargs) for url in urls]\n    return await asyncio.gather(*tasks)\n</code></pre>"},{"location":"reference/ragcrawl/fetcher/#ragcrawl.fetcher.Crawl4AIFetcher.health_check","title":"<code>health_check()</code>","text":"<p>Check if fetcher is ready.</p> Source code in <code>src/ragcrawl/fetcher/crawl4ai_fetcher.py</code> <pre><code>def health_check(self) -&gt; bool:\n    \"\"\"Check if fetcher is ready.\"\"\"\n    return True  # HTTP always available\n</code></pre>"},{"location":"reference/ragcrawl/fetcher/#ragcrawl.fetcher.FetchResult","title":"<code>FetchResult(status, status_code=None, error=None, html=None, markdown=None, plain_text=None, content_type=None, content_length=None, encoding=None, etag=None, last_modified=None, final_url=None, canonical_url=None, fetch_started_at=None, fetch_completed_at=None, latency_ms=0.0, headers=dict(), title=None, description=None, links=list(), used_browser=False)</code>  <code>dataclass</code>","text":"<p>Result of a fetch operation.</p>"},{"location":"reference/ragcrawl/fetcher/#ragcrawl.fetcher.FetchResult.is_client_error","title":"<code>is_client_error</code>  <code>property</code>","text":"<p>Check if response is a client error (4xx).</p>"},{"location":"reference/ragcrawl/fetcher/#ragcrawl.fetcher.FetchResult.is_error","title":"<code>is_error</code>  <code>property</code>","text":"<p>Check if fetch resulted in error.</p>"},{"location":"reference/ragcrawl/fetcher/#ragcrawl.fetcher.FetchResult.is_not_found","title":"<code>is_not_found</code>  <code>property</code>","text":"<p>Check if page was not found (404/410).</p>"},{"location":"reference/ragcrawl/fetcher/#ragcrawl.fetcher.FetchResult.is_not_modified","title":"<code>is_not_modified</code>  <code>property</code>","text":"<p>Check if content was not modified (304).</p>"},{"location":"reference/ragcrawl/fetcher/#ragcrawl.fetcher.FetchResult.is_redirect","title":"<code>is_redirect</code>  <code>property</code>","text":"<p>Check if response was a redirect.</p>"},{"location":"reference/ragcrawl/fetcher/#ragcrawl.fetcher.FetchResult.is_server_error","title":"<code>is_server_error</code>  <code>property</code>","text":"<p>Check if response is a server error (5xx).</p>"},{"location":"reference/ragcrawl/fetcher/#ragcrawl.fetcher.FetchResult.is_success","title":"<code>is_success</code>  <code>property</code>","text":"<p>Check if fetch was successful.</p>"},{"location":"reference/ragcrawl/fetcher/#ragcrawl.fetcher.RevalidationResult","title":"<code>RevalidationResult(status, etag=None, last_modified=None, error=None)</code>  <code>dataclass</code>","text":"<p>Result of a revalidation check.</p>"},{"location":"reference/ragcrawl/fetcher/#ragcrawl.fetcher.RevalidationResult.needs_fetch","title":"<code>needs_fetch</code>  <code>property</code>","text":"<p>Check if full fetch is needed.</p>"},{"location":"reference/ragcrawl/fetcher/#ragcrawl.fetcher.Revalidator","title":"<code>Revalidator(use_etag=True, use_last_modified=True)</code>","text":"<p>Handles HTTP conditional requests for change detection.</p> <p>Uses ETag and Last-Modified headers to efficiently detect content changes without downloading full content.</p> <p>Initialize revalidator.</p> <p>Parameters:</p> Name Type Description Default <code>use_etag</code> <code>bool</code> <p>Whether to use ETag for validation.</p> <code>True</code> <code>use_last_modified</code> <code>bool</code> <p>Whether to use Last-Modified.</p> <code>True</code> Source code in <code>src/ragcrawl/fetcher/revalidation.py</code> <pre><code>def __init__(\n    self,\n    use_etag: bool = True,\n    use_last_modified: bool = True,\n) -&gt; None:\n    \"\"\"\n    Initialize revalidator.\n\n    Args:\n        use_etag: Whether to use ETag for validation.\n        use_last_modified: Whether to use Last-Modified.\n    \"\"\"\n    self.use_etag = use_etag\n    self.use_last_modified = use_last_modified\n</code></pre>"},{"location":"reference/ragcrawl/fetcher/#ragcrawl.fetcher.Revalidator.get_conditional_headers","title":"<code>get_conditional_headers(etag=None, last_modified=None)</code>","text":"<p>Get headers for conditional request.</p> <p>Parameters:</p> Name Type Description Default <code>etag</code> <code>str | None</code> <p>Stored ETag value.</p> <code>None</code> <code>last_modified</code> <code>str | None</code> <p>Stored Last-Modified value.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, str]</code> <p>Dict of conditional request headers.</p> Source code in <code>src/ragcrawl/fetcher/revalidation.py</code> <pre><code>def get_conditional_headers(\n    self,\n    etag: str | None = None,\n    last_modified: str | None = None,\n) -&gt; dict[str, str]:\n    \"\"\"\n    Get headers for conditional request.\n\n    Args:\n        etag: Stored ETag value.\n        last_modified: Stored Last-Modified value.\n\n    Returns:\n        Dict of conditional request headers.\n    \"\"\"\n    headers: dict[str, str] = {}\n\n    if self.use_etag and etag:\n        headers[\"If-None-Match\"] = etag\n\n    if self.use_last_modified and last_modified:\n        headers[\"If-Modified-Since\"] = last_modified\n\n    return headers\n</code></pre>"},{"location":"reference/ragcrawl/fetcher/#ragcrawl.fetcher.Revalidator.has_validators","title":"<code>has_validators(etag=None, last_modified=None)</code>","text":"<p>Check if we have validators for conditional requests.</p> <p>Parameters:</p> Name Type Description Default <code>etag</code> <code>str | None</code> <p>Stored ETag.</p> <code>None</code> <code>last_modified</code> <code>str | None</code> <p>Stored Last-Modified.</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if we can make conditional requests.</p> Source code in <code>src/ragcrawl/fetcher/revalidation.py</code> <pre><code>def has_validators(\n    self,\n    etag: str | None = None,\n    last_modified: str | None = None,\n) -&gt; bool:\n    \"\"\"\n    Check if we have validators for conditional requests.\n\n    Args:\n        etag: Stored ETag.\n        last_modified: Stored Last-Modified.\n\n    Returns:\n        True if we can make conditional requests.\n    \"\"\"\n    if self.use_etag and etag:\n        return True\n    if self.use_last_modified and last_modified:\n        return True\n    return False\n</code></pre>"},{"location":"reference/ragcrawl/fetcher/#ragcrawl.fetcher.Revalidator.parse_response","title":"<code>parse_response(status_code, headers)</code>","text":"<p>Parse response to determine if content changed.</p> <p>Parameters:</p> Name Type Description Default <code>status_code</code> <code>int</code> <p>HTTP status code.</p> required <code>headers</code> <code>dict[str, str]</code> <p>Response headers.</p> required <p>Returns:</p> Type Description <code>RevalidationResult</code> <p>RevalidationResult.</p> Source code in <code>src/ragcrawl/fetcher/revalidation.py</code> <pre><code>def parse_response(\n    self,\n    status_code: int,\n    headers: dict[str, str],\n) -&gt; RevalidationResult:\n    \"\"\"\n    Parse response to determine if content changed.\n\n    Args:\n        status_code: HTTP status code.\n        headers: Response headers.\n\n    Returns:\n        RevalidationResult.\n    \"\"\"\n    # Extract caching headers (case-insensitive)\n    headers_lower = {k.lower(): v for k, v in headers.items()}\n    etag = headers_lower.get(\"etag\")\n    last_modified = headers_lower.get(\"last-modified\")\n\n    if status_code == 304:\n        return RevalidationResult(\n            status=RevalidationStatus.NOT_MODIFIED,\n            etag=etag,\n            last_modified=last_modified,\n        )\n\n    if status_code &gt;= 200 and status_code &lt; 300:\n        return RevalidationResult(\n            status=RevalidationStatus.MODIFIED,\n            etag=etag,\n            last_modified=last_modified,\n        )\n\n    if status_code &gt;= 400:\n        return RevalidationResult(\n            status=RevalidationStatus.ERROR,\n            error=f\"HTTP {status_code}\",\n        )\n\n    return RevalidationResult(\n        status=RevalidationStatus.MODIFIED,\n        etag=etag,\n        last_modified=last_modified,\n    )\n</code></pre>"},{"location":"reference/ragcrawl/fetcher/#ragcrawl.fetcher.RobotsChecker","title":"<code>RobotsChecker(mode=RobotsMode.STRICT, user_agent='ragcrawl', allowlist=None, cache_ttl_seconds=3600)</code>","text":"<p>Checks URL access against robots.txt rules.</p> <p>Initialize robots checker.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>RobotsMode</code> <p>Robots.txt compliance mode.</p> <code>STRICT</code> <code>user_agent</code> <code>str</code> <p>User agent to check rules for.</p> <code>'ragcrawl'</code> <code>allowlist</code> <code>list[str] | None</code> <p>Domains to bypass robots.txt (when mode=ALLOWLIST).</p> <code>None</code> <code>cache_ttl_seconds</code> <code>int</code> <p>How long to cache robots.txt files.</p> <code>3600</code> Source code in <code>src/ragcrawl/fetcher/robots.py</code> <pre><code>def __init__(\n    self,\n    mode: RobotsMode = RobotsMode.STRICT,\n    user_agent: str = \"ragcrawl\",\n    allowlist: list[str] | None = None,\n    cache_ttl_seconds: int = 3600,\n) -&gt; None:\n    \"\"\"\n    Initialize robots checker.\n\n    Args:\n        mode: Robots.txt compliance mode.\n        user_agent: User agent to check rules for.\n        allowlist: Domains to bypass robots.txt (when mode=ALLOWLIST).\n        cache_ttl_seconds: How long to cache robots.txt files.\n    \"\"\"\n    self.mode = mode\n    self.user_agent = user_agent\n    self.allowlist = set(d.lower() for d in (allowlist or []))\n    self.cache_ttl_seconds = cache_ttl_seconds\n\n    # Cache: domain -&gt; (parser, timestamp)\n    self._cache: dict[str, tuple[RobotExclusionRulesParser | None, float]] = {}\n    self._lock = asyncio.Lock()\n</code></pre>"},{"location":"reference/ragcrawl/fetcher/#ragcrawl.fetcher.RobotsChecker.clear_cache","title":"<code>clear_cache()</code>","text":"<p>Clear the robots.txt cache.</p> Source code in <code>src/ragcrawl/fetcher/robots.py</code> <pre><code>def clear_cache(self) -&gt; None:\n    \"\"\"Clear the robots.txt cache.\"\"\"\n    self._cache.clear()\n</code></pre>"},{"location":"reference/ragcrawl/fetcher/#ragcrawl.fetcher.RobotsChecker.get_crawl_delay","title":"<code>get_crawl_delay(url)</code>","text":"<p>Get crawl delay from robots.txt.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>URL to check.</p> required <p>Returns:</p> Type Description <code>float | None</code> <p>Crawl delay in seconds, or None if not specified.</p> Source code in <code>src/ragcrawl/fetcher/robots.py</code> <pre><code>def get_crawl_delay(self, url: str) -&gt; float | None:\n    \"\"\"\n    Get crawl delay from robots.txt.\n\n    Args:\n        url: URL to check.\n\n    Returns:\n        Crawl delay in seconds, or None if not specified.\n    \"\"\"\n    domain = self._get_domain(url)\n\n    if domain in self._cache:\n        parser, _ = self._cache[domain]\n        if parser:\n            delay = parser.get_crawl_delay(self.user_agent)\n            if delay:\n                return float(delay)\n\n    return None\n</code></pre>"},{"location":"reference/ragcrawl/fetcher/#ragcrawl.fetcher.RobotsChecker.get_sitemaps","title":"<code>get_sitemaps(url)</code>","text":"<p>Get sitemap URLs from robots.txt.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>URL to check.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List of sitemap URLs.</p> Source code in <code>src/ragcrawl/fetcher/robots.py</code> <pre><code>def get_sitemaps(self, url: str) -&gt; list[str]:\n    \"\"\"\n    Get sitemap URLs from robots.txt.\n\n    Args:\n        url: URL to check.\n\n    Returns:\n        List of sitemap URLs.\n    \"\"\"\n    domain = self._get_domain(url)\n\n    if domain in self._cache:\n        parser, _ = self._cache[domain]\n        if parser and hasattr(parser, \"sitemaps\"):\n            return list(parser.sitemaps)\n\n    return []\n</code></pre>"},{"location":"reference/ragcrawl/fetcher/#ragcrawl.fetcher.RobotsChecker.is_allowed","title":"<code>is_allowed(url)</code>  <code>async</code>","text":"<p>Check if URL is allowed by robots.txt.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if crawling is allowed.</p> Source code in <code>src/ragcrawl/fetcher/robots.py</code> <pre><code>async def is_allowed(self, url: str) -&gt; bool:\n    \"\"\"\n    Check if URL is allowed by robots.txt.\n\n    Args:\n        url: The URL to check.\n\n    Returns:\n        True if crawling is allowed.\n    \"\"\"\n    if self.mode == RobotsMode.OFF:\n        return True\n\n    domain = self._get_domain(url)\n\n    if self.mode == RobotsMode.ALLOWLIST:\n        if domain in self.allowlist:\n            return True\n\n    if self.mode == RobotsMode.OFF:\n        return True\n\n    # Get or fetch robots.txt\n    parser = await self._get_parser(url)\n\n    if parser is None:\n        # If we can't fetch robots.txt, allow by default\n        return True\n\n    return parser.is_allowed(self.user_agent, url)\n</code></pre>"},{"location":"reference/ragcrawl/fetcher/base/","title":"Base","text":""},{"location":"reference/ragcrawl/fetcher/base/#ragcrawl.fetcher.base","title":"<code>base</code>","text":"<p>Base fetcher protocol and result types.</p>"},{"location":"reference/ragcrawl/fetcher/base/#ragcrawl.fetcher.base.BaseFetcher","title":"<code>BaseFetcher</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for page fetchers.</p> <p>Fetchers are responsible for retrieving page content from URLs.</p>"},{"location":"reference/ragcrawl/fetcher/base/#ragcrawl.fetcher.base.BaseFetcher.close","title":"<code>close()</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Close any resources (browser, connections, etc.).</p> Source code in <code>src/ragcrawl/fetcher/base.py</code> <pre><code>@abstractmethod\nasync def close(self) -&gt; None:\n    \"\"\"Close any resources (browser, connections, etc.).\"\"\"\n    ...\n</code></pre>"},{"location":"reference/ragcrawl/fetcher/base/#ragcrawl.fetcher.base.BaseFetcher.fetch","title":"<code>fetch(url, etag=None, last_modified=None, **kwargs)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Fetch a URL and return the result.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL to fetch.</p> required <code>etag</code> <code>str | None</code> <p>Optional ETag for conditional request.</p> <code>None</code> <code>last_modified</code> <code>str | None</code> <p>Optional Last-Modified for conditional request.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional fetcher-specific options.</p> <code>{}</code> <p>Returns:</p> Type Description <code>FetchResult</code> <p>FetchResult with content and metadata.</p> Source code in <code>src/ragcrawl/fetcher/base.py</code> <pre><code>@abstractmethod\nasync def fetch(\n    self,\n    url: str,\n    etag: str | None = None,\n    last_modified: str | None = None,\n    **kwargs: Any,\n) -&gt; FetchResult:\n    \"\"\"\n    Fetch a URL and return the result.\n\n    Args:\n        url: The URL to fetch.\n        etag: Optional ETag for conditional request.\n        last_modified: Optional Last-Modified for conditional request.\n        **kwargs: Additional fetcher-specific options.\n\n    Returns:\n        FetchResult with content and metadata.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/ragcrawl/fetcher/base/#ragcrawl.fetcher.base.BaseFetcher.fetch_batch","title":"<code>fetch_batch(urls, **kwargs)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Fetch multiple URLs concurrently.</p> <p>Parameters:</p> Name Type Description Default <code>urls</code> <code>list[str]</code> <p>List of URLs to fetch.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional fetcher-specific options.</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[FetchResult]</code> <p>List of FetchResults in the same order as input URLs.</p> Source code in <code>src/ragcrawl/fetcher/base.py</code> <pre><code>@abstractmethod\nasync def fetch_batch(\n    self,\n    urls: list[str],\n    **kwargs: Any,\n) -&gt; list[FetchResult]:\n    \"\"\"\n    Fetch multiple URLs concurrently.\n\n    Args:\n        urls: List of URLs to fetch.\n        **kwargs: Additional fetcher-specific options.\n\n    Returns:\n        List of FetchResults in the same order as input URLs.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/ragcrawl/fetcher/base/#ragcrawl.fetcher.base.BaseFetcher.health_check","title":"<code>health_check()</code>  <code>abstractmethod</code>","text":"<p>Check if the fetcher is ready.</p> Source code in <code>src/ragcrawl/fetcher/base.py</code> <pre><code>@abstractmethod\ndef health_check(self) -&gt; bool:\n    \"\"\"Check if the fetcher is ready.\"\"\"\n    ...\n</code></pre>"},{"location":"reference/ragcrawl/fetcher/base/#ragcrawl.fetcher.base.FetchResult","title":"<code>FetchResult(status, status_code=None, error=None, html=None, markdown=None, plain_text=None, content_type=None, content_length=None, encoding=None, etag=None, last_modified=None, final_url=None, canonical_url=None, fetch_started_at=None, fetch_completed_at=None, latency_ms=0.0, headers=dict(), title=None, description=None, links=list(), used_browser=False)</code>  <code>dataclass</code>","text":"<p>Result of a fetch operation.</p>"},{"location":"reference/ragcrawl/fetcher/base/#ragcrawl.fetcher.base.FetchResult.is_client_error","title":"<code>is_client_error</code>  <code>property</code>","text":"<p>Check if response is a client error (4xx).</p>"},{"location":"reference/ragcrawl/fetcher/base/#ragcrawl.fetcher.base.FetchResult.is_error","title":"<code>is_error</code>  <code>property</code>","text":"<p>Check if fetch resulted in error.</p>"},{"location":"reference/ragcrawl/fetcher/base/#ragcrawl.fetcher.base.FetchResult.is_not_found","title":"<code>is_not_found</code>  <code>property</code>","text":"<p>Check if page was not found (404/410).</p>"},{"location":"reference/ragcrawl/fetcher/base/#ragcrawl.fetcher.base.FetchResult.is_not_modified","title":"<code>is_not_modified</code>  <code>property</code>","text":"<p>Check if content was not modified (304).</p>"},{"location":"reference/ragcrawl/fetcher/base/#ragcrawl.fetcher.base.FetchResult.is_redirect","title":"<code>is_redirect</code>  <code>property</code>","text":"<p>Check if response was a redirect.</p>"},{"location":"reference/ragcrawl/fetcher/base/#ragcrawl.fetcher.base.FetchResult.is_server_error","title":"<code>is_server_error</code>  <code>property</code>","text":"<p>Check if response is a server error (5xx).</p>"},{"location":"reference/ragcrawl/fetcher/base/#ragcrawl.fetcher.base.FetchResult.is_success","title":"<code>is_success</code>  <code>property</code>","text":"<p>Check if fetch was successful.</p>"},{"location":"reference/ragcrawl/fetcher/base/#ragcrawl.fetcher.base.FetchStatus","title":"<code>FetchStatus</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Status of a fetch operation.</p>"},{"location":"reference/ragcrawl/fetcher/crawl4ai_fetcher/","title":"Crawl4ai fetcher","text":""},{"location":"reference/ragcrawl/fetcher/crawl4ai_fetcher/#ragcrawl.fetcher.crawl4ai_fetcher","title":"<code>crawl4ai_fetcher</code>","text":"<p>Crawl4AI-based fetcher implementation.</p>"},{"location":"reference/ragcrawl/fetcher/crawl4ai_fetcher/#ragcrawl.fetcher.crawl4ai_fetcher.Crawl4AIFetcher","title":"<code>Crawl4AIFetcher(fetch_mode=FetchMode.HTTP, user_agent='ragcrawl/0.1', timeout=30, browser_timeout=30000, headers=None, cookies=None, proxy=None, retry_config=None, follow_redirects=True, max_redirects=10)</code>","text":"<p>               Bases: <code>BaseFetcher</code></p> <p>Fetcher implementation using Crawl4AI.</p> <p>Supports HTTP-only, browser rendering, and hybrid modes.</p> <p>Initialize Crawl4AI fetcher.</p> <p>Parameters:</p> Name Type Description Default <code>fetch_mode</code> <code>FetchMode</code> <p>HTTP, browser, or hybrid mode.</p> <code>HTTP</code> <code>user_agent</code> <code>str</code> <p>User agent string.</p> <code>'ragcrawl/0.1'</code> <code>timeout</code> <code>int</code> <p>HTTP timeout in seconds.</p> <code>30</code> <code>browser_timeout</code> <code>int</code> <p>Browser timeout in milliseconds.</p> <code>30000</code> <code>headers</code> <code>dict[str, str] | None</code> <p>Additional HTTP headers.</p> <code>None</code> <code>cookies</code> <code>dict[str, str] | None</code> <p>Cookies to send.</p> <code>None</code> <code>proxy</code> <code>str | None</code> <p>Proxy URL.</p> <code>None</code> <code>retry_config</code> <code>RetryConfig | None</code> <p>Retry configuration.</p> <code>None</code> <code>follow_redirects</code> <code>bool</code> <p>Whether to follow redirects.</p> <code>True</code> <code>max_redirects</code> <code>int</code> <p>Maximum redirects to follow.</p> <code>10</code> Source code in <code>src/ragcrawl/fetcher/crawl4ai_fetcher.py</code> <pre><code>def __init__(\n    self,\n    fetch_mode: FetchMode = FetchMode.HTTP,\n    user_agent: str = \"ragcrawl/0.1\",\n    timeout: int = 30,\n    browser_timeout: int = 30000,\n    headers: dict[str, str] | None = None,\n    cookies: dict[str, str] | None = None,\n    proxy: str | None = None,\n    retry_config: RetryConfig | None = None,\n    follow_redirects: bool = True,\n    max_redirects: int = 10,\n) -&gt; None:\n    \"\"\"\n    Initialize Crawl4AI fetcher.\n\n    Args:\n        fetch_mode: HTTP, browser, or hybrid mode.\n        user_agent: User agent string.\n        timeout: HTTP timeout in seconds.\n        browser_timeout: Browser timeout in milliseconds.\n        headers: Additional HTTP headers.\n        cookies: Cookies to send.\n        proxy: Proxy URL.\n        retry_config: Retry configuration.\n        follow_redirects: Whether to follow redirects.\n        max_redirects: Maximum redirects to follow.\n    \"\"\"\n    self.fetch_mode = fetch_mode\n    self.user_agent = user_agent\n    self.timeout = timeout\n    self.browser_timeout = browser_timeout\n    self.headers = headers or {}\n    self.cookies = cookies or {}\n    self.proxy = proxy\n    self.retry_config = retry_config or RetryConfig()\n    self.follow_redirects = follow_redirects\n    self.max_redirects = max_redirects\n\n    self.revalidator = Revalidator()\n    self._crawler: Any = None\n    self._initialized = False\n</code></pre>"},{"location":"reference/ragcrawl/fetcher/crawl4ai_fetcher/#ragcrawl.fetcher.crawl4ai_fetcher.Crawl4AIFetcher.close","title":"<code>close()</code>  <code>async</code>","text":"<p>Close Crawl4AI resources.</p> Source code in <code>src/ragcrawl/fetcher/crawl4ai_fetcher.py</code> <pre><code>async def close(self) -&gt; None:\n    \"\"\"Close Crawl4AI resources.\"\"\"\n    if self._crawler is not None:\n        try:\n            await self._crawler.aclose()\n        except Exception:\n            pass\n        self._crawler = None\n        self._initialized = False\n</code></pre>"},{"location":"reference/ragcrawl/fetcher/crawl4ai_fetcher/#ragcrawl.fetcher.crawl4ai_fetcher.Crawl4AIFetcher.fetch","title":"<code>fetch(url, etag=None, last_modified=None, **kwargs)</code>  <code>async</code>","text":"<p>Fetch a URL using Crawl4AI.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>URL to fetch.</p> required <code>etag</code> <code>str | None</code> <p>Optional ETag for conditional request.</p> <code>None</code> <code>last_modified</code> <code>str | None</code> <p>Optional Last-Modified for conditional request.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional options.</p> <code>{}</code> <p>Returns:</p> Type Description <code>FetchResult</code> <p>FetchResult with content and metadata.</p> Source code in <code>src/ragcrawl/fetcher/crawl4ai_fetcher.py</code> <pre><code>async def fetch(\n    self,\n    url: str,\n    etag: str | None = None,\n    last_modified: str | None = None,\n    **kwargs: Any,\n) -&gt; FetchResult:\n    \"\"\"\n    Fetch a URL using Crawl4AI.\n\n    Args:\n        url: URL to fetch.\n        etag: Optional ETag for conditional request.\n        last_modified: Optional Last-Modified for conditional request.\n        **kwargs: Additional options.\n\n    Returns:\n        FetchResult with content and metadata.\n    \"\"\"\n    await self._ensure_initialized()\n\n    start_time = time.time()\n    fetch_started = datetime.now()\n\n    try:\n        # Try HTTP mode first if hybrid or HTTP\n        if self.fetch_mode in (FetchMode.HTTP, FetchMode.HYBRID):\n            result = await self._fetch_http(url, etag, last_modified)\n\n            # If hybrid and content looks incomplete, try browser\n            if (\n                self.fetch_mode == FetchMode.HYBRID\n                and result.is_success\n                and self._needs_browser_rendering(result)\n            ):\n                logger.debug(\"Falling back to browser rendering\", url=url)\n                result = await self._fetch_browser(url)\n                result.used_browser = True\n\n        else:  # Browser mode\n            result = await self._fetch_browser(url)\n            result.used_browser = True\n\n        # Set timing\n        result.fetch_started_at = fetch_started\n        result.fetch_completed_at = datetime.now()\n        result.latency_ms = (time.time() - start_time) * 1000\n\n        return result\n\n    except Exception as e:\n        logger.error(\"Fetch failed\", url=url, error=str(e))\n        return FetchResult(\n            status=FetchStatus.ERROR,\n            error=str(e),\n            fetch_started_at=fetch_started,\n            fetch_completed_at=datetime.now(),\n            latency_ms=(time.time() - start_time) * 1000,\n        )\n</code></pre>"},{"location":"reference/ragcrawl/fetcher/crawl4ai_fetcher/#ragcrawl.fetcher.crawl4ai_fetcher.Crawl4AIFetcher.fetch_batch","title":"<code>fetch_batch(urls, **kwargs)</code>  <code>async</code>","text":"<p>Fetch multiple URLs concurrently.</p> Source code in <code>src/ragcrawl/fetcher/crawl4ai_fetcher.py</code> <pre><code>async def fetch_batch(\n    self,\n    urls: list[str],\n    **kwargs: Any,\n) -&gt; list[FetchResult]:\n    \"\"\"Fetch multiple URLs concurrently.\"\"\"\n    tasks = [self.fetch(url, **kwargs) for url in urls]\n    return await asyncio.gather(*tasks)\n</code></pre>"},{"location":"reference/ragcrawl/fetcher/crawl4ai_fetcher/#ragcrawl.fetcher.crawl4ai_fetcher.Crawl4AIFetcher.health_check","title":"<code>health_check()</code>","text":"<p>Check if fetcher is ready.</p> Source code in <code>src/ragcrawl/fetcher/crawl4ai_fetcher.py</code> <pre><code>def health_check(self) -&gt; bool:\n    \"\"\"Check if fetcher is ready.\"\"\"\n    return True  # HTTP always available\n</code></pre>"},{"location":"reference/ragcrawl/fetcher/revalidation/","title":"Revalidation","text":""},{"location":"reference/ragcrawl/fetcher/revalidation/#ragcrawl.fetcher.revalidation","title":"<code>revalidation</code>","text":"<p>HTTP conditional request handling for incremental sync.</p>"},{"location":"reference/ragcrawl/fetcher/revalidation/#ragcrawl.fetcher.revalidation.RevalidationResult","title":"<code>RevalidationResult(status, etag=None, last_modified=None, error=None)</code>  <code>dataclass</code>","text":"<p>Result of a revalidation check.</p>"},{"location":"reference/ragcrawl/fetcher/revalidation/#ragcrawl.fetcher.revalidation.RevalidationResult.needs_fetch","title":"<code>needs_fetch</code>  <code>property</code>","text":"<p>Check if full fetch is needed.</p>"},{"location":"reference/ragcrawl/fetcher/revalidation/#ragcrawl.fetcher.revalidation.RevalidationStatus","title":"<code>RevalidationStatus</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Status of revalidation check.</p>"},{"location":"reference/ragcrawl/fetcher/revalidation/#ragcrawl.fetcher.revalidation.Revalidator","title":"<code>Revalidator(use_etag=True, use_last_modified=True)</code>","text":"<p>Handles HTTP conditional requests for change detection.</p> <p>Uses ETag and Last-Modified headers to efficiently detect content changes without downloading full content.</p> <p>Initialize revalidator.</p> <p>Parameters:</p> Name Type Description Default <code>use_etag</code> <code>bool</code> <p>Whether to use ETag for validation.</p> <code>True</code> <code>use_last_modified</code> <code>bool</code> <p>Whether to use Last-Modified.</p> <code>True</code> Source code in <code>src/ragcrawl/fetcher/revalidation.py</code> <pre><code>def __init__(\n    self,\n    use_etag: bool = True,\n    use_last_modified: bool = True,\n) -&gt; None:\n    \"\"\"\n    Initialize revalidator.\n\n    Args:\n        use_etag: Whether to use ETag for validation.\n        use_last_modified: Whether to use Last-Modified.\n    \"\"\"\n    self.use_etag = use_etag\n    self.use_last_modified = use_last_modified\n</code></pre>"},{"location":"reference/ragcrawl/fetcher/revalidation/#ragcrawl.fetcher.revalidation.Revalidator.get_conditional_headers","title":"<code>get_conditional_headers(etag=None, last_modified=None)</code>","text":"<p>Get headers for conditional request.</p> <p>Parameters:</p> Name Type Description Default <code>etag</code> <code>str | None</code> <p>Stored ETag value.</p> <code>None</code> <code>last_modified</code> <code>str | None</code> <p>Stored Last-Modified value.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, str]</code> <p>Dict of conditional request headers.</p> Source code in <code>src/ragcrawl/fetcher/revalidation.py</code> <pre><code>def get_conditional_headers(\n    self,\n    etag: str | None = None,\n    last_modified: str | None = None,\n) -&gt; dict[str, str]:\n    \"\"\"\n    Get headers for conditional request.\n\n    Args:\n        etag: Stored ETag value.\n        last_modified: Stored Last-Modified value.\n\n    Returns:\n        Dict of conditional request headers.\n    \"\"\"\n    headers: dict[str, str] = {}\n\n    if self.use_etag and etag:\n        headers[\"If-None-Match\"] = etag\n\n    if self.use_last_modified and last_modified:\n        headers[\"If-Modified-Since\"] = last_modified\n\n    return headers\n</code></pre>"},{"location":"reference/ragcrawl/fetcher/revalidation/#ragcrawl.fetcher.revalidation.Revalidator.has_validators","title":"<code>has_validators(etag=None, last_modified=None)</code>","text":"<p>Check if we have validators for conditional requests.</p> <p>Parameters:</p> Name Type Description Default <code>etag</code> <code>str | None</code> <p>Stored ETag.</p> <code>None</code> <code>last_modified</code> <code>str | None</code> <p>Stored Last-Modified.</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if we can make conditional requests.</p> Source code in <code>src/ragcrawl/fetcher/revalidation.py</code> <pre><code>def has_validators(\n    self,\n    etag: str | None = None,\n    last_modified: str | None = None,\n) -&gt; bool:\n    \"\"\"\n    Check if we have validators for conditional requests.\n\n    Args:\n        etag: Stored ETag.\n        last_modified: Stored Last-Modified.\n\n    Returns:\n        True if we can make conditional requests.\n    \"\"\"\n    if self.use_etag and etag:\n        return True\n    if self.use_last_modified and last_modified:\n        return True\n    return False\n</code></pre>"},{"location":"reference/ragcrawl/fetcher/revalidation/#ragcrawl.fetcher.revalidation.Revalidator.parse_response","title":"<code>parse_response(status_code, headers)</code>","text":"<p>Parse response to determine if content changed.</p> <p>Parameters:</p> Name Type Description Default <code>status_code</code> <code>int</code> <p>HTTP status code.</p> required <code>headers</code> <code>dict[str, str]</code> <p>Response headers.</p> required <p>Returns:</p> Type Description <code>RevalidationResult</code> <p>RevalidationResult.</p> Source code in <code>src/ragcrawl/fetcher/revalidation.py</code> <pre><code>def parse_response(\n    self,\n    status_code: int,\n    headers: dict[str, str],\n) -&gt; RevalidationResult:\n    \"\"\"\n    Parse response to determine if content changed.\n\n    Args:\n        status_code: HTTP status code.\n        headers: Response headers.\n\n    Returns:\n        RevalidationResult.\n    \"\"\"\n    # Extract caching headers (case-insensitive)\n    headers_lower = {k.lower(): v for k, v in headers.items()}\n    etag = headers_lower.get(\"etag\")\n    last_modified = headers_lower.get(\"last-modified\")\n\n    if status_code == 304:\n        return RevalidationResult(\n            status=RevalidationStatus.NOT_MODIFIED,\n            etag=etag,\n            last_modified=last_modified,\n        )\n\n    if status_code &gt;= 200 and status_code &lt; 300:\n        return RevalidationResult(\n            status=RevalidationStatus.MODIFIED,\n            etag=etag,\n            last_modified=last_modified,\n        )\n\n    if status_code &gt;= 400:\n        return RevalidationResult(\n            status=RevalidationStatus.ERROR,\n            error=f\"HTTP {status_code}\",\n        )\n\n    return RevalidationResult(\n        status=RevalidationStatus.MODIFIED,\n        etag=etag,\n        last_modified=last_modified,\n    )\n</code></pre>"},{"location":"reference/ragcrawl/fetcher/robots/","title":"Robots","text":""},{"location":"reference/ragcrawl/fetcher/robots/#ragcrawl.fetcher.robots","title":"<code>robots</code>","text":"<p>Robots.txt parsing and checking.</p>"},{"location":"reference/ragcrawl/fetcher/robots/#ragcrawl.fetcher.robots.RobotsChecker","title":"<code>RobotsChecker(mode=RobotsMode.STRICT, user_agent='ragcrawl', allowlist=None, cache_ttl_seconds=3600)</code>","text":"<p>Checks URL access against robots.txt rules.</p> <p>Initialize robots checker.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>RobotsMode</code> <p>Robots.txt compliance mode.</p> <code>STRICT</code> <code>user_agent</code> <code>str</code> <p>User agent to check rules for.</p> <code>'ragcrawl'</code> <code>allowlist</code> <code>list[str] | None</code> <p>Domains to bypass robots.txt (when mode=ALLOWLIST).</p> <code>None</code> <code>cache_ttl_seconds</code> <code>int</code> <p>How long to cache robots.txt files.</p> <code>3600</code> Source code in <code>src/ragcrawl/fetcher/robots.py</code> <pre><code>def __init__(\n    self,\n    mode: RobotsMode = RobotsMode.STRICT,\n    user_agent: str = \"ragcrawl\",\n    allowlist: list[str] | None = None,\n    cache_ttl_seconds: int = 3600,\n) -&gt; None:\n    \"\"\"\n    Initialize robots checker.\n\n    Args:\n        mode: Robots.txt compliance mode.\n        user_agent: User agent to check rules for.\n        allowlist: Domains to bypass robots.txt (when mode=ALLOWLIST).\n        cache_ttl_seconds: How long to cache robots.txt files.\n    \"\"\"\n    self.mode = mode\n    self.user_agent = user_agent\n    self.allowlist = set(d.lower() for d in (allowlist or []))\n    self.cache_ttl_seconds = cache_ttl_seconds\n\n    # Cache: domain -&gt; (parser, timestamp)\n    self._cache: dict[str, tuple[RobotExclusionRulesParser | None, float]] = {}\n    self._lock = asyncio.Lock()\n</code></pre>"},{"location":"reference/ragcrawl/fetcher/robots/#ragcrawl.fetcher.robots.RobotsChecker.clear_cache","title":"<code>clear_cache()</code>","text":"<p>Clear the robots.txt cache.</p> Source code in <code>src/ragcrawl/fetcher/robots.py</code> <pre><code>def clear_cache(self) -&gt; None:\n    \"\"\"Clear the robots.txt cache.\"\"\"\n    self._cache.clear()\n</code></pre>"},{"location":"reference/ragcrawl/fetcher/robots/#ragcrawl.fetcher.robots.RobotsChecker.get_crawl_delay","title":"<code>get_crawl_delay(url)</code>","text":"<p>Get crawl delay from robots.txt.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>URL to check.</p> required <p>Returns:</p> Type Description <code>float | None</code> <p>Crawl delay in seconds, or None if not specified.</p> Source code in <code>src/ragcrawl/fetcher/robots.py</code> <pre><code>def get_crawl_delay(self, url: str) -&gt; float | None:\n    \"\"\"\n    Get crawl delay from robots.txt.\n\n    Args:\n        url: URL to check.\n\n    Returns:\n        Crawl delay in seconds, or None if not specified.\n    \"\"\"\n    domain = self._get_domain(url)\n\n    if domain in self._cache:\n        parser, _ = self._cache[domain]\n        if parser:\n            delay = parser.get_crawl_delay(self.user_agent)\n            if delay:\n                return float(delay)\n\n    return None\n</code></pre>"},{"location":"reference/ragcrawl/fetcher/robots/#ragcrawl.fetcher.robots.RobotsChecker.get_sitemaps","title":"<code>get_sitemaps(url)</code>","text":"<p>Get sitemap URLs from robots.txt.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>URL to check.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List of sitemap URLs.</p> Source code in <code>src/ragcrawl/fetcher/robots.py</code> <pre><code>def get_sitemaps(self, url: str) -&gt; list[str]:\n    \"\"\"\n    Get sitemap URLs from robots.txt.\n\n    Args:\n        url: URL to check.\n\n    Returns:\n        List of sitemap URLs.\n    \"\"\"\n    domain = self._get_domain(url)\n\n    if domain in self._cache:\n        parser, _ = self._cache[domain]\n        if parser and hasattr(parser, \"sitemaps\"):\n            return list(parser.sitemaps)\n\n    return []\n</code></pre>"},{"location":"reference/ragcrawl/fetcher/robots/#ragcrawl.fetcher.robots.RobotsChecker.is_allowed","title":"<code>is_allowed(url)</code>  <code>async</code>","text":"<p>Check if URL is allowed by robots.txt.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if crawling is allowed.</p> Source code in <code>src/ragcrawl/fetcher/robots.py</code> <pre><code>async def is_allowed(self, url: str) -&gt; bool:\n    \"\"\"\n    Check if URL is allowed by robots.txt.\n\n    Args:\n        url: The URL to check.\n\n    Returns:\n        True if crawling is allowed.\n    \"\"\"\n    if self.mode == RobotsMode.OFF:\n        return True\n\n    domain = self._get_domain(url)\n\n    if self.mode == RobotsMode.ALLOWLIST:\n        if domain in self.allowlist:\n            return True\n\n    if self.mode == RobotsMode.OFF:\n        return True\n\n    # Get or fetch robots.txt\n    parser = await self._get_parser(url)\n\n    if parser is None:\n        # If we can't fetch robots.txt, allow by default\n        return True\n\n    return parser.is_allowed(self.user_agent, url)\n</code></pre>"},{"location":"reference/ragcrawl/filters/","title":"Index","text":""},{"location":"reference/ragcrawl/filters/#ragcrawl.filters","title":"<code>filters</code>","text":"<p>URL filtering and normalization for ragcrawl.</p>"},{"location":"reference/ragcrawl/filters/#ragcrawl.filters.FilterResult","title":"<code>FilterResult(allowed, reason, normalized_url=None, details=None)</code>  <code>dataclass</code>","text":"<p>Result of URL filtering.</p>"},{"location":"reference/ragcrawl/filters/#ragcrawl.filters.LinkFilter","title":"<code>LinkFilter(allowed_domains=None, allow_subdomains=True, allowed_schemes=None, allowed_path_prefixes=None, blocked_extensions=None, include_patterns=None, exclude_patterns=None, blocked_query_params=None)</code>","text":"<p>Filters URLs based on domain, path, extension, and pattern constraints.</p> <p>This is the main filter used by the crawler to determine which URLs to include in the frontier.</p> <p>Initialize the link filter.</p> <p>Parameters:</p> Name Type Description Default <code>allowed_domains</code> <code>list[str] | None</code> <p>Domains to allow (empty = all).</p> <code>None</code> <code>allow_subdomains</code> <code>bool</code> <p>Whether to allow subdomains of allowed_domains.</p> <code>True</code> <code>allowed_schemes</code> <code>list[str] | None</code> <p>URL schemes to allow (default: http, https).</p> <code>None</code> <code>allowed_path_prefixes</code> <code>list[str] | None</code> <p>Path prefixes to allow (empty = all).</p> <code>None</code> <code>blocked_extensions</code> <code>list[str] | None</code> <p>File extensions to block.</p> <code>None</code> <code>include_patterns</code> <code>list[str] | None</code> <p>Regex/glob patterns for URLs to include.</p> <code>None</code> <code>exclude_patterns</code> <code>list[str] | None</code> <p>Regex/glob patterns for URLs to exclude.</p> <code>None</code> <code>blocked_query_params</code> <code>list[str] | None</code> <p>Query parameters to strip.</p> <code>None</code> Source code in <code>src/ragcrawl/filters/link_filter.py</code> <pre><code>def __init__(\n    self,\n    allowed_domains: list[str] | None = None,\n    allow_subdomains: bool = True,\n    allowed_schemes: list[str] | None = None,\n    allowed_path_prefixes: list[str] | None = None,\n    blocked_extensions: list[str] | None = None,\n    include_patterns: list[str] | None = None,\n    exclude_patterns: list[str] | None = None,\n    blocked_query_params: list[str] | None = None,\n) -&gt; None:\n    \"\"\"\n    Initialize the link filter.\n\n    Args:\n        allowed_domains: Domains to allow (empty = all).\n        allow_subdomains: Whether to allow subdomains of allowed_domains.\n        allowed_schemes: URL schemes to allow (default: http, https).\n        allowed_path_prefixes: Path prefixes to allow (empty = all).\n        blocked_extensions: File extensions to block.\n        include_patterns: Regex/glob patterns for URLs to include.\n        exclude_patterns: Regex/glob patterns for URLs to exclude.\n        blocked_query_params: Query parameters to strip.\n    \"\"\"\n    self.allowed_domains = set(d.lower() for d in (allowed_domains or []))\n    self.allow_subdomains = allow_subdomains\n    self.allowed_schemes = set(s.lower() for s in (allowed_schemes or [\"http\", \"https\"]))\n    self.allowed_path_prefixes = list(allowed_path_prefixes or [])\n\n    self.normalizer = URLNormalizer(remove_query_params=blocked_query_params)\n    self.pattern_matcher = PatternMatcher(include_patterns, exclude_patterns)\n    self.extension_filter = ExtensionFilter(blocked_extensions)\n\n    # Track seen URLs for deduplication\n    self._seen_urls: set[str] = set()\n</code></pre>"},{"location":"reference/ragcrawl/filters/#ragcrawl.filters.LinkFilter.seen_count","title":"<code>seen_count</code>  <code>property</code>","text":"<p>Get the count of seen URLs.</p>"},{"location":"reference/ragcrawl/filters/#ragcrawl.filters.LinkFilter.clear_seen","title":"<code>clear_seen()</code>","text":"<p>Clear the set of seen URLs.</p> Source code in <code>src/ragcrawl/filters/link_filter.py</code> <pre><code>def clear_seen(self) -&gt; None:\n    \"\"\"Clear the set of seen URLs.\"\"\"\n    self._seen_urls.clear()\n</code></pre>"},{"location":"reference/ragcrawl/filters/#ragcrawl.filters.LinkFilter.filter","title":"<code>filter(url, check_seen=True, current_depth=0, max_depth=None)</code>","text":"<p>Filter a URL and return the result.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL to filter.</p> required <code>check_seen</code> <code>bool</code> <p>Whether to check if URL was already seen.</p> <code>True</code> <code>current_depth</code> <code>int</code> <p>Current crawl depth.</p> <code>0</code> <code>max_depth</code> <code>int | None</code> <p>Maximum allowed depth.</p> <code>None</code> <p>Returns:</p> Type Description <code>FilterResult</code> <p>FilterResult with allowed status and reason.</p> Source code in <code>src/ragcrawl/filters/link_filter.py</code> <pre><code>def filter(\n    self,\n    url: str,\n    check_seen: bool = True,\n    current_depth: int = 0,\n    max_depth: int | None = None,\n) -&gt; FilterResult:\n    \"\"\"\n    Filter a URL and return the result.\n\n    Args:\n        url: The URL to filter.\n        check_seen: Whether to check if URL was already seen.\n        current_depth: Current crawl depth.\n        max_depth: Maximum allowed depth.\n\n    Returns:\n        FilterResult with allowed status and reason.\n    \"\"\"\n    # Parse and validate URL\n    try:\n        parsed = urlparse(url)\n    except Exception:\n        return FilterResult(\n            allowed=False,\n            reason=FilterReason.INVALID_URL,\n            details=\"Failed to parse URL\",\n        )\n\n    if not parsed.scheme or not parsed.netloc:\n        return FilterResult(\n            allowed=False,\n            reason=FilterReason.INVALID_URL,\n            details=\"Missing scheme or netloc\",\n        )\n\n    # Scheme check\n    if parsed.scheme.lower() not in self.allowed_schemes:\n        return FilterResult(\n            allowed=False,\n            reason=FilterReason.INVALID_SCHEME,\n            details=f\"Scheme '{parsed.scheme}' not allowed\",\n        )\n\n    # Normalize URL\n    normalized = self.normalizer.normalize(url)\n\n    # Deduplication check\n    if check_seen and normalized in self._seen_urls:\n        return FilterResult(\n            allowed=False,\n            reason=FilterReason.ALREADY_SEEN,\n            normalized_url=normalized,\n        )\n\n    # Depth check\n    if max_depth is not None and current_depth &gt; max_depth:\n        return FilterResult(\n            allowed=False,\n            reason=FilterReason.MAX_DEPTH_EXCEEDED,\n            normalized_url=normalized,\n            details=f\"Depth {current_depth} exceeds max {max_depth}\",\n        )\n\n    # Domain check\n    if self.allowed_domains:\n        hostname = parsed.netloc.lower()\n        # Remove port if present\n        if \":\" in hostname:\n            hostname = hostname.split(\":\")[0]\n\n        if not self._is_domain_allowed(hostname):\n            return FilterResult(\n                allowed=False,\n                reason=FilterReason.DOMAIN_NOT_ALLOWED,\n                normalized_url=normalized,\n                details=f\"Domain '{hostname}' not in allowed list\",\n            )\n\n    # Path prefix check\n    if self.allowed_path_prefixes:\n        path = parsed.path\n        if not any(path.startswith(prefix) for prefix in self.allowed_path_prefixes):\n            return FilterResult(\n                allowed=False,\n                reason=FilterReason.PATH_NOT_ALLOWED,\n                normalized_url=normalized,\n                details=f\"Path '{path}' doesn't match allowed prefixes\",\n            )\n\n    # Extension check\n    if self.extension_filter.is_blocked(url):\n        ext = self.extension_filter.get_extension(url)\n        return FilterResult(\n            allowed=False,\n            reason=FilterReason.BLOCKED_EXTENSION,\n            normalized_url=normalized,\n            details=f\"Extension '{ext}' is blocked\",\n        )\n\n    # Pattern check\n    if not self.pattern_matcher.should_include(url):\n        reason = self.pattern_matcher.get_match_reason(url)\n        if self.pattern_matcher.matches_exclude(url):\n            return FilterResult(\n                allowed=False,\n                reason=FilterReason.EXCLUDED_PATTERN,\n                normalized_url=normalized,\n                details=reason,\n            )\n        else:\n            return FilterResult(\n                allowed=False,\n                reason=FilterReason.NO_INCLUDE_MATCH,\n                normalized_url=normalized,\n                details=reason,\n            )\n\n    # URL is allowed\n    return FilterResult(\n        allowed=True,\n        reason=FilterReason.ALLOWED,\n        normalized_url=normalized,\n    )\n</code></pre>"},{"location":"reference/ragcrawl/filters/#ragcrawl.filters.LinkFilter.is_seen","title":"<code>is_seen(url)</code>","text":"<p>Check if URL has been seen.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if URL has been seen.</p> Source code in <code>src/ragcrawl/filters/link_filter.py</code> <pre><code>def is_seen(self, url: str) -&gt; bool:\n    \"\"\"\n    Check if URL has been seen.\n\n    Args:\n        url: The URL to check.\n\n    Returns:\n        True if URL has been seen.\n    \"\"\"\n    normalized = self.normalizer.normalize(url)\n    return normalized in self._seen_urls\n</code></pre>"},{"location":"reference/ragcrawl/filters/#ragcrawl.filters.LinkFilter.mark_seen","title":"<code>mark_seen(url)</code>","text":"<p>Mark a URL as seen and return normalized form.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL to mark.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The normalized URL.</p> Source code in <code>src/ragcrawl/filters/link_filter.py</code> <pre><code>def mark_seen(self, url: str) -&gt; str:\n    \"\"\"\n    Mark a URL as seen and return normalized form.\n\n    Args:\n        url: The URL to mark.\n\n    Returns:\n        The normalized URL.\n    \"\"\"\n    normalized = self.normalizer.normalize(url)\n    self._seen_urls.add(normalized)\n    return normalized\n</code></pre>"},{"location":"reference/ragcrawl/filters/#ragcrawl.filters.PatternMatcher","title":"<code>PatternMatcher(include_patterns=None, exclude_patterns=None, case_sensitive=False)</code>","text":"<p>Matches URLs against include/exclude patterns.</p> <p>Supports both regex and glob patterns.</p> <p>Initialize the pattern matcher.</p> <p>Parameters:</p> Name Type Description Default <code>include_patterns</code> <code>list[str] | None</code> <p>Patterns for URLs to include (regex or glob).</p> <code>None</code> <code>exclude_patterns</code> <code>list[str] | None</code> <p>Patterns for URLs to exclude (regex or glob).</p> <code>None</code> <code>case_sensitive</code> <code>bool</code> <p>Whether pattern matching is case-sensitive.</p> <code>False</code> Source code in <code>src/ragcrawl/filters/patterns.py</code> <pre><code>def __init__(\n    self,\n    include_patterns: list[str] | None = None,\n    exclude_patterns: list[str] | None = None,\n    case_sensitive: bool = False,\n) -&gt; None:\n    \"\"\"\n    Initialize the pattern matcher.\n\n    Args:\n        include_patterns: Patterns for URLs to include (regex or glob).\n        exclude_patterns: Patterns for URLs to exclude (regex or glob).\n        case_sensitive: Whether pattern matching is case-sensitive.\n    \"\"\"\n    self.case_sensitive = case_sensitive\n    self._include_patterns = self._compile_patterns(include_patterns or [])\n    self._exclude_patterns = self._compile_patterns(exclude_patterns or [])\n</code></pre>"},{"location":"reference/ragcrawl/filters/#ragcrawl.filters.PatternMatcher.get_match_reason","title":"<code>get_match_reason(url)</code>","text":"<p>Get the reason for inclusion/exclusion.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL to check.</p> required <p>Returns:</p> Type Description <code>str | None</code> <p>A string describing the match, or None if included by default.</p> Source code in <code>src/ragcrawl/filters/patterns.py</code> <pre><code>def get_match_reason(self, url: str) -&gt; str | None:\n    \"\"\"\n    Get the reason for inclusion/exclusion.\n\n    Args:\n        url: The URL to check.\n\n    Returns:\n        A string describing the match, or None if included by default.\n    \"\"\"\n    for pattern in self._exclude_patterns:\n        if pattern.search(url):\n            return f\"excluded by pattern: {pattern.pattern}\"\n\n    if self._include_patterns:\n        for pattern in self._include_patterns:\n            if pattern.search(url):\n                return f\"included by pattern: {pattern.pattern}\"\n\n        return \"no include pattern matched\"\n\n    return None\n</code></pre>"},{"location":"reference/ragcrawl/filters/#ragcrawl.filters.PatternMatcher.matches_exclude","title":"<code>matches_exclude(url)</code>","text":"<p>Check if URL matches any exclude pattern.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if URL matches an exclude pattern.</p> Source code in <code>src/ragcrawl/filters/patterns.py</code> <pre><code>def matches_exclude(self, url: str) -&gt; bool:\n    \"\"\"\n    Check if URL matches any exclude pattern.\n\n    Args:\n        url: The URL to check.\n\n    Returns:\n        True if URL matches an exclude pattern.\n    \"\"\"\n    if not self._exclude_patterns:\n        return False\n\n    return any(p.search(url) for p in self._exclude_patterns)\n</code></pre>"},{"location":"reference/ragcrawl/filters/#ragcrawl.filters.PatternMatcher.matches_include","title":"<code>matches_include(url)</code>","text":"<p>Check if URL matches any include pattern.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if URL matches an include pattern or no include patterns defined.</p> Source code in <code>src/ragcrawl/filters/patterns.py</code> <pre><code>def matches_include(self, url: str) -&gt; bool:\n    \"\"\"\n    Check if URL matches any include pattern.\n\n    Args:\n        url: The URL to check.\n\n    Returns:\n        True if URL matches an include pattern or no include patterns defined.\n    \"\"\"\n    if not self._include_patterns:\n        return True\n\n    return any(p.search(url) for p in self._include_patterns)\n</code></pre>"},{"location":"reference/ragcrawl/filters/#ragcrawl.filters.PatternMatcher.should_include","title":"<code>should_include(url)</code>","text":"<p>Determine if URL should be included based on patterns.</p> <p>Exclude patterns take precedence over include patterns.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if URL should be included.</p> Source code in <code>src/ragcrawl/filters/patterns.py</code> <pre><code>def should_include(self, url: str) -&gt; bool:\n    \"\"\"\n    Determine if URL should be included based on patterns.\n\n    Exclude patterns take precedence over include patterns.\n\n    Args:\n        url: The URL to check.\n\n    Returns:\n        True if URL should be included.\n    \"\"\"\n    # Exclude takes precedence\n    if self.matches_exclude(url):\n        return False\n\n    return self.matches_include(url)\n</code></pre>"},{"location":"reference/ragcrawl/filters/#ragcrawl.filters.QualityGate","title":"<code>QualityGate(min_text_length=100, min_word_count=20, max_duplicate_ratio=0.9, block_patterns=None, detect_language=False, allowed_languages=None)</code>","text":"<p>Evaluates content quality to filter thin/low-value pages.</p> <p>Initialize quality gates.</p> <p>Parameters:</p> Name Type Description Default <code>min_text_length</code> <code>int</code> <p>Minimum text length in characters.</p> <code>100</code> <code>min_word_count</code> <code>int</code> <p>Minimum word count.</p> <code>20</code> <code>max_duplicate_ratio</code> <code>float</code> <p>Maximum ratio of duplicate content.</p> <code>0.9</code> <code>block_patterns</code> <code>list[str] | None</code> <p>URL patterns for thin/low-value pages.</p> <code>None</code> <code>detect_language</code> <code>bool</code> <p>Whether to detect language.</p> <code>False</code> <code>allowed_languages</code> <code>list[str] | None</code> <p>Allowed language codes (None = all).</p> <code>None</code> Source code in <code>src/ragcrawl/filters/quality_gates.py</code> <pre><code>def __init__(\n    self,\n    min_text_length: int = 100,\n    min_word_count: int = 20,\n    max_duplicate_ratio: float = 0.9,\n    block_patterns: list[str] | None = None,\n    detect_language: bool = False,\n    allowed_languages: list[str] | None = None,\n) -&gt; None:\n    \"\"\"\n    Initialize quality gates.\n\n    Args:\n        min_text_length: Minimum text length in characters.\n        min_word_count: Minimum word count.\n        max_duplicate_ratio: Maximum ratio of duplicate content.\n        block_patterns: URL patterns for thin/low-value pages.\n        detect_language: Whether to detect language.\n        allowed_languages: Allowed language codes (None = all).\n    \"\"\"\n    self.min_text_length = min_text_length\n    self.min_word_count = min_word_count\n    self.max_duplicate_ratio = max_duplicate_ratio\n    self.detect_language = detect_language\n    self.allowed_languages = set(allowed_languages or [])\n\n    # Compile block patterns\n    self.block_patterns = []\n    for pattern in block_patterns or []:\n        try:\n            self.block_patterns.append(re.compile(pattern, re.IGNORECASE))\n        except re.error:\n            pass\n\n    # Content hash cache for duplicate detection\n    self._content_hashes: dict[str, str] = {}\n</code></pre>"},{"location":"reference/ragcrawl/filters/#ragcrawl.filters.QualityGate.check_all","title":"<code>check_all(url, content, content_hash=None)</code>","text":"<p>Run all quality checks.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL.</p> required <code>content</code> <code>str</code> <p>The text/markdown content.</p> required <code>content_hash</code> <code>str | None</code> <p>Optional pre-computed content hash.</p> <code>None</code> <p>Returns:</p> Type Description <code>QualityResult</code> <p>QualityResult from first failing check, or passed.</p> Source code in <code>src/ragcrawl/filters/quality_gates.py</code> <pre><code>def check_all(\n    self,\n    url: str,\n    content: str,\n    content_hash: str | None = None,\n) -&gt; QualityResult:\n    \"\"\"\n    Run all quality checks.\n\n    Args:\n        url: The URL.\n        content: The text/markdown content.\n        content_hash: Optional pre-computed content hash.\n\n    Returns:\n        QualityResult from first failing check, or passed.\n    \"\"\"\n    # URL check first (fast)\n    url_result = self.check_url(url)\n    if not url_result.passed:\n        return url_result\n\n    # Content check\n    content_result = self.check_content(content, url, content_hash)\n    return content_result\n</code></pre>"},{"location":"reference/ragcrawl/filters/#ragcrawl.filters.QualityGate.check_content","title":"<code>check_content(content, url=None, content_hash=None)</code>","text":"<p>Check if content passes quality gates.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>The text/markdown content.</p> required <code>url</code> <code>str | None</code> <p>Optional URL for context.</p> <code>None</code> <code>content_hash</code> <code>str | None</code> <p>Optional pre-computed content hash.</p> <code>None</code> <p>Returns:</p> Type Description <code>QualityResult</code> <p>QualityResult.</p> Source code in <code>src/ragcrawl/filters/quality_gates.py</code> <pre><code>def check_content(\n    self,\n    content: str,\n    url: str | None = None,\n    content_hash: str | None = None,\n) -&gt; QualityResult:\n    \"\"\"\n    Check if content passes quality gates.\n\n    Args:\n        content: The text/markdown content.\n        url: Optional URL for context.\n        content_hash: Optional pre-computed content hash.\n\n    Returns:\n        QualityResult.\n    \"\"\"\n    # Length check\n    text_length = len(content)\n    if text_length &lt; self.min_text_length:\n        return QualityResult(\n            passed=False,\n            issue=QualityIssue.TOO_SHORT,\n            details=f\"Content length {text_length} &lt; {self.min_text_length}\",\n            metrics={\"text_length\": text_length},\n        )\n\n    # Word count check\n    word_count = len(content.split())\n    if word_count &lt; self.min_word_count:\n        return QualityResult(\n            passed=False,\n            issue=QualityIssue.LOW_WORD_COUNT,\n            details=f\"Word count {word_count} &lt; {self.min_word_count}\",\n            metrics={\"word_count\": word_count},\n        )\n\n    # Duplicate content check\n    if content_hash:\n        if content_hash in self._content_hashes:\n            original_url = self._content_hashes[content_hash]\n            return QualityResult(\n                passed=False,\n                issue=QualityIssue.DUPLICATE_CONTENT,\n                details=f\"Duplicate of {original_url}\",\n                metrics={\"original_url\": original_url},\n            )\n\n        if url:\n            self._content_hashes[content_hash] = url\n\n    # Language detection (optional)\n    if self.detect_language and self.allowed_languages:\n        detected_lang = self._detect_language(content)\n        if detected_lang and detected_lang not in self.allowed_languages:\n            return QualityResult(\n                passed=False,\n                issue=QualityIssue.WRONG_LANGUAGE,\n                details=f\"Language '{detected_lang}' not in allowed list\",\n                metrics={\"detected_language\": detected_lang},\n            )\n\n    return QualityResult(\n        passed=True,\n        issue=QualityIssue.PASSED,\n        metrics={\n            \"text_length\": text_length,\n            \"word_count\": word_count,\n        },\n    )\n</code></pre>"},{"location":"reference/ragcrawl/filters/#ragcrawl.filters.QualityGate.check_url","title":"<code>check_url(url)</code>","text":"<p>Check if URL matches any block patterns.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL to check.</p> required <p>Returns:</p> Type Description <code>QualityResult</code> <p>QualityResult.</p> Source code in <code>src/ragcrawl/filters/quality_gates.py</code> <pre><code>def check_url(self, url: str) -&gt; QualityResult:\n    \"\"\"\n    Check if URL matches any block patterns.\n\n    Args:\n        url: The URL to check.\n\n    Returns:\n        QualityResult.\n    \"\"\"\n    for pattern in self.block_patterns:\n        if pattern.search(url):\n            return QualityResult(\n                passed=False,\n                issue=QualityIssue.BLOCKED_PATTERN,\n                details=f\"URL matches block pattern: {pattern.pattern}\",\n            )\n\n    return QualityResult(passed=True, issue=QualityIssue.PASSED)\n</code></pre>"},{"location":"reference/ragcrawl/filters/#ragcrawl.filters.QualityGate.clear_hash_cache","title":"<code>clear_hash_cache()</code>","text":"<p>Clear the content hash cache.</p> Source code in <code>src/ragcrawl/filters/quality_gates.py</code> <pre><code>def clear_hash_cache(self) -&gt; None:\n    \"\"\"Clear the content hash cache.\"\"\"\n    self._content_hashes.clear()\n</code></pre>"},{"location":"reference/ragcrawl/filters/#ragcrawl.filters.QualityResult","title":"<code>QualityResult(passed, issue, details=None, metrics=None)</code>  <code>dataclass</code>","text":"<p>Result of quality gate evaluation.</p>"},{"location":"reference/ragcrawl/filters/#ragcrawl.filters.URLNormalizer","title":"<code>URLNormalizer(remove_fragments=True, normalize_trailing_slash=True, sort_query_params=True, remove_query_params=None, lowercase_hostname=True, remove_default_ports=True, remove_www=False)</code>","text":"<p>Normalizes URLs for deterministic deduplication.</p> <p>Handles: - Fragment removal - Trailing slash normalization - Query parameter sorting and filtering - Scheme normalization - Case normalization for hostname - Path normalization</p> <p>Initialize the URL normalizer.</p> <p>Parameters:</p> Name Type Description Default <code>remove_fragments</code> <code>bool</code> <p>Remove URL fragments (#...).</p> <code>True</code> <code>normalize_trailing_slash</code> <code>bool</code> <p>Ensure consistent trailing slash handling.</p> <code>True</code> <code>sort_query_params</code> <code>bool</code> <p>Sort query parameters alphabetically.</p> <code>True</code> <code>remove_query_params</code> <code>list[str] | None</code> <p>List of query params to remove (e.g., tracking params).</p> <code>None</code> <code>lowercase_hostname</code> <code>bool</code> <p>Lowercase the hostname.</p> <code>True</code> <code>remove_default_ports</code> <code>bool</code> <p>Remove default ports (80, 443).</p> <code>True</code> <code>remove_www</code> <code>bool</code> <p>Remove www. prefix from hostname.</p> <code>False</code> Source code in <code>src/ragcrawl/filters/url_normalizer.py</code> <pre><code>def __init__(\n    self,\n    remove_fragments: bool = True,\n    normalize_trailing_slash: bool = True,\n    sort_query_params: bool = True,\n    remove_query_params: list[str] | None = None,\n    lowercase_hostname: bool = True,\n    remove_default_ports: bool = True,\n    remove_www: bool = False,\n) -&gt; None:\n    \"\"\"\n    Initialize the URL normalizer.\n\n    Args:\n        remove_fragments: Remove URL fragments (#...).\n        normalize_trailing_slash: Ensure consistent trailing slash handling.\n        sort_query_params: Sort query parameters alphabetically.\n        remove_query_params: List of query params to remove (e.g., tracking params).\n        lowercase_hostname: Lowercase the hostname.\n        remove_default_ports: Remove default ports (80, 443).\n        remove_www: Remove www. prefix from hostname.\n    \"\"\"\n    self.remove_fragments = remove_fragments\n    self.normalize_trailing_slash = normalize_trailing_slash\n    self.sort_query_params = sort_query_params\n    self.remove_query_params = set(remove_query_params or [])\n    self.lowercase_hostname = lowercase_hostname\n    self.remove_default_ports = remove_default_ports\n    self.remove_www = remove_www\n\n    # Default tracking params to remove\n    self.default_tracking_params = {\n        \"utm_source\",\n        \"utm_medium\",\n        \"utm_campaign\",\n        \"utm_term\",\n        \"utm_content\",\n        \"fbclid\",\n        \"gclid\",\n        \"ref\",\n        \"source\",\n    }\n</code></pre>"},{"location":"reference/ragcrawl/filters/#ragcrawl.filters.URLNormalizer.get_domain","title":"<code>get_domain(url)</code>","text":"<p>Extract the domain from a URL.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The domain (e.g., 'example.com').</p> Source code in <code>src/ragcrawl/filters/url_normalizer.py</code> <pre><code>def get_domain(self, url: str) -&gt; str:\n    \"\"\"\n    Extract the domain from a URL.\n\n    Args:\n        url: The URL.\n\n    Returns:\n        The domain (e.g., 'example.com').\n    \"\"\"\n    try:\n        parsed = urlparse(url)\n        hostname = parsed.netloc.lower()\n\n        # Remove port\n        if \":\" in hostname:\n            hostname = hostname.split(\":\")[0]\n\n        return hostname\n    except Exception:\n        return \"\"\n</code></pre>"},{"location":"reference/ragcrawl/filters/#ragcrawl.filters.URLNormalizer.get_registered_domain","title":"<code>get_registered_domain(url)</code>","text":"<p>Extract the registered domain (eTLD+1) from a URL.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The registered domain (e.g., 'example.com' for 'sub.example.com').</p> Source code in <code>src/ragcrawl/filters/url_normalizer.py</code> <pre><code>def get_registered_domain(self, url: str) -&gt; str:\n    \"\"\"\n    Extract the registered domain (eTLD+1) from a URL.\n\n    Args:\n        url: The URL.\n\n    Returns:\n        The registered domain (e.g., 'example.com' for 'sub.example.com').\n    \"\"\"\n    try:\n        extracted = tldextract.extract(url)\n        if extracted.domain and extracted.suffix:\n            return f\"{extracted.domain}.{extracted.suffix}\"\n        return extracted.domain or \"\"\n    except Exception:\n        return \"\"\n</code></pre>"},{"location":"reference/ragcrawl/filters/#ragcrawl.filters.URLNormalizer.is_same_domain","title":"<code>is_same_domain(url1, url2)</code>","text":"<p>Check if two URLs are on the same domain.</p> <p>Parameters:</p> Name Type Description Default <code>url1</code> <code>str</code> <p>First URL.</p> required <code>url2</code> <code>str</code> <p>Second URL.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if same domain.</p> Source code in <code>src/ragcrawl/filters/url_normalizer.py</code> <pre><code>def is_same_domain(self, url1: str, url2: str) -&gt; bool:\n    \"\"\"\n    Check if two URLs are on the same domain.\n\n    Args:\n        url1: First URL.\n        url2: Second URL.\n\n    Returns:\n        True if same domain.\n    \"\"\"\n    return self.get_domain(url1) == self.get_domain(url2)\n</code></pre>"},{"location":"reference/ragcrawl/filters/#ragcrawl.filters.URLNormalizer.is_same_registered_domain","title":"<code>is_same_registered_domain(url1, url2)</code>","text":"<p>Check if two URLs are on the same registered domain.</p> <p>This considers subdomains as the same domain.</p> <p>Parameters:</p> Name Type Description Default <code>url1</code> <code>str</code> <p>First URL.</p> required <code>url2</code> <code>str</code> <p>Second URL.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if same registered domain.</p> Source code in <code>src/ragcrawl/filters/url_normalizer.py</code> <pre><code>def is_same_registered_domain(self, url1: str, url2: str) -&gt; bool:\n    \"\"\"\n    Check if two URLs are on the same registered domain.\n\n    This considers subdomains as the same domain.\n\n    Args:\n        url1: First URL.\n        url2: Second URL.\n\n    Returns:\n        True if same registered domain.\n    \"\"\"\n    return self.get_registered_domain(url1) == self.get_registered_domain(url2)\n</code></pre>"},{"location":"reference/ragcrawl/filters/#ragcrawl.filters.URLNormalizer.normalize","title":"<code>normalize(url)</code>","text":"<p>Normalize a URL for deduplication.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL to normalize.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The normalized URL string.</p> Source code in <code>src/ragcrawl/filters/url_normalizer.py</code> <pre><code>def normalize(self, url: str) -&gt; str:\n    \"\"\"\n    Normalize a URL for deduplication.\n\n    Args:\n        url: The URL to normalize.\n\n    Returns:\n        The normalized URL string.\n    \"\"\"\n    try:\n        parsed = urlparse(url)\n    except Exception:\n        return url\n\n    # Scheme normalization (lowercase)\n    scheme = parsed.scheme.lower()\n\n    # Hostname normalization\n    hostname = parsed.netloc\n    if self.lowercase_hostname:\n        hostname = hostname.lower()\n\n    # Remove default ports\n    if self.remove_default_ports:\n        if scheme == \"http\" and hostname.endswith(\":80\"):\n            hostname = hostname[:-3]\n        elif scheme == \"https\" and hostname.endswith(\":443\"):\n            hostname = hostname[:-4]\n\n    # Remove www prefix\n    if self.remove_www and hostname.startswith(\"www.\"):\n        hostname = hostname[4:]\n\n    # Path normalization\n    path = parsed.path\n\n    # Remove duplicate slashes\n    path = re.sub(r\"/+\", \"/\", path)\n\n    # Normalize path encoding\n    # Decode safe characters that don't need encoding\n    path = path.replace(\"%7E\", \"~\")\n\n    # Handle trailing slash\n    if self.normalize_trailing_slash:\n        # Keep trailing slash only for directories (no extension)\n        if path and not path.endswith(\"/\"):\n            # Check if it looks like a file (has extension)\n            last_segment = path.split(\"/\")[-1]\n            if \".\" not in last_segment and path != \"/\":\n                # It's a directory-like path, could add trailing slash\n                # But for consistency, we'll remove trailing slashes\n                pass\n        # Remove trailing slash except for root\n        if path != \"/\" and path.endswith(\"/\"):\n            path = path.rstrip(\"/\")\n\n    # Empty path becomes /\n    if not path:\n        path = \"/\"\n\n    # Query parameter normalization\n    query = parsed.query\n    if query:\n        params = parse_qs(query, keep_blank_values=True)\n\n        # Remove tracking and specified params\n        params_to_remove = self.remove_query_params | self.default_tracking_params\n        params = {k: v for k, v in params.items() if k not in params_to_remove}\n\n        # Sort and rebuild query string\n        if self.sort_query_params:\n            sorted_params = sorted(params.items())\n            # Flatten multi-value params\n            flat_params = []\n            for k, values in sorted_params:\n                for v in sorted(values):\n                    flat_params.append((k, v))\n            query = urlencode(flat_params)\n        else:\n            query = urlencode(params, doseq=True)\n    else:\n        query = \"\"\n\n    # Fragment handling\n    fragment = \"\" if self.remove_fragments else parsed.fragment\n\n    # Rebuild URL\n    normalized = urlunparse((scheme, hostname, path, \"\", query, fragment))\n\n    return normalized\n</code></pre>"},{"location":"reference/ragcrawl/filters/#ragcrawl.filters.normalize_url","title":"<code>normalize_url(url)</code>","text":"<p>Normalize a URL using default settings.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL to normalize.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The normalized URL.</p> Source code in <code>src/ragcrawl/filters/url_normalizer.py</code> <pre><code>def normalize_url(url: str) -&gt; str:\n    \"\"\"\n    Normalize a URL using default settings.\n\n    Args:\n        url: The URL to normalize.\n\n    Returns:\n        The normalized URL.\n    \"\"\"\n    return _default_normalizer.normalize(url)\n</code></pre>"},{"location":"reference/ragcrawl/filters/link_filter/","title":"Link filter","text":""},{"location":"reference/ragcrawl/filters/link_filter/#ragcrawl.filters.link_filter","title":"<code>link_filter</code>","text":"<p>Link filtering based on domain, path, and pattern constraints.</p>"},{"location":"reference/ragcrawl/filters/link_filter/#ragcrawl.filters.link_filter.FilterReason","title":"<code>FilterReason</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Reason for filtering a URL.</p>"},{"location":"reference/ragcrawl/filters/link_filter/#ragcrawl.filters.link_filter.FilterResult","title":"<code>FilterResult(allowed, reason, normalized_url=None, details=None)</code>  <code>dataclass</code>","text":"<p>Result of URL filtering.</p>"},{"location":"reference/ragcrawl/filters/link_filter/#ragcrawl.filters.link_filter.LinkFilter","title":"<code>LinkFilter(allowed_domains=None, allow_subdomains=True, allowed_schemes=None, allowed_path_prefixes=None, blocked_extensions=None, include_patterns=None, exclude_patterns=None, blocked_query_params=None)</code>","text":"<p>Filters URLs based on domain, path, extension, and pattern constraints.</p> <p>This is the main filter used by the crawler to determine which URLs to include in the frontier.</p> <p>Initialize the link filter.</p> <p>Parameters:</p> Name Type Description Default <code>allowed_domains</code> <code>list[str] | None</code> <p>Domains to allow (empty = all).</p> <code>None</code> <code>allow_subdomains</code> <code>bool</code> <p>Whether to allow subdomains of allowed_domains.</p> <code>True</code> <code>allowed_schemes</code> <code>list[str] | None</code> <p>URL schemes to allow (default: http, https).</p> <code>None</code> <code>allowed_path_prefixes</code> <code>list[str] | None</code> <p>Path prefixes to allow (empty = all).</p> <code>None</code> <code>blocked_extensions</code> <code>list[str] | None</code> <p>File extensions to block.</p> <code>None</code> <code>include_patterns</code> <code>list[str] | None</code> <p>Regex/glob patterns for URLs to include.</p> <code>None</code> <code>exclude_patterns</code> <code>list[str] | None</code> <p>Regex/glob patterns for URLs to exclude.</p> <code>None</code> <code>blocked_query_params</code> <code>list[str] | None</code> <p>Query parameters to strip.</p> <code>None</code> Source code in <code>src/ragcrawl/filters/link_filter.py</code> <pre><code>def __init__(\n    self,\n    allowed_domains: list[str] | None = None,\n    allow_subdomains: bool = True,\n    allowed_schemes: list[str] | None = None,\n    allowed_path_prefixes: list[str] | None = None,\n    blocked_extensions: list[str] | None = None,\n    include_patterns: list[str] | None = None,\n    exclude_patterns: list[str] | None = None,\n    blocked_query_params: list[str] | None = None,\n) -&gt; None:\n    \"\"\"\n    Initialize the link filter.\n\n    Args:\n        allowed_domains: Domains to allow (empty = all).\n        allow_subdomains: Whether to allow subdomains of allowed_domains.\n        allowed_schemes: URL schemes to allow (default: http, https).\n        allowed_path_prefixes: Path prefixes to allow (empty = all).\n        blocked_extensions: File extensions to block.\n        include_patterns: Regex/glob patterns for URLs to include.\n        exclude_patterns: Regex/glob patterns for URLs to exclude.\n        blocked_query_params: Query parameters to strip.\n    \"\"\"\n    self.allowed_domains = set(d.lower() for d in (allowed_domains or []))\n    self.allow_subdomains = allow_subdomains\n    self.allowed_schemes = set(s.lower() for s in (allowed_schemes or [\"http\", \"https\"]))\n    self.allowed_path_prefixes = list(allowed_path_prefixes or [])\n\n    self.normalizer = URLNormalizer(remove_query_params=blocked_query_params)\n    self.pattern_matcher = PatternMatcher(include_patterns, exclude_patterns)\n    self.extension_filter = ExtensionFilter(blocked_extensions)\n\n    # Track seen URLs for deduplication\n    self._seen_urls: set[str] = set()\n</code></pre>"},{"location":"reference/ragcrawl/filters/link_filter/#ragcrawl.filters.link_filter.LinkFilter.seen_count","title":"<code>seen_count</code>  <code>property</code>","text":"<p>Get the count of seen URLs.</p>"},{"location":"reference/ragcrawl/filters/link_filter/#ragcrawl.filters.link_filter.LinkFilter.clear_seen","title":"<code>clear_seen()</code>","text":"<p>Clear the set of seen URLs.</p> Source code in <code>src/ragcrawl/filters/link_filter.py</code> <pre><code>def clear_seen(self) -&gt; None:\n    \"\"\"Clear the set of seen URLs.\"\"\"\n    self._seen_urls.clear()\n</code></pre>"},{"location":"reference/ragcrawl/filters/link_filter/#ragcrawl.filters.link_filter.LinkFilter.filter","title":"<code>filter(url, check_seen=True, current_depth=0, max_depth=None)</code>","text":"<p>Filter a URL and return the result.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL to filter.</p> required <code>check_seen</code> <code>bool</code> <p>Whether to check if URL was already seen.</p> <code>True</code> <code>current_depth</code> <code>int</code> <p>Current crawl depth.</p> <code>0</code> <code>max_depth</code> <code>int | None</code> <p>Maximum allowed depth.</p> <code>None</code> <p>Returns:</p> Type Description <code>FilterResult</code> <p>FilterResult with allowed status and reason.</p> Source code in <code>src/ragcrawl/filters/link_filter.py</code> <pre><code>def filter(\n    self,\n    url: str,\n    check_seen: bool = True,\n    current_depth: int = 0,\n    max_depth: int | None = None,\n) -&gt; FilterResult:\n    \"\"\"\n    Filter a URL and return the result.\n\n    Args:\n        url: The URL to filter.\n        check_seen: Whether to check if URL was already seen.\n        current_depth: Current crawl depth.\n        max_depth: Maximum allowed depth.\n\n    Returns:\n        FilterResult with allowed status and reason.\n    \"\"\"\n    # Parse and validate URL\n    try:\n        parsed = urlparse(url)\n    except Exception:\n        return FilterResult(\n            allowed=False,\n            reason=FilterReason.INVALID_URL,\n            details=\"Failed to parse URL\",\n        )\n\n    if not parsed.scheme or not parsed.netloc:\n        return FilterResult(\n            allowed=False,\n            reason=FilterReason.INVALID_URL,\n            details=\"Missing scheme or netloc\",\n        )\n\n    # Scheme check\n    if parsed.scheme.lower() not in self.allowed_schemes:\n        return FilterResult(\n            allowed=False,\n            reason=FilterReason.INVALID_SCHEME,\n            details=f\"Scheme '{parsed.scheme}' not allowed\",\n        )\n\n    # Normalize URL\n    normalized = self.normalizer.normalize(url)\n\n    # Deduplication check\n    if check_seen and normalized in self._seen_urls:\n        return FilterResult(\n            allowed=False,\n            reason=FilterReason.ALREADY_SEEN,\n            normalized_url=normalized,\n        )\n\n    # Depth check\n    if max_depth is not None and current_depth &gt; max_depth:\n        return FilterResult(\n            allowed=False,\n            reason=FilterReason.MAX_DEPTH_EXCEEDED,\n            normalized_url=normalized,\n            details=f\"Depth {current_depth} exceeds max {max_depth}\",\n        )\n\n    # Domain check\n    if self.allowed_domains:\n        hostname = parsed.netloc.lower()\n        # Remove port if present\n        if \":\" in hostname:\n            hostname = hostname.split(\":\")[0]\n\n        if not self._is_domain_allowed(hostname):\n            return FilterResult(\n                allowed=False,\n                reason=FilterReason.DOMAIN_NOT_ALLOWED,\n                normalized_url=normalized,\n                details=f\"Domain '{hostname}' not in allowed list\",\n            )\n\n    # Path prefix check\n    if self.allowed_path_prefixes:\n        path = parsed.path\n        if not any(path.startswith(prefix) for prefix in self.allowed_path_prefixes):\n            return FilterResult(\n                allowed=False,\n                reason=FilterReason.PATH_NOT_ALLOWED,\n                normalized_url=normalized,\n                details=f\"Path '{path}' doesn't match allowed prefixes\",\n            )\n\n    # Extension check\n    if self.extension_filter.is_blocked(url):\n        ext = self.extension_filter.get_extension(url)\n        return FilterResult(\n            allowed=False,\n            reason=FilterReason.BLOCKED_EXTENSION,\n            normalized_url=normalized,\n            details=f\"Extension '{ext}' is blocked\",\n        )\n\n    # Pattern check\n    if not self.pattern_matcher.should_include(url):\n        reason = self.pattern_matcher.get_match_reason(url)\n        if self.pattern_matcher.matches_exclude(url):\n            return FilterResult(\n                allowed=False,\n                reason=FilterReason.EXCLUDED_PATTERN,\n                normalized_url=normalized,\n                details=reason,\n            )\n        else:\n            return FilterResult(\n                allowed=False,\n                reason=FilterReason.NO_INCLUDE_MATCH,\n                normalized_url=normalized,\n                details=reason,\n            )\n\n    # URL is allowed\n    return FilterResult(\n        allowed=True,\n        reason=FilterReason.ALLOWED,\n        normalized_url=normalized,\n    )\n</code></pre>"},{"location":"reference/ragcrawl/filters/link_filter/#ragcrawl.filters.link_filter.LinkFilter.is_seen","title":"<code>is_seen(url)</code>","text":"<p>Check if URL has been seen.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if URL has been seen.</p> Source code in <code>src/ragcrawl/filters/link_filter.py</code> <pre><code>def is_seen(self, url: str) -&gt; bool:\n    \"\"\"\n    Check if URL has been seen.\n\n    Args:\n        url: The URL to check.\n\n    Returns:\n        True if URL has been seen.\n    \"\"\"\n    normalized = self.normalizer.normalize(url)\n    return normalized in self._seen_urls\n</code></pre>"},{"location":"reference/ragcrawl/filters/link_filter/#ragcrawl.filters.link_filter.LinkFilter.mark_seen","title":"<code>mark_seen(url)</code>","text":"<p>Mark a URL as seen and return normalized form.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL to mark.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The normalized URL.</p> Source code in <code>src/ragcrawl/filters/link_filter.py</code> <pre><code>def mark_seen(self, url: str) -&gt; str:\n    \"\"\"\n    Mark a URL as seen and return normalized form.\n\n    Args:\n        url: The URL to mark.\n\n    Returns:\n        The normalized URL.\n    \"\"\"\n    normalized = self.normalizer.normalize(url)\n    self._seen_urls.add(normalized)\n    return normalized\n</code></pre>"},{"location":"reference/ragcrawl/filters/patterns/","title":"Patterns","text":""},{"location":"reference/ragcrawl/filters/patterns/#ragcrawl.filters.patterns","title":"<code>patterns</code>","text":"<p>Pattern matching for URL filtering.</p>"},{"location":"reference/ragcrawl/filters/patterns/#ragcrawl.filters.patterns.ExtensionFilter","title":"<code>ExtensionFilter(blocked_extensions=None)</code>","text":"<p>Filter URLs by file extension.</p> <p>Initialize extension filter.</p> <p>Parameters:</p> Name Type Description Default <code>blocked_extensions</code> <code>list[str] | None</code> <p>List of extensions to block (e.g., ['.pdf', '.zip']).</p> <code>None</code> Source code in <code>src/ragcrawl/filters/patterns.py</code> <pre><code>def __init__(self, blocked_extensions: list[str] | None = None) -&gt; None:\n    \"\"\"\n    Initialize extension filter.\n\n    Args:\n        blocked_extensions: List of extensions to block (e.g., ['.pdf', '.zip']).\n    \"\"\"\n    self.blocked_extensions = set(\n        ext.lower() if ext.startswith(\".\") else f\".{ext.lower()}\"\n        for ext in (blocked_extensions or [])\n    )\n</code></pre>"},{"location":"reference/ragcrawl/filters/patterns/#ragcrawl.filters.patterns.ExtensionFilter.get_extension","title":"<code>get_extension(url)</code>","text":"<p>Get the extension from a URL.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL.</p> required <p>Returns:</p> Type Description <code>str | None</code> <p>The extension (e.g., '.html') or None.</p> Source code in <code>src/ragcrawl/filters/patterns.py</code> <pre><code>def get_extension(self, url: str) -&gt; str | None:\n    \"\"\"\n    Get the extension from a URL.\n\n    Args:\n        url: The URL.\n\n    Returns:\n        The extension (e.g., '.html') or None.\n    \"\"\"\n    from urllib.parse import urlparse\n\n    try:\n        path = urlparse(url).path\n        if \".\" in path:\n            ext = \".\" + path.rsplit(\".\", 1)[-1].lower()\n            # Filter out paths that look like directories\n            if \"/\" not in ext and len(ext) &lt;= 10:\n                return ext\n    except Exception:\n        pass\n\n    return None\n</code></pre>"},{"location":"reference/ragcrawl/filters/patterns/#ragcrawl.filters.patterns.ExtensionFilter.is_blocked","title":"<code>is_blocked(url)</code>","text":"<p>Check if URL has a blocked extension.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the URL has a blocked extension.</p> Source code in <code>src/ragcrawl/filters/patterns.py</code> <pre><code>def is_blocked(self, url: str) -&gt; bool:\n    \"\"\"\n    Check if URL has a blocked extension.\n\n    Args:\n        url: The URL to check.\n\n    Returns:\n        True if the URL has a blocked extension.\n    \"\"\"\n    if not self.blocked_extensions:\n        return False\n\n    # Extract path from URL\n    from urllib.parse import urlparse\n\n    try:\n        path = urlparse(url).path.lower()\n    except Exception:\n        return False\n\n    # Check extension\n    for ext in self.blocked_extensions:\n        if path.endswith(ext):\n            return True\n\n    return False\n</code></pre>"},{"location":"reference/ragcrawl/filters/patterns/#ragcrawl.filters.patterns.PatternMatcher","title":"<code>PatternMatcher(include_patterns=None, exclude_patterns=None, case_sensitive=False)</code>","text":"<p>Matches URLs against include/exclude patterns.</p> <p>Supports both regex and glob patterns.</p> <p>Initialize the pattern matcher.</p> <p>Parameters:</p> Name Type Description Default <code>include_patterns</code> <code>list[str] | None</code> <p>Patterns for URLs to include (regex or glob).</p> <code>None</code> <code>exclude_patterns</code> <code>list[str] | None</code> <p>Patterns for URLs to exclude (regex or glob).</p> <code>None</code> <code>case_sensitive</code> <code>bool</code> <p>Whether pattern matching is case-sensitive.</p> <code>False</code> Source code in <code>src/ragcrawl/filters/patterns.py</code> <pre><code>def __init__(\n    self,\n    include_patterns: list[str] | None = None,\n    exclude_patterns: list[str] | None = None,\n    case_sensitive: bool = False,\n) -&gt; None:\n    \"\"\"\n    Initialize the pattern matcher.\n\n    Args:\n        include_patterns: Patterns for URLs to include (regex or glob).\n        exclude_patterns: Patterns for URLs to exclude (regex or glob).\n        case_sensitive: Whether pattern matching is case-sensitive.\n    \"\"\"\n    self.case_sensitive = case_sensitive\n    self._include_patterns = self._compile_patterns(include_patterns or [])\n    self._exclude_patterns = self._compile_patterns(exclude_patterns or [])\n</code></pre>"},{"location":"reference/ragcrawl/filters/patterns/#ragcrawl.filters.patterns.PatternMatcher.get_match_reason","title":"<code>get_match_reason(url)</code>","text":"<p>Get the reason for inclusion/exclusion.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL to check.</p> required <p>Returns:</p> Type Description <code>str | None</code> <p>A string describing the match, or None if included by default.</p> Source code in <code>src/ragcrawl/filters/patterns.py</code> <pre><code>def get_match_reason(self, url: str) -&gt; str | None:\n    \"\"\"\n    Get the reason for inclusion/exclusion.\n\n    Args:\n        url: The URL to check.\n\n    Returns:\n        A string describing the match, or None if included by default.\n    \"\"\"\n    for pattern in self._exclude_patterns:\n        if pattern.search(url):\n            return f\"excluded by pattern: {pattern.pattern}\"\n\n    if self._include_patterns:\n        for pattern in self._include_patterns:\n            if pattern.search(url):\n                return f\"included by pattern: {pattern.pattern}\"\n\n        return \"no include pattern matched\"\n\n    return None\n</code></pre>"},{"location":"reference/ragcrawl/filters/patterns/#ragcrawl.filters.patterns.PatternMatcher.matches_exclude","title":"<code>matches_exclude(url)</code>","text":"<p>Check if URL matches any exclude pattern.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if URL matches an exclude pattern.</p> Source code in <code>src/ragcrawl/filters/patterns.py</code> <pre><code>def matches_exclude(self, url: str) -&gt; bool:\n    \"\"\"\n    Check if URL matches any exclude pattern.\n\n    Args:\n        url: The URL to check.\n\n    Returns:\n        True if URL matches an exclude pattern.\n    \"\"\"\n    if not self._exclude_patterns:\n        return False\n\n    return any(p.search(url) for p in self._exclude_patterns)\n</code></pre>"},{"location":"reference/ragcrawl/filters/patterns/#ragcrawl.filters.patterns.PatternMatcher.matches_include","title":"<code>matches_include(url)</code>","text":"<p>Check if URL matches any include pattern.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if URL matches an include pattern or no include patterns defined.</p> Source code in <code>src/ragcrawl/filters/patterns.py</code> <pre><code>def matches_include(self, url: str) -&gt; bool:\n    \"\"\"\n    Check if URL matches any include pattern.\n\n    Args:\n        url: The URL to check.\n\n    Returns:\n        True if URL matches an include pattern or no include patterns defined.\n    \"\"\"\n    if not self._include_patterns:\n        return True\n\n    return any(p.search(url) for p in self._include_patterns)\n</code></pre>"},{"location":"reference/ragcrawl/filters/patterns/#ragcrawl.filters.patterns.PatternMatcher.should_include","title":"<code>should_include(url)</code>","text":"<p>Determine if URL should be included based on patterns.</p> <p>Exclude patterns take precedence over include patterns.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if URL should be included.</p> Source code in <code>src/ragcrawl/filters/patterns.py</code> <pre><code>def should_include(self, url: str) -&gt; bool:\n    \"\"\"\n    Determine if URL should be included based on patterns.\n\n    Exclude patterns take precedence over include patterns.\n\n    Args:\n        url: The URL to check.\n\n    Returns:\n        True if URL should be included.\n    \"\"\"\n    # Exclude takes precedence\n    if self.matches_exclude(url):\n        return False\n\n    return self.matches_include(url)\n</code></pre>"},{"location":"reference/ragcrawl/filters/quality_gates/","title":"Quality gates","text":""},{"location":"reference/ragcrawl/filters/quality_gates/#ragcrawl.filters.quality_gates","title":"<code>quality_gates</code>","text":"<p>Content quality gates for filtering low-value pages.</p>"},{"location":"reference/ragcrawl/filters/quality_gates/#ragcrawl.filters.quality_gates.QualityGate","title":"<code>QualityGate(min_text_length=100, min_word_count=20, max_duplicate_ratio=0.9, block_patterns=None, detect_language=False, allowed_languages=None)</code>","text":"<p>Evaluates content quality to filter thin/low-value pages.</p> <p>Initialize quality gates.</p> <p>Parameters:</p> Name Type Description Default <code>min_text_length</code> <code>int</code> <p>Minimum text length in characters.</p> <code>100</code> <code>min_word_count</code> <code>int</code> <p>Minimum word count.</p> <code>20</code> <code>max_duplicate_ratio</code> <code>float</code> <p>Maximum ratio of duplicate content.</p> <code>0.9</code> <code>block_patterns</code> <code>list[str] | None</code> <p>URL patterns for thin/low-value pages.</p> <code>None</code> <code>detect_language</code> <code>bool</code> <p>Whether to detect language.</p> <code>False</code> <code>allowed_languages</code> <code>list[str] | None</code> <p>Allowed language codes (None = all).</p> <code>None</code> Source code in <code>src/ragcrawl/filters/quality_gates.py</code> <pre><code>def __init__(\n    self,\n    min_text_length: int = 100,\n    min_word_count: int = 20,\n    max_duplicate_ratio: float = 0.9,\n    block_patterns: list[str] | None = None,\n    detect_language: bool = False,\n    allowed_languages: list[str] | None = None,\n) -&gt; None:\n    \"\"\"\n    Initialize quality gates.\n\n    Args:\n        min_text_length: Minimum text length in characters.\n        min_word_count: Minimum word count.\n        max_duplicate_ratio: Maximum ratio of duplicate content.\n        block_patterns: URL patterns for thin/low-value pages.\n        detect_language: Whether to detect language.\n        allowed_languages: Allowed language codes (None = all).\n    \"\"\"\n    self.min_text_length = min_text_length\n    self.min_word_count = min_word_count\n    self.max_duplicate_ratio = max_duplicate_ratio\n    self.detect_language = detect_language\n    self.allowed_languages = set(allowed_languages or [])\n\n    # Compile block patterns\n    self.block_patterns = []\n    for pattern in block_patterns or []:\n        try:\n            self.block_patterns.append(re.compile(pattern, re.IGNORECASE))\n        except re.error:\n            pass\n\n    # Content hash cache for duplicate detection\n    self._content_hashes: dict[str, str] = {}\n</code></pre>"},{"location":"reference/ragcrawl/filters/quality_gates/#ragcrawl.filters.quality_gates.QualityGate.check_all","title":"<code>check_all(url, content, content_hash=None)</code>","text":"<p>Run all quality checks.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL.</p> required <code>content</code> <code>str</code> <p>The text/markdown content.</p> required <code>content_hash</code> <code>str | None</code> <p>Optional pre-computed content hash.</p> <code>None</code> <p>Returns:</p> Type Description <code>QualityResult</code> <p>QualityResult from first failing check, or passed.</p> Source code in <code>src/ragcrawl/filters/quality_gates.py</code> <pre><code>def check_all(\n    self,\n    url: str,\n    content: str,\n    content_hash: str | None = None,\n) -&gt; QualityResult:\n    \"\"\"\n    Run all quality checks.\n\n    Args:\n        url: The URL.\n        content: The text/markdown content.\n        content_hash: Optional pre-computed content hash.\n\n    Returns:\n        QualityResult from first failing check, or passed.\n    \"\"\"\n    # URL check first (fast)\n    url_result = self.check_url(url)\n    if not url_result.passed:\n        return url_result\n\n    # Content check\n    content_result = self.check_content(content, url, content_hash)\n    return content_result\n</code></pre>"},{"location":"reference/ragcrawl/filters/quality_gates/#ragcrawl.filters.quality_gates.QualityGate.check_content","title":"<code>check_content(content, url=None, content_hash=None)</code>","text":"<p>Check if content passes quality gates.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>The text/markdown content.</p> required <code>url</code> <code>str | None</code> <p>Optional URL for context.</p> <code>None</code> <code>content_hash</code> <code>str | None</code> <p>Optional pre-computed content hash.</p> <code>None</code> <p>Returns:</p> Type Description <code>QualityResult</code> <p>QualityResult.</p> Source code in <code>src/ragcrawl/filters/quality_gates.py</code> <pre><code>def check_content(\n    self,\n    content: str,\n    url: str | None = None,\n    content_hash: str | None = None,\n) -&gt; QualityResult:\n    \"\"\"\n    Check if content passes quality gates.\n\n    Args:\n        content: The text/markdown content.\n        url: Optional URL for context.\n        content_hash: Optional pre-computed content hash.\n\n    Returns:\n        QualityResult.\n    \"\"\"\n    # Length check\n    text_length = len(content)\n    if text_length &lt; self.min_text_length:\n        return QualityResult(\n            passed=False,\n            issue=QualityIssue.TOO_SHORT,\n            details=f\"Content length {text_length} &lt; {self.min_text_length}\",\n            metrics={\"text_length\": text_length},\n        )\n\n    # Word count check\n    word_count = len(content.split())\n    if word_count &lt; self.min_word_count:\n        return QualityResult(\n            passed=False,\n            issue=QualityIssue.LOW_WORD_COUNT,\n            details=f\"Word count {word_count} &lt; {self.min_word_count}\",\n            metrics={\"word_count\": word_count},\n        )\n\n    # Duplicate content check\n    if content_hash:\n        if content_hash in self._content_hashes:\n            original_url = self._content_hashes[content_hash]\n            return QualityResult(\n                passed=False,\n                issue=QualityIssue.DUPLICATE_CONTENT,\n                details=f\"Duplicate of {original_url}\",\n                metrics={\"original_url\": original_url},\n            )\n\n        if url:\n            self._content_hashes[content_hash] = url\n\n    # Language detection (optional)\n    if self.detect_language and self.allowed_languages:\n        detected_lang = self._detect_language(content)\n        if detected_lang and detected_lang not in self.allowed_languages:\n            return QualityResult(\n                passed=False,\n                issue=QualityIssue.WRONG_LANGUAGE,\n                details=f\"Language '{detected_lang}' not in allowed list\",\n                metrics={\"detected_language\": detected_lang},\n            )\n\n    return QualityResult(\n        passed=True,\n        issue=QualityIssue.PASSED,\n        metrics={\n            \"text_length\": text_length,\n            \"word_count\": word_count,\n        },\n    )\n</code></pre>"},{"location":"reference/ragcrawl/filters/quality_gates/#ragcrawl.filters.quality_gates.QualityGate.check_url","title":"<code>check_url(url)</code>","text":"<p>Check if URL matches any block patterns.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL to check.</p> required <p>Returns:</p> Type Description <code>QualityResult</code> <p>QualityResult.</p> Source code in <code>src/ragcrawl/filters/quality_gates.py</code> <pre><code>def check_url(self, url: str) -&gt; QualityResult:\n    \"\"\"\n    Check if URL matches any block patterns.\n\n    Args:\n        url: The URL to check.\n\n    Returns:\n        QualityResult.\n    \"\"\"\n    for pattern in self.block_patterns:\n        if pattern.search(url):\n            return QualityResult(\n                passed=False,\n                issue=QualityIssue.BLOCKED_PATTERN,\n                details=f\"URL matches block pattern: {pattern.pattern}\",\n            )\n\n    return QualityResult(passed=True, issue=QualityIssue.PASSED)\n</code></pre>"},{"location":"reference/ragcrawl/filters/quality_gates/#ragcrawl.filters.quality_gates.QualityGate.clear_hash_cache","title":"<code>clear_hash_cache()</code>","text":"<p>Clear the content hash cache.</p> Source code in <code>src/ragcrawl/filters/quality_gates.py</code> <pre><code>def clear_hash_cache(self) -&gt; None:\n    \"\"\"Clear the content hash cache.\"\"\"\n    self._content_hashes.clear()\n</code></pre>"},{"location":"reference/ragcrawl/filters/quality_gates/#ragcrawl.filters.quality_gates.QualityIssue","title":"<code>QualityIssue</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Types of quality issues.</p>"},{"location":"reference/ragcrawl/filters/quality_gates/#ragcrawl.filters.quality_gates.QualityResult","title":"<code>QualityResult(passed, issue, details=None, metrics=None)</code>  <code>dataclass</code>","text":"<p>Result of quality gate evaluation.</p>"},{"location":"reference/ragcrawl/filters/url_normalizer/","title":"Url normalizer","text":""},{"location":"reference/ragcrawl/filters/url_normalizer/#ragcrawl.filters.url_normalizer","title":"<code>url_normalizer</code>","text":"<p>URL normalization for deterministic deduplication.</p>"},{"location":"reference/ragcrawl/filters/url_normalizer/#ragcrawl.filters.url_normalizer.URLNormalizer","title":"<code>URLNormalizer(remove_fragments=True, normalize_trailing_slash=True, sort_query_params=True, remove_query_params=None, lowercase_hostname=True, remove_default_ports=True, remove_www=False)</code>","text":"<p>Normalizes URLs for deterministic deduplication.</p> <p>Handles: - Fragment removal - Trailing slash normalization - Query parameter sorting and filtering - Scheme normalization - Case normalization for hostname - Path normalization</p> <p>Initialize the URL normalizer.</p> <p>Parameters:</p> Name Type Description Default <code>remove_fragments</code> <code>bool</code> <p>Remove URL fragments (#...).</p> <code>True</code> <code>normalize_trailing_slash</code> <code>bool</code> <p>Ensure consistent trailing slash handling.</p> <code>True</code> <code>sort_query_params</code> <code>bool</code> <p>Sort query parameters alphabetically.</p> <code>True</code> <code>remove_query_params</code> <code>list[str] | None</code> <p>List of query params to remove (e.g., tracking params).</p> <code>None</code> <code>lowercase_hostname</code> <code>bool</code> <p>Lowercase the hostname.</p> <code>True</code> <code>remove_default_ports</code> <code>bool</code> <p>Remove default ports (80, 443).</p> <code>True</code> <code>remove_www</code> <code>bool</code> <p>Remove www. prefix from hostname.</p> <code>False</code> Source code in <code>src/ragcrawl/filters/url_normalizer.py</code> <pre><code>def __init__(\n    self,\n    remove_fragments: bool = True,\n    normalize_trailing_slash: bool = True,\n    sort_query_params: bool = True,\n    remove_query_params: list[str] | None = None,\n    lowercase_hostname: bool = True,\n    remove_default_ports: bool = True,\n    remove_www: bool = False,\n) -&gt; None:\n    \"\"\"\n    Initialize the URL normalizer.\n\n    Args:\n        remove_fragments: Remove URL fragments (#...).\n        normalize_trailing_slash: Ensure consistent trailing slash handling.\n        sort_query_params: Sort query parameters alphabetically.\n        remove_query_params: List of query params to remove (e.g., tracking params).\n        lowercase_hostname: Lowercase the hostname.\n        remove_default_ports: Remove default ports (80, 443).\n        remove_www: Remove www. prefix from hostname.\n    \"\"\"\n    self.remove_fragments = remove_fragments\n    self.normalize_trailing_slash = normalize_trailing_slash\n    self.sort_query_params = sort_query_params\n    self.remove_query_params = set(remove_query_params or [])\n    self.lowercase_hostname = lowercase_hostname\n    self.remove_default_ports = remove_default_ports\n    self.remove_www = remove_www\n\n    # Default tracking params to remove\n    self.default_tracking_params = {\n        \"utm_source\",\n        \"utm_medium\",\n        \"utm_campaign\",\n        \"utm_term\",\n        \"utm_content\",\n        \"fbclid\",\n        \"gclid\",\n        \"ref\",\n        \"source\",\n    }\n</code></pre>"},{"location":"reference/ragcrawl/filters/url_normalizer/#ragcrawl.filters.url_normalizer.URLNormalizer.get_domain","title":"<code>get_domain(url)</code>","text":"<p>Extract the domain from a URL.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The domain (e.g., 'example.com').</p> Source code in <code>src/ragcrawl/filters/url_normalizer.py</code> <pre><code>def get_domain(self, url: str) -&gt; str:\n    \"\"\"\n    Extract the domain from a URL.\n\n    Args:\n        url: The URL.\n\n    Returns:\n        The domain (e.g., 'example.com').\n    \"\"\"\n    try:\n        parsed = urlparse(url)\n        hostname = parsed.netloc.lower()\n\n        # Remove port\n        if \":\" in hostname:\n            hostname = hostname.split(\":\")[0]\n\n        return hostname\n    except Exception:\n        return \"\"\n</code></pre>"},{"location":"reference/ragcrawl/filters/url_normalizer/#ragcrawl.filters.url_normalizer.URLNormalizer.get_registered_domain","title":"<code>get_registered_domain(url)</code>","text":"<p>Extract the registered domain (eTLD+1) from a URL.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The registered domain (e.g., 'example.com' for 'sub.example.com').</p> Source code in <code>src/ragcrawl/filters/url_normalizer.py</code> <pre><code>def get_registered_domain(self, url: str) -&gt; str:\n    \"\"\"\n    Extract the registered domain (eTLD+1) from a URL.\n\n    Args:\n        url: The URL.\n\n    Returns:\n        The registered domain (e.g., 'example.com' for 'sub.example.com').\n    \"\"\"\n    try:\n        extracted = tldextract.extract(url)\n        if extracted.domain and extracted.suffix:\n            return f\"{extracted.domain}.{extracted.suffix}\"\n        return extracted.domain or \"\"\n    except Exception:\n        return \"\"\n</code></pre>"},{"location":"reference/ragcrawl/filters/url_normalizer/#ragcrawl.filters.url_normalizer.URLNormalizer.is_same_domain","title":"<code>is_same_domain(url1, url2)</code>","text":"<p>Check if two URLs are on the same domain.</p> <p>Parameters:</p> Name Type Description Default <code>url1</code> <code>str</code> <p>First URL.</p> required <code>url2</code> <code>str</code> <p>Second URL.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if same domain.</p> Source code in <code>src/ragcrawl/filters/url_normalizer.py</code> <pre><code>def is_same_domain(self, url1: str, url2: str) -&gt; bool:\n    \"\"\"\n    Check if two URLs are on the same domain.\n\n    Args:\n        url1: First URL.\n        url2: Second URL.\n\n    Returns:\n        True if same domain.\n    \"\"\"\n    return self.get_domain(url1) == self.get_domain(url2)\n</code></pre>"},{"location":"reference/ragcrawl/filters/url_normalizer/#ragcrawl.filters.url_normalizer.URLNormalizer.is_same_registered_domain","title":"<code>is_same_registered_domain(url1, url2)</code>","text":"<p>Check if two URLs are on the same registered domain.</p> <p>This considers subdomains as the same domain.</p> <p>Parameters:</p> Name Type Description Default <code>url1</code> <code>str</code> <p>First URL.</p> required <code>url2</code> <code>str</code> <p>Second URL.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if same registered domain.</p> Source code in <code>src/ragcrawl/filters/url_normalizer.py</code> <pre><code>def is_same_registered_domain(self, url1: str, url2: str) -&gt; bool:\n    \"\"\"\n    Check if two URLs are on the same registered domain.\n\n    This considers subdomains as the same domain.\n\n    Args:\n        url1: First URL.\n        url2: Second URL.\n\n    Returns:\n        True if same registered domain.\n    \"\"\"\n    return self.get_registered_domain(url1) == self.get_registered_domain(url2)\n</code></pre>"},{"location":"reference/ragcrawl/filters/url_normalizer/#ragcrawl.filters.url_normalizer.URLNormalizer.normalize","title":"<code>normalize(url)</code>","text":"<p>Normalize a URL for deduplication.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL to normalize.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The normalized URL string.</p> Source code in <code>src/ragcrawl/filters/url_normalizer.py</code> <pre><code>def normalize(self, url: str) -&gt; str:\n    \"\"\"\n    Normalize a URL for deduplication.\n\n    Args:\n        url: The URL to normalize.\n\n    Returns:\n        The normalized URL string.\n    \"\"\"\n    try:\n        parsed = urlparse(url)\n    except Exception:\n        return url\n\n    # Scheme normalization (lowercase)\n    scheme = parsed.scheme.lower()\n\n    # Hostname normalization\n    hostname = parsed.netloc\n    if self.lowercase_hostname:\n        hostname = hostname.lower()\n\n    # Remove default ports\n    if self.remove_default_ports:\n        if scheme == \"http\" and hostname.endswith(\":80\"):\n            hostname = hostname[:-3]\n        elif scheme == \"https\" and hostname.endswith(\":443\"):\n            hostname = hostname[:-4]\n\n    # Remove www prefix\n    if self.remove_www and hostname.startswith(\"www.\"):\n        hostname = hostname[4:]\n\n    # Path normalization\n    path = parsed.path\n\n    # Remove duplicate slashes\n    path = re.sub(r\"/+\", \"/\", path)\n\n    # Normalize path encoding\n    # Decode safe characters that don't need encoding\n    path = path.replace(\"%7E\", \"~\")\n\n    # Handle trailing slash\n    if self.normalize_trailing_slash:\n        # Keep trailing slash only for directories (no extension)\n        if path and not path.endswith(\"/\"):\n            # Check if it looks like a file (has extension)\n            last_segment = path.split(\"/\")[-1]\n            if \".\" not in last_segment and path != \"/\":\n                # It's a directory-like path, could add trailing slash\n                # But for consistency, we'll remove trailing slashes\n                pass\n        # Remove trailing slash except for root\n        if path != \"/\" and path.endswith(\"/\"):\n            path = path.rstrip(\"/\")\n\n    # Empty path becomes /\n    if not path:\n        path = \"/\"\n\n    # Query parameter normalization\n    query = parsed.query\n    if query:\n        params = parse_qs(query, keep_blank_values=True)\n\n        # Remove tracking and specified params\n        params_to_remove = self.remove_query_params | self.default_tracking_params\n        params = {k: v for k, v in params.items() if k not in params_to_remove}\n\n        # Sort and rebuild query string\n        if self.sort_query_params:\n            sorted_params = sorted(params.items())\n            # Flatten multi-value params\n            flat_params = []\n            for k, values in sorted_params:\n                for v in sorted(values):\n                    flat_params.append((k, v))\n            query = urlencode(flat_params)\n        else:\n            query = urlencode(params, doseq=True)\n    else:\n        query = \"\"\n\n    # Fragment handling\n    fragment = \"\" if self.remove_fragments else parsed.fragment\n\n    # Rebuild URL\n    normalized = urlunparse((scheme, hostname, path, \"\", query, fragment))\n\n    return normalized\n</code></pre>"},{"location":"reference/ragcrawl/filters/url_normalizer/#ragcrawl.filters.url_normalizer.get_domain","title":"<code>get_domain(url)</code>","text":"<p>Get the domain from a URL.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The domain.</p> Source code in <code>src/ragcrawl/filters/url_normalizer.py</code> <pre><code>def get_domain(url: str) -&gt; str:\n    \"\"\"\n    Get the domain from a URL.\n\n    Args:\n        url: The URL.\n\n    Returns:\n        The domain.\n    \"\"\"\n    return _default_normalizer.get_domain(url)\n</code></pre>"},{"location":"reference/ragcrawl/filters/url_normalizer/#ragcrawl.filters.url_normalizer.get_registered_domain","title":"<code>get_registered_domain(url)</code>","text":"<p>Get the registered domain from a URL.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The registered domain.</p> Source code in <code>src/ragcrawl/filters/url_normalizer.py</code> <pre><code>def get_registered_domain(url: str) -&gt; str:\n    \"\"\"\n    Get the registered domain from a URL.\n\n    Args:\n        url: The URL.\n\n    Returns:\n        The registered domain.\n    \"\"\"\n    return _default_normalizer.get_registered_domain(url)\n</code></pre>"},{"location":"reference/ragcrawl/filters/url_normalizer/#ragcrawl.filters.url_normalizer.normalize_url","title":"<code>normalize_url(url)</code>","text":"<p>Normalize a URL using default settings.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL to normalize.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The normalized URL.</p> Source code in <code>src/ragcrawl/filters/url_normalizer.py</code> <pre><code>def normalize_url(url: str) -&gt; str:\n    \"\"\"\n    Normalize a URL using default settings.\n\n    Args:\n        url: The URL to normalize.\n\n    Returns:\n        The normalized URL.\n    \"\"\"\n    return _default_normalizer.normalize(url)\n</code></pre>"},{"location":"reference/ragcrawl/hooks/","title":"Index","text":""},{"location":"reference/ragcrawl/hooks/#ragcrawl.hooks","title":"<code>hooks</code>","text":"<p>Hooks and callbacks for ragcrawl.</p>"},{"location":"reference/ragcrawl/hooks/#ragcrawl.hooks.HookManager","title":"<code>HookManager()</code>","text":"<p>Manages hooks/callbacks for crawl events.</p> <p>Initialize hook manager.</p> Source code in <code>src/ragcrawl/hooks/callbacks.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize hook manager.\"\"\"\n    self._on_page_hooks: list[OnPageCallback] = []\n    self._on_error_hooks: list[OnErrorCallback] = []\n    self._on_change_hooks: list[OnChangeCallback] = []\n    self._redaction_hook: RedactionHook | None = None\n</code></pre>"},{"location":"reference/ragcrawl/hooks/#ragcrawl.hooks.HookManager.apply_redaction","title":"<code>apply_redaction(content)</code>","text":"<p>Apply redaction hook to content.</p> Source code in <code>src/ragcrawl/hooks/callbacks.py</code> <pre><code>def apply_redaction(self, content: str) -&gt; str:\n    \"\"\"Apply redaction hook to content.\"\"\"\n    if self._redaction_hook is None:\n        return content\n\n    try:\n        return self._redaction_hook(content)\n    except Exception as e:\n        logger.warning(\"Redaction hook error\", error=str(e))\n        return content\n</code></pre>"},{"location":"reference/ragcrawl/hooks/#ragcrawl.hooks.HookManager.register_on_change","title":"<code>register_on_change(callback)</code>","text":"<p>Register an on_change_detected callback.</p> Source code in <code>src/ragcrawl/hooks/callbacks.py</code> <pre><code>def register_on_change(self, callback: OnChangeCallback) -&gt; None:\n    \"\"\"Register an on_change_detected callback.\"\"\"\n    self._on_change_hooks.append(callback)\n</code></pre>"},{"location":"reference/ragcrawl/hooks/#ragcrawl.hooks.HookManager.register_on_error","title":"<code>register_on_error(callback)</code>","text":"<p>Register an on_error callback.</p> Source code in <code>src/ragcrawl/hooks/callbacks.py</code> <pre><code>def register_on_error(self, callback: OnErrorCallback) -&gt; None:\n    \"\"\"Register an on_error callback.\"\"\"\n    self._on_error_hooks.append(callback)\n</code></pre>"},{"location":"reference/ragcrawl/hooks/#ragcrawl.hooks.HookManager.register_on_page","title":"<code>register_on_page(callback)</code>","text":"<p>Register an on_page callback.</p> Source code in <code>src/ragcrawl/hooks/callbacks.py</code> <pre><code>def register_on_page(self, callback: OnPageCallback) -&gt; None:\n    \"\"\"Register an on_page callback.\"\"\"\n    self._on_page_hooks.append(callback)\n</code></pre>"},{"location":"reference/ragcrawl/hooks/#ragcrawl.hooks.HookManager.set_redaction_hook","title":"<code>set_redaction_hook(hook)</code>","text":"<p>Set the redaction hook.</p> Source code in <code>src/ragcrawl/hooks/callbacks.py</code> <pre><code>def set_redaction_hook(self, hook: RedactionHook) -&gt; None:\n    \"\"\"Set the redaction hook.\"\"\"\n    self._redaction_hook = hook\n</code></pre>"},{"location":"reference/ragcrawl/hooks/#ragcrawl.hooks.HookManager.trigger_on_change","title":"<code>trigger_on_change(document, previous_page)</code>","text":"<p>Trigger on_change_detected hooks.</p> Source code in <code>src/ragcrawl/hooks/callbacks.py</code> <pre><code>def trigger_on_change(\n    self, document: Document, previous_page: Page | None\n) -&gt; None:\n    \"\"\"Trigger on_change_detected hooks.\"\"\"\n    for hook in self._on_change_hooks:\n        try:\n            hook(document, previous_page)\n        except Exception as e:\n            logger.warning(\"on_change hook error\", error=str(e))\n</code></pre>"},{"location":"reference/ragcrawl/hooks/#ragcrawl.hooks.HookManager.trigger_on_error","title":"<code>trigger_on_error(url, error)</code>","text":"<p>Trigger on_error hooks.</p> Source code in <code>src/ragcrawl/hooks/callbacks.py</code> <pre><code>def trigger_on_error(self, url: str, error: Exception) -&gt; None:\n    \"\"\"Trigger on_error hooks.\"\"\"\n    for hook in self._on_error_hooks:\n        try:\n            hook(url, error)\n        except Exception as e:\n            logger.warning(\"on_error hook error\", error=str(e))\n</code></pre>"},{"location":"reference/ragcrawl/hooks/#ragcrawl.hooks.HookManager.trigger_on_page","title":"<code>trigger_on_page(document)</code>","text":"<p>Trigger on_page hooks.</p> Source code in <code>src/ragcrawl/hooks/callbacks.py</code> <pre><code>def trigger_on_page(self, document: Document) -&gt; None:\n    \"\"\"Trigger on_page hooks.\"\"\"\n    for hook in self._on_page_hooks:\n        try:\n            hook(document)\n        except Exception as e:\n            logger.warning(\"on_page hook error\", error=str(e))\n</code></pre>"},{"location":"reference/ragcrawl/hooks/#ragcrawl.hooks.OnChangeCallback","title":"<code>OnChangeCallback</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for on_change_detected callbacks.</p>"},{"location":"reference/ragcrawl/hooks/#ragcrawl.hooks.OnChangeCallback.__call__","title":"<code>__call__(document, previous_page)</code>","text":"<p>Called when content change is detected.</p> Source code in <code>src/ragcrawl/hooks/callbacks.py</code> <pre><code>def __call__(self, document: Document, previous_page: Page | None) -&gt; None:\n    \"\"\"Called when content change is detected.\"\"\"\n    ...\n</code></pre>"},{"location":"reference/ragcrawl/hooks/#ragcrawl.hooks.OnErrorCallback","title":"<code>OnErrorCallback</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for on_error callbacks.</p>"},{"location":"reference/ragcrawl/hooks/#ragcrawl.hooks.OnErrorCallback.__call__","title":"<code>__call__(url, error)</code>","text":"<p>Called when an error occurs during crawling.</p> Source code in <code>src/ragcrawl/hooks/callbacks.py</code> <pre><code>def __call__(self, url: str, error: Exception) -&gt; None:\n    \"\"\"Called when an error occurs during crawling.\"\"\"\n    ...\n</code></pre>"},{"location":"reference/ragcrawl/hooks/#ragcrawl.hooks.OnPageCallback","title":"<code>OnPageCallback</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for on_page callbacks.</p>"},{"location":"reference/ragcrawl/hooks/#ragcrawl.hooks.OnPageCallback.__call__","title":"<code>__call__(document)</code>","text":"<p>Called when a page is successfully crawled.</p> Source code in <code>src/ragcrawl/hooks/callbacks.py</code> <pre><code>def __call__(self, document: Document) -&gt; None:\n    \"\"\"Called when a page is successfully crawled.\"\"\"\n    ...\n</code></pre>"},{"location":"reference/ragcrawl/hooks/#ragcrawl.hooks.RedactionHook","title":"<code>RedactionHook</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for redaction hooks.</p>"},{"location":"reference/ragcrawl/hooks/#ragcrawl.hooks.RedactionHook.__call__","title":"<code>__call__(content)</code>","text":"<p>Redact sensitive content before persistence.</p> Source code in <code>src/ragcrawl/hooks/callbacks.py</code> <pre><code>def __call__(self, content: str) -&gt; str:\n    \"\"\"Redact sensitive content before persistence.\"\"\"\n    ...\n</code></pre>"},{"location":"reference/ragcrawl/hooks/callbacks/","title":"Callbacks","text":""},{"location":"reference/ragcrawl/hooks/callbacks/#ragcrawl.hooks.callbacks","title":"<code>callbacks</code>","text":"<p>Callback definitions and hook manager.</p>"},{"location":"reference/ragcrawl/hooks/callbacks/#ragcrawl.hooks.callbacks.HookManager","title":"<code>HookManager()</code>","text":"<p>Manages hooks/callbacks for crawl events.</p> <p>Initialize hook manager.</p> Source code in <code>src/ragcrawl/hooks/callbacks.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize hook manager.\"\"\"\n    self._on_page_hooks: list[OnPageCallback] = []\n    self._on_error_hooks: list[OnErrorCallback] = []\n    self._on_change_hooks: list[OnChangeCallback] = []\n    self._redaction_hook: RedactionHook | None = None\n</code></pre>"},{"location":"reference/ragcrawl/hooks/callbacks/#ragcrawl.hooks.callbacks.HookManager.apply_redaction","title":"<code>apply_redaction(content)</code>","text":"<p>Apply redaction hook to content.</p> Source code in <code>src/ragcrawl/hooks/callbacks.py</code> <pre><code>def apply_redaction(self, content: str) -&gt; str:\n    \"\"\"Apply redaction hook to content.\"\"\"\n    if self._redaction_hook is None:\n        return content\n\n    try:\n        return self._redaction_hook(content)\n    except Exception as e:\n        logger.warning(\"Redaction hook error\", error=str(e))\n        return content\n</code></pre>"},{"location":"reference/ragcrawl/hooks/callbacks/#ragcrawl.hooks.callbacks.HookManager.register_on_change","title":"<code>register_on_change(callback)</code>","text":"<p>Register an on_change_detected callback.</p> Source code in <code>src/ragcrawl/hooks/callbacks.py</code> <pre><code>def register_on_change(self, callback: OnChangeCallback) -&gt; None:\n    \"\"\"Register an on_change_detected callback.\"\"\"\n    self._on_change_hooks.append(callback)\n</code></pre>"},{"location":"reference/ragcrawl/hooks/callbacks/#ragcrawl.hooks.callbacks.HookManager.register_on_error","title":"<code>register_on_error(callback)</code>","text":"<p>Register an on_error callback.</p> Source code in <code>src/ragcrawl/hooks/callbacks.py</code> <pre><code>def register_on_error(self, callback: OnErrorCallback) -&gt; None:\n    \"\"\"Register an on_error callback.\"\"\"\n    self._on_error_hooks.append(callback)\n</code></pre>"},{"location":"reference/ragcrawl/hooks/callbacks/#ragcrawl.hooks.callbacks.HookManager.register_on_page","title":"<code>register_on_page(callback)</code>","text":"<p>Register an on_page callback.</p> Source code in <code>src/ragcrawl/hooks/callbacks.py</code> <pre><code>def register_on_page(self, callback: OnPageCallback) -&gt; None:\n    \"\"\"Register an on_page callback.\"\"\"\n    self._on_page_hooks.append(callback)\n</code></pre>"},{"location":"reference/ragcrawl/hooks/callbacks/#ragcrawl.hooks.callbacks.HookManager.set_redaction_hook","title":"<code>set_redaction_hook(hook)</code>","text":"<p>Set the redaction hook.</p> Source code in <code>src/ragcrawl/hooks/callbacks.py</code> <pre><code>def set_redaction_hook(self, hook: RedactionHook) -&gt; None:\n    \"\"\"Set the redaction hook.\"\"\"\n    self._redaction_hook = hook\n</code></pre>"},{"location":"reference/ragcrawl/hooks/callbacks/#ragcrawl.hooks.callbacks.HookManager.trigger_on_change","title":"<code>trigger_on_change(document, previous_page)</code>","text":"<p>Trigger on_change_detected hooks.</p> Source code in <code>src/ragcrawl/hooks/callbacks.py</code> <pre><code>def trigger_on_change(\n    self, document: Document, previous_page: Page | None\n) -&gt; None:\n    \"\"\"Trigger on_change_detected hooks.\"\"\"\n    for hook in self._on_change_hooks:\n        try:\n            hook(document, previous_page)\n        except Exception as e:\n            logger.warning(\"on_change hook error\", error=str(e))\n</code></pre>"},{"location":"reference/ragcrawl/hooks/callbacks/#ragcrawl.hooks.callbacks.HookManager.trigger_on_error","title":"<code>trigger_on_error(url, error)</code>","text":"<p>Trigger on_error hooks.</p> Source code in <code>src/ragcrawl/hooks/callbacks.py</code> <pre><code>def trigger_on_error(self, url: str, error: Exception) -&gt; None:\n    \"\"\"Trigger on_error hooks.\"\"\"\n    for hook in self._on_error_hooks:\n        try:\n            hook(url, error)\n        except Exception as e:\n            logger.warning(\"on_error hook error\", error=str(e))\n</code></pre>"},{"location":"reference/ragcrawl/hooks/callbacks/#ragcrawl.hooks.callbacks.HookManager.trigger_on_page","title":"<code>trigger_on_page(document)</code>","text":"<p>Trigger on_page hooks.</p> Source code in <code>src/ragcrawl/hooks/callbacks.py</code> <pre><code>def trigger_on_page(self, document: Document) -&gt; None:\n    \"\"\"Trigger on_page hooks.\"\"\"\n    for hook in self._on_page_hooks:\n        try:\n            hook(document)\n        except Exception as e:\n            logger.warning(\"on_page hook error\", error=str(e))\n</code></pre>"},{"location":"reference/ragcrawl/hooks/callbacks/#ragcrawl.hooks.callbacks.OnChangeCallback","title":"<code>OnChangeCallback</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for on_change_detected callbacks.</p>"},{"location":"reference/ragcrawl/hooks/callbacks/#ragcrawl.hooks.callbacks.OnChangeCallback.__call__","title":"<code>__call__(document, previous_page)</code>","text":"<p>Called when content change is detected.</p> Source code in <code>src/ragcrawl/hooks/callbacks.py</code> <pre><code>def __call__(self, document: Document, previous_page: Page | None) -&gt; None:\n    \"\"\"Called when content change is detected.\"\"\"\n    ...\n</code></pre>"},{"location":"reference/ragcrawl/hooks/callbacks/#ragcrawl.hooks.callbacks.OnErrorCallback","title":"<code>OnErrorCallback</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for on_error callbacks.</p>"},{"location":"reference/ragcrawl/hooks/callbacks/#ragcrawl.hooks.callbacks.OnErrorCallback.__call__","title":"<code>__call__(url, error)</code>","text":"<p>Called when an error occurs during crawling.</p> Source code in <code>src/ragcrawl/hooks/callbacks.py</code> <pre><code>def __call__(self, url: str, error: Exception) -&gt; None:\n    \"\"\"Called when an error occurs during crawling.\"\"\"\n    ...\n</code></pre>"},{"location":"reference/ragcrawl/hooks/callbacks/#ragcrawl.hooks.callbacks.OnPageCallback","title":"<code>OnPageCallback</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for on_page callbacks.</p>"},{"location":"reference/ragcrawl/hooks/callbacks/#ragcrawl.hooks.callbacks.OnPageCallback.__call__","title":"<code>__call__(document)</code>","text":"<p>Called when a page is successfully crawled.</p> Source code in <code>src/ragcrawl/hooks/callbacks.py</code> <pre><code>def __call__(self, document: Document) -&gt; None:\n    \"\"\"Called when a page is successfully crawled.\"\"\"\n    ...\n</code></pre>"},{"location":"reference/ragcrawl/hooks/callbacks/#ragcrawl.hooks.callbacks.PatternRedactor","title":"<code>PatternRedactor(patterns=None, redact_emails=True, redact_phone_numbers=True, redact_ssn=True, redact_credit_cards=True)</code>","text":"<p>Redacts content based on regex patterns.</p> <p>Initialize pattern redactor.</p> <p>Parameters:</p> Name Type Description Default <code>patterns</code> <code>list[tuple[str, str]] | None</code> <p>List of (pattern, replacement) tuples.</p> <code>None</code> <code>redact_emails</code> <code>bool</code> <p>Redact email addresses.</p> <code>True</code> <code>redact_phone_numbers</code> <code>bool</code> <p>Redact phone numbers.</p> <code>True</code> <code>redact_ssn</code> <code>bool</code> <p>Redact SSN patterns.</p> <code>True</code> <code>redact_credit_cards</code> <code>bool</code> <p>Redact credit card numbers.</p> <code>True</code> Source code in <code>src/ragcrawl/hooks/callbacks.py</code> <pre><code>def __init__(\n    self,\n    patterns: list[tuple[str, str]] | None = None,\n    redact_emails: bool = True,\n    redact_phone_numbers: bool = True,\n    redact_ssn: bool = True,\n    redact_credit_cards: bool = True,\n) -&gt; None:\n    \"\"\"\n    Initialize pattern redactor.\n\n    Args:\n        patterns: List of (pattern, replacement) tuples.\n        redact_emails: Redact email addresses.\n        redact_phone_numbers: Redact phone numbers.\n        redact_ssn: Redact SSN patterns.\n        redact_credit_cards: Redact credit card numbers.\n    \"\"\"\n    self.patterns: list[tuple[re.Pattern[str], str]] = []\n\n    # Add custom patterns\n    if patterns:\n        for pattern, replacement in patterns:\n            try:\n                self.patterns.append((re.compile(pattern), replacement))\n            except re.error:\n                pass\n\n    # Add built-in patterns\n    if redact_emails:\n        self.patterns.append((\n            re.compile(r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b\"),\n            \"[EMAIL REDACTED]\",\n        ))\n\n    if redact_phone_numbers:\n        self.patterns.append((\n            re.compile(r\"\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b\"),\n            \"[PHONE REDACTED]\",\n        ))\n\n    if redact_ssn:\n        self.patterns.append((\n            re.compile(r\"\\b\\d{3}-\\d{2}-\\d{4}\\b\"),\n            \"[SSN REDACTED]\",\n        ))\n\n    if redact_credit_cards:\n        self.patterns.append((\n            re.compile(r\"\\b\\d{4}[-\\s]?\\d{4}[-\\s]?\\d{4}[-\\s]?\\d{4}\\b\"),\n            \"[CARD REDACTED]\",\n        ))\n</code></pre>"},{"location":"reference/ragcrawl/hooks/callbacks/#ragcrawl.hooks.callbacks.PatternRedactor.__call__","title":"<code>__call__(content)</code>","text":"<p>Apply redaction patterns to content.</p> Source code in <code>src/ragcrawl/hooks/callbacks.py</code> <pre><code>def __call__(self, content: str) -&gt; str:\n    \"\"\"Apply redaction patterns to content.\"\"\"\n    result = content\n    for pattern, replacement in self.patterns:\n        result = pattern.sub(replacement, result)\n    return result\n</code></pre>"},{"location":"reference/ragcrawl/hooks/callbacks/#ragcrawl.hooks.callbacks.RedactionHook","title":"<code>RedactionHook</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for redaction hooks.</p>"},{"location":"reference/ragcrawl/hooks/callbacks/#ragcrawl.hooks.callbacks.RedactionHook.__call__","title":"<code>__call__(content)</code>","text":"<p>Redact sensitive content before persistence.</p> Source code in <code>src/ragcrawl/hooks/callbacks.py</code> <pre><code>def __call__(self, content: str) -&gt; str:\n    \"\"\"Redact sensitive content before persistence.\"\"\"\n    ...\n</code></pre>"},{"location":"reference/ragcrawl/models/","title":"Index","text":""},{"location":"reference/ragcrawl/models/#ragcrawl.models","title":"<code>models</code>","text":"<p>Data models for ragcrawl.</p>"},{"location":"reference/ragcrawl/models/#ragcrawl.models.Chunk","title":"<code>Chunk</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a chunk of content for RAG/embedding pipelines.</p> <p>Chunks are segments of a document optimized for vector embedding and retrieval, with metadata for context reconstruction.</p>"},{"location":"reference/ragcrawl/models/#ragcrawl.models.Chunk.is_first","title":"<code>is_first</code>  <code>property</code>","text":"<p>Check if this is the first chunk.</p>"},{"location":"reference/ragcrawl/models/#ragcrawl.models.Chunk.is_last","title":"<code>is_last</code>  <code>property</code>","text":"<p>Check if this is the last chunk.</p>"},{"location":"reference/ragcrawl/models/#ragcrawl.models.CrawlRun","title":"<code>CrawlRun</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a single crawl or sync execution.</p> <p>Tracks the status, configuration snapshot, and statistics for a crawl run.</p>"},{"location":"reference/ragcrawl/models/#ragcrawl.models.CrawlRun.duration_seconds","title":"<code>duration_seconds</code>  <code>property</code>","text":"<p>Get run duration in seconds.</p>"},{"location":"reference/ragcrawl/models/#ragcrawl.models.CrawlRun.mark_cancelled","title":"<code>mark_cancelled()</code>","text":"<p>Mark the run as cancelled.</p> Source code in <code>src/ragcrawl/models/crawl_run.py</code> <pre><code>def mark_cancelled(self) -&gt; None:\n    \"\"\"Mark the run as cancelled.\"\"\"\n    self.status = RunStatus.CANCELLED\n    self.completed_at = datetime.now()\n</code></pre>"},{"location":"reference/ragcrawl/models/#ragcrawl.models.CrawlRun.mark_completed","title":"<code>mark_completed(partial=False)</code>","text":"<p>Mark the run as completed.</p> Source code in <code>src/ragcrawl/models/crawl_run.py</code> <pre><code>def mark_completed(self, partial: bool = False) -&gt; None:\n    \"\"\"Mark the run as completed.\"\"\"\n    self.status = RunStatus.PARTIAL if partial else RunStatus.COMPLETED\n    self.completed_at = datetime.now()\n</code></pre>"},{"location":"reference/ragcrawl/models/#ragcrawl.models.CrawlRun.mark_failed","title":"<code>mark_failed(error)</code>","text":"<p>Mark the run as failed.</p> Source code in <code>src/ragcrawl/models/crawl_run.py</code> <pre><code>def mark_failed(self, error: str) -&gt; None:\n    \"\"\"Mark the run as failed.\"\"\"\n    self.status = RunStatus.FAILED\n    self.error_message = error\n    self.completed_at = datetime.now()\n</code></pre>"},{"location":"reference/ragcrawl/models/#ragcrawl.models.CrawlRun.mark_started","title":"<code>mark_started()</code>","text":"<p>Mark the run as started.</p> Source code in <code>src/ragcrawl/models/crawl_run.py</code> <pre><code>def mark_started(self) -&gt; None:\n    \"\"\"Mark the run as started.\"\"\"\n    self.status = RunStatus.RUNNING\n    self.started_at = datetime.now()\n</code></pre>"},{"location":"reference/ragcrawl/models/#ragcrawl.models.Document","title":"<code>Document</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A crawled document with rich metadata for LLM/RAG consumption.</p> <p>This is the primary output model containing all extracted content and metadata from a crawled page.</p>"},{"location":"reference/ragcrawl/models/#ragcrawl.models.FrontierItem","title":"<code>FrontierItem</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a URL in the crawl frontier queue.</p> <p>Used for pause/resume functionality and tracking crawl progress.</p>"},{"location":"reference/ragcrawl/models/#ragcrawl.models.FrontierItem.mark_completed","title":"<code>mark_completed()</code>","text":"<p>Mark item as successfully crawled.</p> Source code in <code>src/ragcrawl/models/frontier_item.py</code> <pre><code>def mark_completed(self) -&gt; None:\n    \"\"\"Mark item as successfully crawled.\"\"\"\n    self.status = FrontierStatus.COMPLETED\n    self.completed_at = datetime.now()\n</code></pre>"},{"location":"reference/ragcrawl/models/#ragcrawl.models.FrontierItem.mark_failed","title":"<code>mark_failed(error)</code>","text":"<p>Mark item as failed.</p> Source code in <code>src/ragcrawl/models/frontier_item.py</code> <pre><code>def mark_failed(self, error: str) -&gt; None:\n    \"\"\"Mark item as failed.\"\"\"\n    self.status = FrontierStatus.FAILED\n    self.last_error = error\n    self.retry_count += 1\n    self.completed_at = datetime.now()\n</code></pre>"},{"location":"reference/ragcrawl/models/#ragcrawl.models.FrontierItem.mark_in_progress","title":"<code>mark_in_progress()</code>","text":"<p>Mark item as being crawled.</p> Source code in <code>src/ragcrawl/models/frontier_item.py</code> <pre><code>def mark_in_progress(self) -&gt; None:\n    \"\"\"Mark item as being crawled.\"\"\"\n    self.status = FrontierStatus.IN_PROGRESS\n    self.started_at = datetime.now()\n</code></pre>"},{"location":"reference/ragcrawl/models/#ragcrawl.models.FrontierItem.mark_skipped","title":"<code>mark_skipped(reason)</code>","text":"<p>Mark item as skipped.</p> Source code in <code>src/ragcrawl/models/frontier_item.py</code> <pre><code>def mark_skipped(self, reason: str) -&gt; None:\n    \"\"\"Mark item as skipped.\"\"\"\n    self.status = FrontierStatus.SKIPPED\n    self.last_error = reason\n    self.completed_at = datetime.now()\n</code></pre>"},{"location":"reference/ragcrawl/models/#ragcrawl.models.FrontierStatus","title":"<code>FrontierStatus</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Status of a frontier item.</p>"},{"location":"reference/ragcrawl/models/#ragcrawl.models.Page","title":"<code>Page</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents the current state of a URL in the crawl database.</p> <p>This model tracks freshness information and points to the current version of the page content. It's used for incremental sync to determine what needs re-crawling.</p>"},{"location":"reference/ragcrawl/models/#ragcrawl.models.Page.needs_recrawl","title":"<code>needs_recrawl(max_age_hours=None, force=False)</code>","text":"<p>Determine if this page needs to be re-crawled.</p> <p>Parameters:</p> Name Type Description Default <code>max_age_hours</code> <code>float | None</code> <p>Maximum age in hours before recrawl. None means always recrawl.</p> <code>None</code> <code>force</code> <code>bool</code> <p>If True, always return True.</p> <code>False</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the page should be re-crawled.</p> Source code in <code>src/ragcrawl/models/page.py</code> <pre><code>def needs_recrawl(\n    self,\n    max_age_hours: float | None = None,\n    force: bool = False,\n) -&gt; bool:\n    \"\"\"\n    Determine if this page needs to be re-crawled.\n\n    Args:\n        max_age_hours: Maximum age in hours before recrawl. None means always recrawl.\n        force: If True, always return True.\n\n    Returns:\n        True if the page should be re-crawled.\n    \"\"\"\n    if force:\n        return True\n\n    if self.is_tombstone:\n        return False\n\n    if self.last_crawled is None:\n        return True\n\n    if max_age_hours is None:\n        return True\n\n    age = datetime.now() - self.last_crawled\n    return age.total_seconds() / 3600 &gt; max_age_hours\n</code></pre>"},{"location":"reference/ragcrawl/models/#ragcrawl.models.PageVersion","title":"<code>PageVersion</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a specific version of a page's content.</p> <p>Each time content changes, a new PageVersion is created. This enables version history and change tracking for KB updates.</p>"},{"location":"reference/ragcrawl/models/#ragcrawl.models.RunStatus","title":"<code>RunStatus</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Status of a crawl run.</p>"},{"location":"reference/ragcrawl/models/#ragcrawl.models.Site","title":"<code>Site</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a website/crawl target with its configuration.</p> <p>Stores the configuration snapshot and metadata for a crawl target.</p>"},{"location":"reference/ragcrawl/models/chunk/","title":"Chunk","text":""},{"location":"reference/ragcrawl/models/chunk/#ragcrawl.models.chunk","title":"<code>chunk</code>","text":"<p>Chunk model for RAG-ready content segmentation.</p>"},{"location":"reference/ragcrawl/models/chunk/#ragcrawl.models.chunk.Chunk","title":"<code>Chunk</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a chunk of content for RAG/embedding pipelines.</p> <p>Chunks are segments of a document optimized for vector embedding and retrieval, with metadata for context reconstruction.</p>"},{"location":"reference/ragcrawl/models/chunk/#ragcrawl.models.chunk.Chunk.is_first","title":"<code>is_first</code>  <code>property</code>","text":"<p>Check if this is the first chunk.</p>"},{"location":"reference/ragcrawl/models/chunk/#ragcrawl.models.chunk.Chunk.is_last","title":"<code>is_last</code>  <code>property</code>","text":"<p>Check if this is the last chunk.</p>"},{"location":"reference/ragcrawl/models/crawl_run/","title":"Crawl run","text":""},{"location":"reference/ragcrawl/models/crawl_run/#ragcrawl.models.crawl_run","title":"<code>crawl_run</code>","text":"<p>CrawlRun model representing a single crawl execution.</p>"},{"location":"reference/ragcrawl/models/crawl_run/#ragcrawl.models.crawl_run.CrawlRun","title":"<code>CrawlRun</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a single crawl or sync execution.</p> <p>Tracks the status, configuration snapshot, and statistics for a crawl run.</p>"},{"location":"reference/ragcrawl/models/crawl_run/#ragcrawl.models.crawl_run.CrawlRun.duration_seconds","title":"<code>duration_seconds</code>  <code>property</code>","text":"<p>Get run duration in seconds.</p>"},{"location":"reference/ragcrawl/models/crawl_run/#ragcrawl.models.crawl_run.CrawlRun.mark_cancelled","title":"<code>mark_cancelled()</code>","text":"<p>Mark the run as cancelled.</p> Source code in <code>src/ragcrawl/models/crawl_run.py</code> <pre><code>def mark_cancelled(self) -&gt; None:\n    \"\"\"Mark the run as cancelled.\"\"\"\n    self.status = RunStatus.CANCELLED\n    self.completed_at = datetime.now()\n</code></pre>"},{"location":"reference/ragcrawl/models/crawl_run/#ragcrawl.models.crawl_run.CrawlRun.mark_completed","title":"<code>mark_completed(partial=False)</code>","text":"<p>Mark the run as completed.</p> Source code in <code>src/ragcrawl/models/crawl_run.py</code> <pre><code>def mark_completed(self, partial: bool = False) -&gt; None:\n    \"\"\"Mark the run as completed.\"\"\"\n    self.status = RunStatus.PARTIAL if partial else RunStatus.COMPLETED\n    self.completed_at = datetime.now()\n</code></pre>"},{"location":"reference/ragcrawl/models/crawl_run/#ragcrawl.models.crawl_run.CrawlRun.mark_failed","title":"<code>mark_failed(error)</code>","text":"<p>Mark the run as failed.</p> Source code in <code>src/ragcrawl/models/crawl_run.py</code> <pre><code>def mark_failed(self, error: str) -&gt; None:\n    \"\"\"Mark the run as failed.\"\"\"\n    self.status = RunStatus.FAILED\n    self.error_message = error\n    self.completed_at = datetime.now()\n</code></pre>"},{"location":"reference/ragcrawl/models/crawl_run/#ragcrawl.models.crawl_run.CrawlRun.mark_started","title":"<code>mark_started()</code>","text":"<p>Mark the run as started.</p> Source code in <code>src/ragcrawl/models/crawl_run.py</code> <pre><code>def mark_started(self) -&gt; None:\n    \"\"\"Mark the run as started.\"\"\"\n    self.status = RunStatus.RUNNING\n    self.started_at = datetime.now()\n</code></pre>"},{"location":"reference/ragcrawl/models/crawl_run/#ragcrawl.models.crawl_run.CrawlStats","title":"<code>CrawlStats</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Statistics for a crawl run.</p>"},{"location":"reference/ragcrawl/models/crawl_run/#ragcrawl.models.crawl_run.RunStatus","title":"<code>RunStatus</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Status of a crawl run.</p>"},{"location":"reference/ragcrawl/models/document/","title":"Document","text":""},{"location":"reference/ragcrawl/models/document/#ragcrawl.models.document","title":"<code>document</code>","text":"<p>Document model representing a crawled page with rich metadata.</p>"},{"location":"reference/ragcrawl/models/document/#ragcrawl.models.document.Document","title":"<code>Document</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A crawled document with rich metadata for LLM/RAG consumption.</p> <p>This is the primary output model containing all extracted content and metadata from a crawled page.</p>"},{"location":"reference/ragcrawl/models/document/#ragcrawl.models.document.DocumentDiagnostics","title":"<code>DocumentDiagnostics</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Diagnostic information from crawling/extraction.</p>"},{"location":"reference/ragcrawl/models/document/#ragcrawl.models.document.HeadingInfo","title":"<code>HeadingInfo</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Information about a heading in the document.</p>"},{"location":"reference/ragcrawl/models/frontier_item/","title":"Frontier item","text":""},{"location":"reference/ragcrawl/models/frontier_item/#ragcrawl.models.frontier_item","title":"<code>frontier_item</code>","text":"<p>FrontierItem model for crawl queue state.</p>"},{"location":"reference/ragcrawl/models/frontier_item/#ragcrawl.models.frontier_item.FrontierItem","title":"<code>FrontierItem</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a URL in the crawl frontier queue.</p> <p>Used for pause/resume functionality and tracking crawl progress.</p>"},{"location":"reference/ragcrawl/models/frontier_item/#ragcrawl.models.frontier_item.FrontierItem.mark_completed","title":"<code>mark_completed()</code>","text":"<p>Mark item as successfully crawled.</p> Source code in <code>src/ragcrawl/models/frontier_item.py</code> <pre><code>def mark_completed(self) -&gt; None:\n    \"\"\"Mark item as successfully crawled.\"\"\"\n    self.status = FrontierStatus.COMPLETED\n    self.completed_at = datetime.now()\n</code></pre>"},{"location":"reference/ragcrawl/models/frontier_item/#ragcrawl.models.frontier_item.FrontierItem.mark_failed","title":"<code>mark_failed(error)</code>","text":"<p>Mark item as failed.</p> Source code in <code>src/ragcrawl/models/frontier_item.py</code> <pre><code>def mark_failed(self, error: str) -&gt; None:\n    \"\"\"Mark item as failed.\"\"\"\n    self.status = FrontierStatus.FAILED\n    self.last_error = error\n    self.retry_count += 1\n    self.completed_at = datetime.now()\n</code></pre>"},{"location":"reference/ragcrawl/models/frontier_item/#ragcrawl.models.frontier_item.FrontierItem.mark_in_progress","title":"<code>mark_in_progress()</code>","text":"<p>Mark item as being crawled.</p> Source code in <code>src/ragcrawl/models/frontier_item.py</code> <pre><code>def mark_in_progress(self) -&gt; None:\n    \"\"\"Mark item as being crawled.\"\"\"\n    self.status = FrontierStatus.IN_PROGRESS\n    self.started_at = datetime.now()\n</code></pre>"},{"location":"reference/ragcrawl/models/frontier_item/#ragcrawl.models.frontier_item.FrontierItem.mark_skipped","title":"<code>mark_skipped(reason)</code>","text":"<p>Mark item as skipped.</p> Source code in <code>src/ragcrawl/models/frontier_item.py</code> <pre><code>def mark_skipped(self, reason: str) -&gt; None:\n    \"\"\"Mark item as skipped.\"\"\"\n    self.status = FrontierStatus.SKIPPED\n    self.last_error = reason\n    self.completed_at = datetime.now()\n</code></pre>"},{"location":"reference/ragcrawl/models/frontier_item/#ragcrawl.models.frontier_item.FrontierStatus","title":"<code>FrontierStatus</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Status of a frontier item.</p>"},{"location":"reference/ragcrawl/models/page/","title":"Page","text":""},{"location":"reference/ragcrawl/models/page/#ragcrawl.models.page","title":"<code>page</code>","text":"<p>Page model representing the current state of a crawled URL.</p>"},{"location":"reference/ragcrawl/models/page/#ragcrawl.models.page.Page","title":"<code>Page</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents the current state of a URL in the crawl database.</p> <p>This model tracks freshness information and points to the current version of the page content. It's used for incremental sync to determine what needs re-crawling.</p>"},{"location":"reference/ragcrawl/models/page/#ragcrawl.models.page.Page.needs_recrawl","title":"<code>needs_recrawl(max_age_hours=None, force=False)</code>","text":"<p>Determine if this page needs to be re-crawled.</p> <p>Parameters:</p> Name Type Description Default <code>max_age_hours</code> <code>float | None</code> <p>Maximum age in hours before recrawl. None means always recrawl.</p> <code>None</code> <code>force</code> <code>bool</code> <p>If True, always return True.</p> <code>False</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the page should be re-crawled.</p> Source code in <code>src/ragcrawl/models/page.py</code> <pre><code>def needs_recrawl(\n    self,\n    max_age_hours: float | None = None,\n    force: bool = False,\n) -&gt; bool:\n    \"\"\"\n    Determine if this page needs to be re-crawled.\n\n    Args:\n        max_age_hours: Maximum age in hours before recrawl. None means always recrawl.\n        force: If True, always return True.\n\n    Returns:\n        True if the page should be re-crawled.\n    \"\"\"\n    if force:\n        return True\n\n    if self.is_tombstone:\n        return False\n\n    if self.last_crawled is None:\n        return True\n\n    if max_age_hours is None:\n        return True\n\n    age = datetime.now() - self.last_crawled\n    return age.total_seconds() / 3600 &gt; max_age_hours\n</code></pre>"},{"location":"reference/ragcrawl/models/page_version/","title":"Page version","text":""},{"location":"reference/ragcrawl/models/page_version/#ragcrawl.models.page_version","title":"<code>page_version</code>","text":"<p>PageVersion model representing a specific version of page content.</p>"},{"location":"reference/ragcrawl/models/page_version/#ragcrawl.models.page_version.PageVersion","title":"<code>PageVersion</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a specific version of a page's content.</p> <p>Each time content changes, a new PageVersion is created. This enables version history and change tracking for KB updates.</p>"},{"location":"reference/ragcrawl/models/site/","title":"Site","text":""},{"location":"reference/ragcrawl/models/site/#ragcrawl.models.site","title":"<code>site</code>","text":"<p>Site model representing a crawl target configuration.</p>"},{"location":"reference/ragcrawl/models/site/#ragcrawl.models.site.Site","title":"<code>Site</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a website/crawl target with its configuration.</p> <p>Stores the configuration snapshot and metadata for a crawl target.</p>"},{"location":"reference/ragcrawl/output/","title":"Index","text":""},{"location":"reference/ragcrawl/output/#ragcrawl.output","title":"<code>output</code>","text":"<p>Output publishing for ragcrawl.</p>"},{"location":"reference/ragcrawl/output/#ragcrawl.output.LinkRewriter","title":"<code>LinkRewriter(config)</code>","text":"<p>Rewrites internal links to point to local markdown files.</p> <p>Initialize link rewriter.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>OutputConfig</code> <p>Output configuration.</p> required Source code in <code>src/ragcrawl/output/link_rewriter.py</code> <pre><code>def __init__(self, config: OutputConfig) -&gt; None:\n    \"\"\"\n    Initialize link rewriter.\n\n    Args:\n        config: Output configuration.\n    \"\"\"\n    self.config = config\n    self._url_to_path: dict[str, Path] = {}\n\n    # Regex for markdown links\n    self._link_pattern = re.compile(\n        r\"\\[([^\\]]+)\\]\\(([^)]+)\\)\",\n        re.MULTILINE,\n    )\n</code></pre>"},{"location":"reference/ragcrawl/output/#ragcrawl.output.LinkRewriter.rewrite","title":"<code>rewrite(content, source_url)</code>","text":"<p>Rewrite links in content.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>Markdown content.</p> required <code>source_url</code> <code>str</code> <p>URL of the source page.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Content with rewritten links.</p> Source code in <code>src/ragcrawl/output/link_rewriter.py</code> <pre><code>def rewrite(self, content: str, source_url: str) -&gt; str:\n    \"\"\"\n    Rewrite links in content.\n\n    Args:\n        content: Markdown content.\n        source_url: URL of the source page.\n\n    Returns:\n        Content with rewritten links.\n    \"\"\"\n    if not self.config.rewrite_internal_links:\n        return content\n\n    def replace_link(match: re.Match) -&gt; str:\n        text = match.group(1)\n        href = match.group(2)\n\n        # Skip non-http links\n        if href.startswith((\"#\", \"mailto:\", \"tel:\", \"javascript:\")):\n            return match.group(0)\n\n        # Resolve relative URLs\n        if not href.startswith((\"http://\", \"https://\")):\n            href = urljoin(source_url, href)\n\n        # Check if it's an internal link we know about\n        # Normalize the URL\n        parsed = urlparse(href)\n        normalized = f\"{parsed.scheme}://{parsed.netloc}{parsed.path}\"\n\n        if normalized in self._url_to_path:\n            local_path = self._url_to_path[normalized]\n            # Calculate relative path from source\n            source_path = self._url_to_path.get(source_url)\n            if source_path:\n                relative = self._get_relative_path(source_path, local_path)\n                return f\"[{text}]({relative})\"\n\n        # Keep original link for external URLs\n        return match.group(0)\n\n    return self._link_pattern.sub(replace_link, content)\n</code></pre>"},{"location":"reference/ragcrawl/output/#ragcrawl.output.LinkRewriter.set_url_mapping","title":"<code>set_url_mapping(mapping)</code>","text":"<p>Set the URL to path mapping.</p> <p>Parameters:</p> Name Type Description Default <code>mapping</code> <code>dict[str, Path]</code> <p>Dict mapping URLs to output paths.</p> required Source code in <code>src/ragcrawl/output/link_rewriter.py</code> <pre><code>def set_url_mapping(self, mapping: dict[str, Path]) -&gt; None:\n    \"\"\"\n    Set the URL to path mapping.\n\n    Args:\n        mapping: Dict mapping URLs to output paths.\n    \"\"\"\n    self._url_to_path = mapping\n</code></pre>"},{"location":"reference/ragcrawl/output/#ragcrawl.output.MarkdownPublisher","title":"<code>MarkdownPublisher(config)</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for markdown publishers.</p> <p>Publishers write crawled content to disk in configured formats.</p> <p>Initialize publisher.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>OutputConfig</code> <p>Output configuration.</p> required Source code in <code>src/ragcrawl/output/publisher.py</code> <pre><code>def __init__(self, config: OutputConfig) -&gt; None:\n    \"\"\"\n    Initialize publisher.\n\n    Args:\n        config: Output configuration.\n    \"\"\"\n    self.config = config\n    self.output_path = Path(config.root_dir)\n</code></pre>"},{"location":"reference/ragcrawl/output/#ragcrawl.output.MarkdownPublisher.ensure_output_dir","title":"<code>ensure_output_dir()</code>","text":"<p>Ensure output directory exists.</p> Source code in <code>src/ragcrawl/output/publisher.py</code> <pre><code>def ensure_output_dir(self) -&gt; None:\n    \"\"\"Ensure output directory exists.\"\"\"\n    self.output_path.mkdir(parents=True, exist_ok=True)\n</code></pre>"},{"location":"reference/ragcrawl/output/#ragcrawl.output.MarkdownPublisher.publish","title":"<code>publish(documents)</code>  <code>abstractmethod</code>","text":"<p>Publish documents to disk.</p> <p>Parameters:</p> Name Type Description Default <code>documents</code> <code>list[Document]</code> <p>Documents to publish.</p> required <p>Returns:</p> Type Description <code>list[Path]</code> <p>List of created file paths.</p> Source code in <code>src/ragcrawl/output/publisher.py</code> <pre><code>@abstractmethod\ndef publish(self, documents: list[Document]) -&gt; list[Path]:\n    \"\"\"\n    Publish documents to disk.\n\n    Args:\n        documents: Documents to publish.\n\n    Returns:\n        List of created file paths.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/ragcrawl/output/#ragcrawl.output.MarkdownPublisher.publish_single","title":"<code>publish_single(document)</code>  <code>abstractmethod</code>","text":"<p>Publish a single document.</p> <p>Parameters:</p> Name Type Description Default <code>document</code> <code>Document</code> <p>Document to publish.</p> required <p>Returns:</p> Type Description <code>Path | None</code> <p>Created file path, or None if not written.</p> Source code in <code>src/ragcrawl/output/publisher.py</code> <pre><code>@abstractmethod\ndef publish_single(self, document: Document) -&gt; Path | None:\n    \"\"\"\n    Publish a single document.\n\n    Args:\n        document: Document to publish.\n\n    Returns:\n        Created file path, or None if not written.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/ragcrawl/output/#ragcrawl.output.MultiPagePublisher","title":"<code>MultiPagePublisher(config)</code>","text":"<p>               Bases: <code>MarkdownPublisher</code></p> <p>Publishes documents as individual markdown files.</p> <p>Features: - Preserves site folder structure - Rewrites internal links to local markdown files - Generates navigation aids (index, breadcrumbs, prev/next) - Handles deleted pages via tombstones or redirects</p> <p>Initialize multi-page publisher.</p> Source code in <code>src/ragcrawl/output/multi_page.py</code> <pre><code>def __init__(self, config: OutputConfig) -&gt; None:\n    \"\"\"Initialize multi-page publisher.\"\"\"\n    super().__init__(config)\n    self.link_rewriter = LinkRewriter(config)\n    self.nav_generator = NavigationGenerator(config)\n</code></pre>"},{"location":"reference/ragcrawl/output/#ragcrawl.output.MultiPagePublisher.publish","title":"<code>publish(documents)</code>","text":"<p>Publish documents as individual files.</p> <p>Parameters:</p> Name Type Description Default <code>documents</code> <code>list[Document]</code> <p>Documents to publish.</p> required <p>Returns:</p> Type Description <code>list[Path]</code> <p>List of created file paths.</p> Source code in <code>src/ragcrawl/output/multi_page.py</code> <pre><code>def publish(self, documents: list[Document]) -&gt; list[Path]:\n    \"\"\"\n    Publish documents as individual files.\n\n    Args:\n        documents: Documents to publish.\n\n    Returns:\n        List of created file paths.\n    \"\"\"\n    if not documents:\n        return []\n\n    self.ensure_output_dir()\n\n    # Build URL to path mapping for link rewriting\n    url_to_path = {}\n    for doc in documents:\n        output_path = self._url_to_path(doc.normalized_url)\n        url_to_path[doc.normalized_url] = output_path\n\n    self.link_rewriter.set_url_mapping(url_to_path)\n\n    # Sort by depth for proper ordering\n    sorted_docs = sorted(documents, key=lambda d: (d.depth, d.normalized_url))\n\n    created_files = []\n\n    # Publish each document\n    for i, doc in enumerate(sorted_docs):\n        # Get prev/next for navigation\n        prev_doc = sorted_docs[i - 1] if i &gt; 0 else None\n        next_doc = sorted_docs[i + 1] if i &lt; len(sorted_docs) - 1 else None\n\n        file_path = self._publish_document(doc, prev_doc, next_doc)\n        if file_path:\n            created_files.append(file_path)\n\n    # Generate index if enabled\n    if self.config.generate_index:\n        index_path = self._generate_index(sorted_docs)\n        created_files.append(index_path)\n\n    return created_files\n</code></pre>"},{"location":"reference/ragcrawl/output/#ragcrawl.output.MultiPagePublisher.publish_single","title":"<code>publish_single(document)</code>","text":"<p>Publish a single document.</p> <p>Parameters:</p> Name Type Description Default <code>document</code> <code>Document</code> <p>Document to publish.</p> required <p>Returns:</p> Type Description <code>Path | None</code> <p>Created file path.</p> Source code in <code>src/ragcrawl/output/multi_page.py</code> <pre><code>def publish_single(self, document: Document) -&gt; Path | None:\n    \"\"\"\n    Publish a single document.\n\n    Args:\n        document: Document to publish.\n\n    Returns:\n        Created file path.\n    \"\"\"\n    self.ensure_output_dir()\n    return self._publish_document(document, None, None)\n</code></pre>"},{"location":"reference/ragcrawl/output/#ragcrawl.output.NavigationGenerator","title":"<code>NavigationGenerator(config)</code>","text":"<p>Generates navigation elements for multi-page output.</p> <p>Initialize navigation generator.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>OutputConfig</code> <p>Output configuration.</p> required Source code in <code>src/ragcrawl/output/navigation.py</code> <pre><code>def __init__(self, config: OutputConfig) -&gt; None:\n    \"\"\"\n    Initialize navigation generator.\n\n    Args:\n        config: Output configuration.\n    \"\"\"\n    self.config = config\n</code></pre>"},{"location":"reference/ragcrawl/output/#ragcrawl.output.NavigationGenerator.generate_breadcrumbs","title":"<code>generate_breadcrumbs(document)</code>","text":"<p>Generate breadcrumb navigation.</p> <p>Parameters:</p> Name Type Description Default <code>document</code> <code>Document</code> <p>Document to generate breadcrumbs for.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Markdown breadcrumb string.</p> Source code in <code>src/ragcrawl/output/navigation.py</code> <pre><code>def generate_breadcrumbs(self, document: Document) -&gt; str:\n    \"\"\"\n    Generate breadcrumb navigation.\n\n    Args:\n        document: Document to generate breadcrumbs for.\n\n    Returns:\n        Markdown breadcrumb string.\n    \"\"\"\n    parsed = urlparse(document.normalized_url)\n    path_parts = parsed.path.strip(\"/\").split(\"/\")\n\n    if not path_parts or path_parts == [\"\"]:\n        return \"\"\n\n    breadcrumbs = [\"[Home](/\" + self.config.index_file_name + \")\"]\n\n    # Build breadcrumb for each level\n    current_path = \"\"\n    for i, part in enumerate(path_parts[:-1]):\n        current_path += f\"/{part}\"\n        title = part.replace(\"-\", \" \").replace(\"_\", \" \").title()\n        breadcrumbs.append(f\"[{title}]({current_path}/{self.config.index_file_name})\")\n\n    # Current page (not a link)\n    title = document.title or path_parts[-1].replace(\"-\", \" \").replace(\"_\", \" \").title()\n    breadcrumbs.append(title)\n\n    return \" &gt; \".join(breadcrumbs)\n</code></pre>"},{"location":"reference/ragcrawl/output/#ragcrawl.output.NavigationGenerator.generate_index","title":"<code>generate_index(documents)</code>","text":"<p>Generate index/TOC page.</p> <p>Parameters:</p> Name Type Description Default <code>documents</code> <code>list[Document]</code> <p>All documents in the site.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Markdown index content.</p> Source code in <code>src/ragcrawl/output/navigation.py</code> <pre><code>def generate_index(self, documents: list[Document]) -&gt; str:\n    \"\"\"\n    Generate index/TOC page.\n\n    Args:\n        documents: All documents in the site.\n\n    Returns:\n        Markdown index content.\n    \"\"\"\n    lines = [\n        \"---\",\n        \"title: Index\",\n        \"---\",\n        \"\",\n        \"# Documentation Index\",\n        \"\",\n    ]\n\n    # Group by depth/path\n    by_path: dict[str, list[Document]] = {}\n    for doc in documents:\n        if doc.is_tombstone:\n            continue\n\n        parsed = urlparse(doc.normalized_url)\n        path = parsed.path.strip(\"/\")\n        parts = path.split(\"/\")\n\n        if len(parts) &gt; 1:\n            section = parts[0]\n        else:\n            section = \"/\"\n\n        if section not in by_path:\n            by_path[section] = []\n        by_path[section].append(doc)\n\n    # Generate sections\n    for section, docs in sorted(by_path.items()):\n        if section != \"/\":\n            section_title = section.replace(\"-\", \" \").replace(\"_\", \" \").title()\n            lines.append(f\"\\n## {section_title}\\n\")\n\n        for doc in sorted(docs, key=lambda d: d.normalized_url):\n            title = doc.title or self._url_to_title(doc.normalized_url)\n            path = self._url_to_path(doc.normalized_url)\n            lines.append(f\"- [{title}]({path})\")\n\n    return \"\\n\".join(lines)\n</code></pre>"},{"location":"reference/ragcrawl/output/#ragcrawl.output.NavigationGenerator.generate_prev_next","title":"<code>generate_prev_next(prev_doc, next_doc)</code>","text":"<p>Generate previous/next navigation.</p> <p>Parameters:</p> Name Type Description Default <code>prev_doc</code> <code>Document | None</code> <p>Previous document in sequence.</p> required <code>next_doc</code> <code>Document | None</code> <p>Next document in sequence.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Markdown navigation string.</p> Source code in <code>src/ragcrawl/output/navigation.py</code> <pre><code>def generate_prev_next(\n    self,\n    prev_doc: Document | None,\n    next_doc: Document | None,\n) -&gt; str:\n    \"\"\"\n    Generate previous/next navigation.\n\n    Args:\n        prev_doc: Previous document in sequence.\n        next_doc: Next document in sequence.\n\n    Returns:\n        Markdown navigation string.\n    \"\"\"\n    parts = []\n\n    parts.append(\"---\\n\")\n\n    if prev_doc:\n        prev_title = prev_doc.title or \"Previous\"\n        prev_path = self._url_to_path(prev_doc.normalized_url)\n        parts.append(f\"**Previous:** [{prev_title}]({prev_path})\")\n\n    if prev_doc and next_doc:\n        parts.append(\" | \")\n\n    if next_doc:\n        next_title = next_doc.title or \"Next\"\n        next_path = self._url_to_path(next_doc.normalized_url)\n        parts.append(f\"**Next:** [{next_title}]({next_path})\")\n\n    return \"\".join(parts)\n</code></pre>"},{"location":"reference/ragcrawl/output/#ragcrawl.output.SinglePagePublisher","title":"<code>SinglePagePublisher(config)</code>","text":"<p>               Bases: <code>MarkdownPublisher</code></p> <p>Publishes all documents to a single markdown file.</p> <p>Features: - Auto-generated table of contents - Per-page anchors for navigation - Configurable page separators</p> Source code in <code>src/ragcrawl/output/publisher.py</code> <pre><code>def __init__(self, config: OutputConfig) -&gt; None:\n    \"\"\"\n    Initialize publisher.\n\n    Args:\n        config: Output configuration.\n    \"\"\"\n    self.config = config\n    self.output_path = Path(config.root_dir)\n</code></pre>"},{"location":"reference/ragcrawl/output/#ragcrawl.output.SinglePagePublisher.publish","title":"<code>publish(documents)</code>","text":"<p>Publish all documents to a single file.</p> <p>Parameters:</p> Name Type Description Default <code>documents</code> <code>list[Document]</code> <p>Documents to publish.</p> required <p>Returns:</p> Type Description <code>list[Path]</code> <p>List containing the single output file path.</p> Source code in <code>src/ragcrawl/output/single_page.py</code> <pre><code>def publish(self, documents: list[Document]) -&gt; list[Path]:\n    \"\"\"\n    Publish all documents to a single file.\n\n    Args:\n        documents: Documents to publish.\n\n    Returns:\n        List containing the single output file path.\n    \"\"\"\n    if not documents:\n        return []\n\n    self.ensure_output_dir()\n\n    # Sort documents by depth, then URL\n    sorted_docs = sorted(documents, key=lambda d: (d.depth, d.normalized_url))\n\n    # Build content\n    content_parts = []\n\n    # Generate TOC if enabled\n    if self.config.generate_toc:\n        toc = self._generate_toc(sorted_docs)\n        content_parts.append(toc)\n        content_parts.append(self.config.page_separator)\n\n    # Add each document\n    for doc in sorted_docs:\n        page_content = self._format_document(doc)\n        content_parts.append(page_content)\n        content_parts.append(self.config.page_separator)\n\n    # Write file\n    output_file = self.output_path / self.config.single_file_name\n    output_file.write_text(\"\".join(content_parts))\n\n    return [output_file]\n</code></pre>"},{"location":"reference/ragcrawl/output/#ragcrawl.output.SinglePagePublisher.publish_single","title":"<code>publish_single(document)</code>","text":"<p>Single page mode doesn't support individual publishing.</p> Source code in <code>src/ragcrawl/output/single_page.py</code> <pre><code>def publish_single(self, document: Document) -&gt; Path | None:\n    \"\"\"Single page mode doesn't support individual publishing.\"\"\"\n    return None\n</code></pre>"},{"location":"reference/ragcrawl/output/link_rewriter/","title":"Link rewriter","text":""},{"location":"reference/ragcrawl/output/link_rewriter/#ragcrawl.output.link_rewriter","title":"<code>link_rewriter</code>","text":"<p>Internal link rewriting for multi-page output.</p>"},{"location":"reference/ragcrawl/output/link_rewriter/#ragcrawl.output.link_rewriter.LinkRewriter","title":"<code>LinkRewriter(config)</code>","text":"<p>Rewrites internal links to point to local markdown files.</p> <p>Initialize link rewriter.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>OutputConfig</code> <p>Output configuration.</p> required Source code in <code>src/ragcrawl/output/link_rewriter.py</code> <pre><code>def __init__(self, config: OutputConfig) -&gt; None:\n    \"\"\"\n    Initialize link rewriter.\n\n    Args:\n        config: Output configuration.\n    \"\"\"\n    self.config = config\n    self._url_to_path: dict[str, Path] = {}\n\n    # Regex for markdown links\n    self._link_pattern = re.compile(\n        r\"\\[([^\\]]+)\\]\\(([^)]+)\\)\",\n        re.MULTILINE,\n    )\n</code></pre>"},{"location":"reference/ragcrawl/output/link_rewriter/#ragcrawl.output.link_rewriter.LinkRewriter.rewrite","title":"<code>rewrite(content, source_url)</code>","text":"<p>Rewrite links in content.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>Markdown content.</p> required <code>source_url</code> <code>str</code> <p>URL of the source page.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Content with rewritten links.</p> Source code in <code>src/ragcrawl/output/link_rewriter.py</code> <pre><code>def rewrite(self, content: str, source_url: str) -&gt; str:\n    \"\"\"\n    Rewrite links in content.\n\n    Args:\n        content: Markdown content.\n        source_url: URL of the source page.\n\n    Returns:\n        Content with rewritten links.\n    \"\"\"\n    if not self.config.rewrite_internal_links:\n        return content\n\n    def replace_link(match: re.Match) -&gt; str:\n        text = match.group(1)\n        href = match.group(2)\n\n        # Skip non-http links\n        if href.startswith((\"#\", \"mailto:\", \"tel:\", \"javascript:\")):\n            return match.group(0)\n\n        # Resolve relative URLs\n        if not href.startswith((\"http://\", \"https://\")):\n            href = urljoin(source_url, href)\n\n        # Check if it's an internal link we know about\n        # Normalize the URL\n        parsed = urlparse(href)\n        normalized = f\"{parsed.scheme}://{parsed.netloc}{parsed.path}\"\n\n        if normalized in self._url_to_path:\n            local_path = self._url_to_path[normalized]\n            # Calculate relative path from source\n            source_path = self._url_to_path.get(source_url)\n            if source_path:\n                relative = self._get_relative_path(source_path, local_path)\n                return f\"[{text}]({relative})\"\n\n        # Keep original link for external URLs\n        return match.group(0)\n\n    return self._link_pattern.sub(replace_link, content)\n</code></pre>"},{"location":"reference/ragcrawl/output/link_rewriter/#ragcrawl.output.link_rewriter.LinkRewriter.set_url_mapping","title":"<code>set_url_mapping(mapping)</code>","text":"<p>Set the URL to path mapping.</p> <p>Parameters:</p> Name Type Description Default <code>mapping</code> <code>dict[str, Path]</code> <p>Dict mapping URLs to output paths.</p> required Source code in <code>src/ragcrawl/output/link_rewriter.py</code> <pre><code>def set_url_mapping(self, mapping: dict[str, Path]) -&gt; None:\n    \"\"\"\n    Set the URL to path mapping.\n\n    Args:\n        mapping: Dict mapping URLs to output paths.\n    \"\"\"\n    self._url_to_path = mapping\n</code></pre>"},{"location":"reference/ragcrawl/output/multi_page/","title":"Multi page","text":""},{"location":"reference/ragcrawl/output/multi_page/#ragcrawl.output.multi_page","title":"<code>multi_page</code>","text":"<p>Multi-page markdown publisher.</p>"},{"location":"reference/ragcrawl/output/multi_page/#ragcrawl.output.multi_page.MultiPagePublisher","title":"<code>MultiPagePublisher(config)</code>","text":"<p>               Bases: <code>MarkdownPublisher</code></p> <p>Publishes documents as individual markdown files.</p> <p>Features: - Preserves site folder structure - Rewrites internal links to local markdown files - Generates navigation aids (index, breadcrumbs, prev/next) - Handles deleted pages via tombstones or redirects</p> <p>Initialize multi-page publisher.</p> Source code in <code>src/ragcrawl/output/multi_page.py</code> <pre><code>def __init__(self, config: OutputConfig) -&gt; None:\n    \"\"\"Initialize multi-page publisher.\"\"\"\n    super().__init__(config)\n    self.link_rewriter = LinkRewriter(config)\n    self.nav_generator = NavigationGenerator(config)\n</code></pre>"},{"location":"reference/ragcrawl/output/multi_page/#ragcrawl.output.multi_page.MultiPagePublisher.publish","title":"<code>publish(documents)</code>","text":"<p>Publish documents as individual files.</p> <p>Parameters:</p> Name Type Description Default <code>documents</code> <code>list[Document]</code> <p>Documents to publish.</p> required <p>Returns:</p> Type Description <code>list[Path]</code> <p>List of created file paths.</p> Source code in <code>src/ragcrawl/output/multi_page.py</code> <pre><code>def publish(self, documents: list[Document]) -&gt; list[Path]:\n    \"\"\"\n    Publish documents as individual files.\n\n    Args:\n        documents: Documents to publish.\n\n    Returns:\n        List of created file paths.\n    \"\"\"\n    if not documents:\n        return []\n\n    self.ensure_output_dir()\n\n    # Build URL to path mapping for link rewriting\n    url_to_path = {}\n    for doc in documents:\n        output_path = self._url_to_path(doc.normalized_url)\n        url_to_path[doc.normalized_url] = output_path\n\n    self.link_rewriter.set_url_mapping(url_to_path)\n\n    # Sort by depth for proper ordering\n    sorted_docs = sorted(documents, key=lambda d: (d.depth, d.normalized_url))\n\n    created_files = []\n\n    # Publish each document\n    for i, doc in enumerate(sorted_docs):\n        # Get prev/next for navigation\n        prev_doc = sorted_docs[i - 1] if i &gt; 0 else None\n        next_doc = sorted_docs[i + 1] if i &lt; len(sorted_docs) - 1 else None\n\n        file_path = self._publish_document(doc, prev_doc, next_doc)\n        if file_path:\n            created_files.append(file_path)\n\n    # Generate index if enabled\n    if self.config.generate_index:\n        index_path = self._generate_index(sorted_docs)\n        created_files.append(index_path)\n\n    return created_files\n</code></pre>"},{"location":"reference/ragcrawl/output/multi_page/#ragcrawl.output.multi_page.MultiPagePublisher.publish_single","title":"<code>publish_single(document)</code>","text":"<p>Publish a single document.</p> <p>Parameters:</p> Name Type Description Default <code>document</code> <code>Document</code> <p>Document to publish.</p> required <p>Returns:</p> Type Description <code>Path | None</code> <p>Created file path.</p> Source code in <code>src/ragcrawl/output/multi_page.py</code> <pre><code>def publish_single(self, document: Document) -&gt; Path | None:\n    \"\"\"\n    Publish a single document.\n\n    Args:\n        document: Document to publish.\n\n    Returns:\n        Created file path.\n    \"\"\"\n    self.ensure_output_dir()\n    return self._publish_document(document, None, None)\n</code></pre>"},{"location":"reference/ragcrawl/output/navigation/","title":"Navigation","text":""},{"location":"reference/ragcrawl/output/navigation/#ragcrawl.output.navigation","title":"<code>navigation</code>","text":"<p>Navigation generation for multi-page output.</p>"},{"location":"reference/ragcrawl/output/navigation/#ragcrawl.output.navigation.NavigationGenerator","title":"<code>NavigationGenerator(config)</code>","text":"<p>Generates navigation elements for multi-page output.</p> <p>Initialize navigation generator.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>OutputConfig</code> <p>Output configuration.</p> required Source code in <code>src/ragcrawl/output/navigation.py</code> <pre><code>def __init__(self, config: OutputConfig) -&gt; None:\n    \"\"\"\n    Initialize navigation generator.\n\n    Args:\n        config: Output configuration.\n    \"\"\"\n    self.config = config\n</code></pre>"},{"location":"reference/ragcrawl/output/navigation/#ragcrawl.output.navigation.NavigationGenerator.generate_breadcrumbs","title":"<code>generate_breadcrumbs(document)</code>","text":"<p>Generate breadcrumb navigation.</p> <p>Parameters:</p> Name Type Description Default <code>document</code> <code>Document</code> <p>Document to generate breadcrumbs for.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Markdown breadcrumb string.</p> Source code in <code>src/ragcrawl/output/navigation.py</code> <pre><code>def generate_breadcrumbs(self, document: Document) -&gt; str:\n    \"\"\"\n    Generate breadcrumb navigation.\n\n    Args:\n        document: Document to generate breadcrumbs for.\n\n    Returns:\n        Markdown breadcrumb string.\n    \"\"\"\n    parsed = urlparse(document.normalized_url)\n    path_parts = parsed.path.strip(\"/\").split(\"/\")\n\n    if not path_parts or path_parts == [\"\"]:\n        return \"\"\n\n    breadcrumbs = [\"[Home](/\" + self.config.index_file_name + \")\"]\n\n    # Build breadcrumb for each level\n    current_path = \"\"\n    for i, part in enumerate(path_parts[:-1]):\n        current_path += f\"/{part}\"\n        title = part.replace(\"-\", \" \").replace(\"_\", \" \").title()\n        breadcrumbs.append(f\"[{title}]({current_path}/{self.config.index_file_name})\")\n\n    # Current page (not a link)\n    title = document.title or path_parts[-1].replace(\"-\", \" \").replace(\"_\", \" \").title()\n    breadcrumbs.append(title)\n\n    return \" &gt; \".join(breadcrumbs)\n</code></pre>"},{"location":"reference/ragcrawl/output/navigation/#ragcrawl.output.navigation.NavigationGenerator.generate_index","title":"<code>generate_index(documents)</code>","text":"<p>Generate index/TOC page.</p> <p>Parameters:</p> Name Type Description Default <code>documents</code> <code>list[Document]</code> <p>All documents in the site.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Markdown index content.</p> Source code in <code>src/ragcrawl/output/navigation.py</code> <pre><code>def generate_index(self, documents: list[Document]) -&gt; str:\n    \"\"\"\n    Generate index/TOC page.\n\n    Args:\n        documents: All documents in the site.\n\n    Returns:\n        Markdown index content.\n    \"\"\"\n    lines = [\n        \"---\",\n        \"title: Index\",\n        \"---\",\n        \"\",\n        \"# Documentation Index\",\n        \"\",\n    ]\n\n    # Group by depth/path\n    by_path: dict[str, list[Document]] = {}\n    for doc in documents:\n        if doc.is_tombstone:\n            continue\n\n        parsed = urlparse(doc.normalized_url)\n        path = parsed.path.strip(\"/\")\n        parts = path.split(\"/\")\n\n        if len(parts) &gt; 1:\n            section = parts[0]\n        else:\n            section = \"/\"\n\n        if section not in by_path:\n            by_path[section] = []\n        by_path[section].append(doc)\n\n    # Generate sections\n    for section, docs in sorted(by_path.items()):\n        if section != \"/\":\n            section_title = section.replace(\"-\", \" \").replace(\"_\", \" \").title()\n            lines.append(f\"\\n## {section_title}\\n\")\n\n        for doc in sorted(docs, key=lambda d: d.normalized_url):\n            title = doc.title or self._url_to_title(doc.normalized_url)\n            path = self._url_to_path(doc.normalized_url)\n            lines.append(f\"- [{title}]({path})\")\n\n    return \"\\n\".join(lines)\n</code></pre>"},{"location":"reference/ragcrawl/output/navigation/#ragcrawl.output.navigation.NavigationGenerator.generate_prev_next","title":"<code>generate_prev_next(prev_doc, next_doc)</code>","text":"<p>Generate previous/next navigation.</p> <p>Parameters:</p> Name Type Description Default <code>prev_doc</code> <code>Document | None</code> <p>Previous document in sequence.</p> required <code>next_doc</code> <code>Document | None</code> <p>Next document in sequence.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Markdown navigation string.</p> Source code in <code>src/ragcrawl/output/navigation.py</code> <pre><code>def generate_prev_next(\n    self,\n    prev_doc: Document | None,\n    next_doc: Document | None,\n) -&gt; str:\n    \"\"\"\n    Generate previous/next navigation.\n\n    Args:\n        prev_doc: Previous document in sequence.\n        next_doc: Next document in sequence.\n\n    Returns:\n        Markdown navigation string.\n    \"\"\"\n    parts = []\n\n    parts.append(\"---\\n\")\n\n    if prev_doc:\n        prev_title = prev_doc.title or \"Previous\"\n        prev_path = self._url_to_path(prev_doc.normalized_url)\n        parts.append(f\"**Previous:** [{prev_title}]({prev_path})\")\n\n    if prev_doc and next_doc:\n        parts.append(\" | \")\n\n    if next_doc:\n        next_title = next_doc.title or \"Next\"\n        next_path = self._url_to_path(next_doc.normalized_url)\n        parts.append(f\"**Next:** [{next_title}]({next_path})\")\n\n    return \"\".join(parts)\n</code></pre>"},{"location":"reference/ragcrawl/output/publisher/","title":"Publisher","text":""},{"location":"reference/ragcrawl/output/publisher/#ragcrawl.output.publisher","title":"<code>publisher</code>","text":"<p>Base publisher protocol.</p>"},{"location":"reference/ragcrawl/output/publisher/#ragcrawl.output.publisher.MarkdownPublisher","title":"<code>MarkdownPublisher(config)</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for markdown publishers.</p> <p>Publishers write crawled content to disk in configured formats.</p> <p>Initialize publisher.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>OutputConfig</code> <p>Output configuration.</p> required Source code in <code>src/ragcrawl/output/publisher.py</code> <pre><code>def __init__(self, config: OutputConfig) -&gt; None:\n    \"\"\"\n    Initialize publisher.\n\n    Args:\n        config: Output configuration.\n    \"\"\"\n    self.config = config\n    self.output_path = Path(config.root_dir)\n</code></pre>"},{"location":"reference/ragcrawl/output/publisher/#ragcrawl.output.publisher.MarkdownPublisher.ensure_output_dir","title":"<code>ensure_output_dir()</code>","text":"<p>Ensure output directory exists.</p> Source code in <code>src/ragcrawl/output/publisher.py</code> <pre><code>def ensure_output_dir(self) -&gt; None:\n    \"\"\"Ensure output directory exists.\"\"\"\n    self.output_path.mkdir(parents=True, exist_ok=True)\n</code></pre>"},{"location":"reference/ragcrawl/output/publisher/#ragcrawl.output.publisher.MarkdownPublisher.publish","title":"<code>publish(documents)</code>  <code>abstractmethod</code>","text":"<p>Publish documents to disk.</p> <p>Parameters:</p> Name Type Description Default <code>documents</code> <code>list[Document]</code> <p>Documents to publish.</p> required <p>Returns:</p> Type Description <code>list[Path]</code> <p>List of created file paths.</p> Source code in <code>src/ragcrawl/output/publisher.py</code> <pre><code>@abstractmethod\ndef publish(self, documents: list[Document]) -&gt; list[Path]:\n    \"\"\"\n    Publish documents to disk.\n\n    Args:\n        documents: Documents to publish.\n\n    Returns:\n        List of created file paths.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/ragcrawl/output/publisher/#ragcrawl.output.publisher.MarkdownPublisher.publish_single","title":"<code>publish_single(document)</code>  <code>abstractmethod</code>","text":"<p>Publish a single document.</p> <p>Parameters:</p> Name Type Description Default <code>document</code> <code>Document</code> <p>Document to publish.</p> required <p>Returns:</p> Type Description <code>Path | None</code> <p>Created file path, or None if not written.</p> Source code in <code>src/ragcrawl/output/publisher.py</code> <pre><code>@abstractmethod\ndef publish_single(self, document: Document) -&gt; Path | None:\n    \"\"\"\n    Publish a single document.\n\n    Args:\n        document: Document to publish.\n\n    Returns:\n        Created file path, or None if not written.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/ragcrawl/output/single_page/","title":"Single page","text":""},{"location":"reference/ragcrawl/output/single_page/#ragcrawl.output.single_page","title":"<code>single_page</code>","text":"<p>Single-page markdown publisher.</p>"},{"location":"reference/ragcrawl/output/single_page/#ragcrawl.output.single_page.SinglePagePublisher","title":"<code>SinglePagePublisher(config)</code>","text":"<p>               Bases: <code>MarkdownPublisher</code></p> <p>Publishes all documents to a single markdown file.</p> <p>Features: - Auto-generated table of contents - Per-page anchors for navigation - Configurable page separators</p> Source code in <code>src/ragcrawl/output/publisher.py</code> <pre><code>def __init__(self, config: OutputConfig) -&gt; None:\n    \"\"\"\n    Initialize publisher.\n\n    Args:\n        config: Output configuration.\n    \"\"\"\n    self.config = config\n    self.output_path = Path(config.root_dir)\n</code></pre>"},{"location":"reference/ragcrawl/output/single_page/#ragcrawl.output.single_page.SinglePagePublisher.publish","title":"<code>publish(documents)</code>","text":"<p>Publish all documents to a single file.</p> <p>Parameters:</p> Name Type Description Default <code>documents</code> <code>list[Document]</code> <p>Documents to publish.</p> required <p>Returns:</p> Type Description <code>list[Path]</code> <p>List containing the single output file path.</p> Source code in <code>src/ragcrawl/output/single_page.py</code> <pre><code>def publish(self, documents: list[Document]) -&gt; list[Path]:\n    \"\"\"\n    Publish all documents to a single file.\n\n    Args:\n        documents: Documents to publish.\n\n    Returns:\n        List containing the single output file path.\n    \"\"\"\n    if not documents:\n        return []\n\n    self.ensure_output_dir()\n\n    # Sort documents by depth, then URL\n    sorted_docs = sorted(documents, key=lambda d: (d.depth, d.normalized_url))\n\n    # Build content\n    content_parts = []\n\n    # Generate TOC if enabled\n    if self.config.generate_toc:\n        toc = self._generate_toc(sorted_docs)\n        content_parts.append(toc)\n        content_parts.append(self.config.page_separator)\n\n    # Add each document\n    for doc in sorted_docs:\n        page_content = self._format_document(doc)\n        content_parts.append(page_content)\n        content_parts.append(self.config.page_separator)\n\n    # Write file\n    output_file = self.output_path / self.config.single_file_name\n    output_file.write_text(\"\".join(content_parts))\n\n    return [output_file]\n</code></pre>"},{"location":"reference/ragcrawl/output/single_page/#ragcrawl.output.single_page.SinglePagePublisher.publish_single","title":"<code>publish_single(document)</code>","text":"<p>Single page mode doesn't support individual publishing.</p> Source code in <code>src/ragcrawl/output/single_page.py</code> <pre><code>def publish_single(self, document: Document) -&gt; Path | None:\n    \"\"\"Single page mode doesn't support individual publishing.\"\"\"\n    return None\n</code></pre>"},{"location":"reference/ragcrawl/storage/","title":"Index","text":""},{"location":"reference/ragcrawl/storage/#ragcrawl.storage","title":"<code>storage</code>","text":"<p>Storage backends for ragcrawl.</p>"},{"location":"reference/ragcrawl/storage/#ragcrawl.storage.StorageBackend","title":"<code>StorageBackend</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for storage backends.</p> <p>All backends must implement this interface to ensure feature parity.</p>"},{"location":"reference/ragcrawl/storage/#ragcrawl.storage.StorageBackend.clear_frontier","title":"<code>clear_frontier(run_id)</code>  <code>abstractmethod</code>","text":"<p>Clear all frontier items for a run. Returns count deleted.</p> Source code in <code>src/ragcrawl/storage/backend.py</code> <pre><code>@abstractmethod\ndef clear_frontier(self, run_id: str) -&gt; int:\n    \"\"\"Clear all frontier items for a run. Returns count deleted.\"\"\"\n    ...\n</code></pre>"},{"location":"reference/ragcrawl/storage/#ragcrawl.storage.StorageBackend.close","title":"<code>close()</code>  <code>abstractmethod</code>","text":"<p>Close any connections.</p> Source code in <code>src/ragcrawl/storage/backend.py</code> <pre><code>@abstractmethod\ndef close(self) -&gt; None:\n    \"\"\"Close any connections.\"\"\"\n    ...\n</code></pre>"},{"location":"reference/ragcrawl/storage/#ragcrawl.storage.StorageBackend.count_pages","title":"<code>count_pages(site_id, include_tombstones=False)</code>  <code>abstractmethod</code>","text":"<p>Count pages for a site.</p> Source code in <code>src/ragcrawl/storage/backend.py</code> <pre><code>@abstractmethod\ndef count_pages(self, site_id: str, include_tombstones: bool = False) -&gt; int:\n    \"\"\"Count pages for a site.\"\"\"\n    ...\n</code></pre>"},{"location":"reference/ragcrawl/storage/#ragcrawl.storage.StorageBackend.delete_site","title":"<code>delete_site(site_id)</code>  <code>abstractmethod</code>","text":"<p>Delete a site and all associated data.</p> Source code in <code>src/ragcrawl/storage/backend.py</code> <pre><code>@abstractmethod\ndef delete_site(self, site_id: str) -&gt; bool:\n    \"\"\"Delete a site and all associated data.\"\"\"\n    ...\n</code></pre>"},{"location":"reference/ragcrawl/storage/#ragcrawl.storage.StorageBackend.get_current_version","title":"<code>get_current_version(page_id)</code>  <code>abstractmethod</code>","text":"<p>Get the current version for a page.</p> Source code in <code>src/ragcrawl/storage/backend.py</code> <pre><code>@abstractmethod\ndef get_current_version(self, page_id: str) -&gt; PageVersion | None:\n    \"\"\"Get the current version for a page.\"\"\"\n    ...\n</code></pre>"},{"location":"reference/ragcrawl/storage/#ragcrawl.storage.StorageBackend.get_frontier_items","title":"<code>get_frontier_items(run_id, status=None, limit=1000)</code>  <code>abstractmethod</code>","text":"<p>Get frontier items for a run.</p> Source code in <code>src/ragcrawl/storage/backend.py</code> <pre><code>@abstractmethod\ndef get_frontier_items(\n    self,\n    run_id: str,\n    status: str | None = None,\n    limit: int = 1000,\n) -&gt; list[FrontierItem]:\n    \"\"\"Get frontier items for a run.\"\"\"\n    ...\n</code></pre>"},{"location":"reference/ragcrawl/storage/#ragcrawl.storage.StorageBackend.get_latest_run","title":"<code>get_latest_run(site_id)</code>  <code>abstractmethod</code>","text":"<p>Get the latest crawl run for a site.</p> Source code in <code>src/ragcrawl/storage/backend.py</code> <pre><code>@abstractmethod\ndef get_latest_run(self, site_id: str) -&gt; CrawlRun | None:\n    \"\"\"Get the latest crawl run for a site.\"\"\"\n    ...\n</code></pre>"},{"location":"reference/ragcrawl/storage/#ragcrawl.storage.StorageBackend.get_page","title":"<code>get_page(page_id)</code>  <code>abstractmethod</code>","text":"<p>Get a page by ID.</p> Source code in <code>src/ragcrawl/storage/backend.py</code> <pre><code>@abstractmethod\ndef get_page(self, page_id: str) -&gt; Page | None:\n    \"\"\"Get a page by ID.\"\"\"\n    ...\n</code></pre>"},{"location":"reference/ragcrawl/storage/#ragcrawl.storage.StorageBackend.get_page_by_url","title":"<code>get_page_by_url(site_id, url)</code>  <code>abstractmethod</code>","text":"<p>Get a page by normalized URL.</p> Source code in <code>src/ragcrawl/storage/backend.py</code> <pre><code>@abstractmethod\ndef get_page_by_url(self, site_id: str, url: str) -&gt; Page | None:\n    \"\"\"Get a page by normalized URL.\"\"\"\n    ...\n</code></pre>"},{"location":"reference/ragcrawl/storage/#ragcrawl.storage.StorageBackend.get_pages_needing_recrawl","title":"<code>get_pages_needing_recrawl(site_id, max_age_hours=None, limit=1000)</code>  <code>abstractmethod</code>","text":"<p>Get pages that need to be re-crawled.</p> Source code in <code>src/ragcrawl/storage/backend.py</code> <pre><code>@abstractmethod\ndef get_pages_needing_recrawl(\n    self,\n    site_id: str,\n    max_age_hours: float | None = None,\n    limit: int = 1000,\n) -&gt; list[Page]:\n    \"\"\"Get pages that need to be re-crawled.\"\"\"\n    ...\n</code></pre>"},{"location":"reference/ragcrawl/storage/#ragcrawl.storage.StorageBackend.get_run","title":"<code>get_run(run_id)</code>  <code>abstractmethod</code>","text":"<p>Get a crawl run by ID.</p> Source code in <code>src/ragcrawl/storage/backend.py</code> <pre><code>@abstractmethod\ndef get_run(self, run_id: str) -&gt; CrawlRun | None:\n    \"\"\"Get a crawl run by ID.\"\"\"\n    ...\n</code></pre>"},{"location":"reference/ragcrawl/storage/#ragcrawl.storage.StorageBackend.get_site","title":"<code>get_site(site_id)</code>  <code>abstractmethod</code>","text":"<p>Get a site by ID.</p> Source code in <code>src/ragcrawl/storage/backend.py</code> <pre><code>@abstractmethod\ndef get_site(self, site_id: str) -&gt; Site | None:\n    \"\"\"Get a site by ID.\"\"\"\n    ...\n</code></pre>"},{"location":"reference/ragcrawl/storage/#ragcrawl.storage.StorageBackend.get_version","title":"<code>get_version(version_id)</code>  <code>abstractmethod</code>","text":"<p>Get a page version by ID.</p> Source code in <code>src/ragcrawl/storage/backend.py</code> <pre><code>@abstractmethod\ndef get_version(self, version_id: str) -&gt; PageVersion | None:\n    \"\"\"Get a page version by ID.\"\"\"\n    ...\n</code></pre>"},{"location":"reference/ragcrawl/storage/#ragcrawl.storage.StorageBackend.health_check","title":"<code>health_check()</code>  <code>abstractmethod</code>","text":"<p>Check if the backend is healthy/available.</p> Source code in <code>src/ragcrawl/storage/backend.py</code> <pre><code>@abstractmethod\ndef health_check(self) -&gt; bool:\n    \"\"\"Check if the backend is healthy/available.\"\"\"\n    ...\n</code></pre>"},{"location":"reference/ragcrawl/storage/#ragcrawl.storage.StorageBackend.initialize","title":"<code>initialize()</code>  <code>abstractmethod</code>","text":"<p>Initialize the storage backend (create tables, etc.).</p> Source code in <code>src/ragcrawl/storage/backend.py</code> <pre><code>@abstractmethod\ndef initialize(self) -&gt; None:\n    \"\"\"Initialize the storage backend (create tables, etc.).\"\"\"\n    ...\n</code></pre>"},{"location":"reference/ragcrawl/storage/#ragcrawl.storage.StorageBackend.list_pages","title":"<code>list_pages(site_id, limit=1000, offset=0, include_tombstones=False)</code>  <code>abstractmethod</code>","text":"<p>List pages for a site.</p> Source code in <code>src/ragcrawl/storage/backend.py</code> <pre><code>@abstractmethod\ndef list_pages(\n    self,\n    site_id: str,\n    limit: int = 1000,\n    offset: int = 0,\n    include_tombstones: bool = False,\n) -&gt; list[Page]:\n    \"\"\"List pages for a site.\"\"\"\n    ...\n</code></pre>"},{"location":"reference/ragcrawl/storage/#ragcrawl.storage.StorageBackend.list_runs","title":"<code>list_runs(site_id, limit=100, offset=0)</code>  <code>abstractmethod</code>","text":"<p>List crawl runs for a site.</p> Source code in <code>src/ragcrawl/storage/backend.py</code> <pre><code>@abstractmethod\ndef list_runs(\n    self,\n    site_id: str,\n    limit: int = 100,\n    offset: int = 0,\n) -&gt; list[CrawlRun]:\n    \"\"\"List crawl runs for a site.\"\"\"\n    ...\n</code></pre>"},{"location":"reference/ragcrawl/storage/#ragcrawl.storage.StorageBackend.list_sites","title":"<code>list_sites()</code>  <code>abstractmethod</code>","text":"<p>List all sites.</p> Source code in <code>src/ragcrawl/storage/backend.py</code> <pre><code>@abstractmethod\ndef list_sites(self) -&gt; list[Site]:\n    \"\"\"List all sites.\"\"\"\n    ...\n</code></pre>"},{"location":"reference/ragcrawl/storage/#ragcrawl.storage.StorageBackend.list_versions","title":"<code>list_versions(page_id, limit=100)</code>  <code>abstractmethod</code>","text":"<p>List versions for a page.</p> Source code in <code>src/ragcrawl/storage/backend.py</code> <pre><code>@abstractmethod\ndef list_versions(\n    self,\n    page_id: str,\n    limit: int = 100,\n) -&gt; list[PageVersion]:\n    \"\"\"List versions for a page.\"\"\"\n    ...\n</code></pre>"},{"location":"reference/ragcrawl/storage/#ragcrawl.storage.StorageBackend.save_frontier_item","title":"<code>save_frontier_item(item)</code>  <code>abstractmethod</code>","text":"<p>Save a frontier item.</p> Source code in <code>src/ragcrawl/storage/backend.py</code> <pre><code>@abstractmethod\ndef save_frontier_item(self, item: FrontierItem) -&gt; None:\n    \"\"\"Save a frontier item.\"\"\"\n    ...\n</code></pre>"},{"location":"reference/ragcrawl/storage/#ragcrawl.storage.StorageBackend.save_page","title":"<code>save_page(page)</code>  <code>abstractmethod</code>","text":"<p>Save or update a page.</p> Source code in <code>src/ragcrawl/storage/backend.py</code> <pre><code>@abstractmethod\ndef save_page(self, page: Page) -&gt; None:\n    \"\"\"Save or update a page.\"\"\"\n    ...\n</code></pre>"},{"location":"reference/ragcrawl/storage/#ragcrawl.storage.StorageBackend.save_pages_bulk","title":"<code>save_pages_bulk(pages)</code>  <code>abstractmethod</code>","text":"<p>Bulk save pages. Returns count saved.</p> Source code in <code>src/ragcrawl/storage/backend.py</code> <pre><code>@abstractmethod\ndef save_pages_bulk(self, pages: list[Page]) -&gt; int:\n    \"\"\"Bulk save pages. Returns count saved.\"\"\"\n    ...\n</code></pre>"},{"location":"reference/ragcrawl/storage/#ragcrawl.storage.StorageBackend.save_run","title":"<code>save_run(run)</code>  <code>abstractmethod</code>","text":"<p>Save or update a crawl run.</p> Source code in <code>src/ragcrawl/storage/backend.py</code> <pre><code>@abstractmethod\ndef save_run(self, run: CrawlRun) -&gt; None:\n    \"\"\"Save or update a crawl run.\"\"\"\n    ...\n</code></pre>"},{"location":"reference/ragcrawl/storage/#ragcrawl.storage.StorageBackend.save_site","title":"<code>save_site(site)</code>  <code>abstractmethod</code>","text":"<p>Save or update a site.</p> Source code in <code>src/ragcrawl/storage/backend.py</code> <pre><code>@abstractmethod\ndef save_site(self, site: Site) -&gt; None:\n    \"\"\"Save or update a site.\"\"\"\n    ...\n</code></pre>"},{"location":"reference/ragcrawl/storage/#ragcrawl.storage.StorageBackend.save_version","title":"<code>save_version(version)</code>  <code>abstractmethod</code>","text":"<p>Save a page version.</p> Source code in <code>src/ragcrawl/storage/backend.py</code> <pre><code>@abstractmethod\ndef save_version(self, version: PageVersion) -&gt; None:\n    \"\"\"Save a page version.\"\"\"\n    ...\n</code></pre>"},{"location":"reference/ragcrawl/storage/#ragcrawl.storage.StorageBackend.save_versions_bulk","title":"<code>save_versions_bulk(versions)</code>  <code>abstractmethod</code>","text":"<p>Bulk save versions. Returns count saved.</p> Source code in <code>src/ragcrawl/storage/backend.py</code> <pre><code>@abstractmethod\ndef save_versions_bulk(self, versions: list[PageVersion]) -&gt; int:\n    \"\"\"Bulk save versions. Returns count saved.\"\"\"\n    ...\n</code></pre>"},{"location":"reference/ragcrawl/storage/#ragcrawl.storage.StorageBackend.update_frontier_status","title":"<code>update_frontier_status(item_id, status, error=None)</code>  <code>abstractmethod</code>","text":"<p>Update frontier item status.</p> Source code in <code>src/ragcrawl/storage/backend.py</code> <pre><code>@abstractmethod\ndef update_frontier_status(\n    self,\n    item_id: str,\n    status: str,\n    error: str | None = None,\n) -&gt; None:\n    \"\"\"Update frontier item status.\"\"\"\n    ...\n</code></pre>"},{"location":"reference/ragcrawl/storage/#ragcrawl.storage.create_storage_backend","title":"<code>create_storage_backend(config)</code>","text":"<p>Create a storage backend from configuration.</p> <p>Falls back to DuckDB if the configured backend is unavailable and fail_if_unavailable is False.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>StorageConfig</code> <p>Storage configuration.</p> required <p>Returns:</p> Type Description <code>StorageBackend</code> <p>A StorageBackend instance.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If backend unavailable and fail_if_unavailable is True.</p> Source code in <code>src/ragcrawl/storage/backend.py</code> <pre><code>def create_storage_backend(config: StorageConfig) -&gt; StorageBackend:\n    \"\"\"\n    Create a storage backend from configuration.\n\n    Falls back to DuckDB if the configured backend is unavailable\n    and fail_if_unavailable is False.\n\n    Args:\n        config: Storage configuration.\n\n    Returns:\n        A StorageBackend instance.\n\n    Raises:\n        RuntimeError: If backend unavailable and fail_if_unavailable is True.\n    \"\"\"\n    if config.storage_type == StorageType.DYNAMODB:\n        try:\n            from ragcrawl.storage.dynamodb.backend import DynamoDBBackend\n\n            assert isinstance(config.backend, DynamoDBConfig)\n            backend = DynamoDBBackend(config.backend)\n\n            if backend.health_check():\n                logger.info(\"Using DynamoDB storage backend\")\n                return backend\n            else:\n                raise RuntimeError(\"DynamoDB health check failed\")\n\n        except Exception as e:\n            if config.fail_if_unavailable:\n                raise RuntimeError(f\"DynamoDB unavailable: {e}\") from e\n\n            logger.warning(\n                \"DynamoDB unavailable, falling back to DuckDB\",\n                error=str(e),\n            )\n\n    # Default to DuckDB\n    from ragcrawl.storage.duckdb.backend import DuckDBBackend\n\n    if isinstance(config.backend, DuckDBConfig):\n        db_config = config.backend\n    else:\n        # Fallback config\n        db_config = DuckDBConfig()\n\n    logger.info(\"Using DuckDB storage backend\", path=str(db_config.path))\n    return DuckDBBackend(db_config)\n</code></pre>"},{"location":"reference/ragcrawl/storage/backend/","title":"Backend","text":""},{"location":"reference/ragcrawl/storage/backend/#ragcrawl.storage.backend","title":"<code>backend</code>","text":"<p>Storage backend protocol and factory.</p>"},{"location":"reference/ragcrawl/storage/backend/#ragcrawl.storage.backend.StorageBackend","title":"<code>StorageBackend</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for storage backends.</p> <p>All backends must implement this interface to ensure feature parity.</p>"},{"location":"reference/ragcrawl/storage/backend/#ragcrawl.storage.backend.StorageBackend.clear_frontier","title":"<code>clear_frontier(run_id)</code>  <code>abstractmethod</code>","text":"<p>Clear all frontier items for a run. Returns count deleted.</p> Source code in <code>src/ragcrawl/storage/backend.py</code> <pre><code>@abstractmethod\ndef clear_frontier(self, run_id: str) -&gt; int:\n    \"\"\"Clear all frontier items for a run. Returns count deleted.\"\"\"\n    ...\n</code></pre>"},{"location":"reference/ragcrawl/storage/backend/#ragcrawl.storage.backend.StorageBackend.close","title":"<code>close()</code>  <code>abstractmethod</code>","text":"<p>Close any connections.</p> Source code in <code>src/ragcrawl/storage/backend.py</code> <pre><code>@abstractmethod\ndef close(self) -&gt; None:\n    \"\"\"Close any connections.\"\"\"\n    ...\n</code></pre>"},{"location":"reference/ragcrawl/storage/backend/#ragcrawl.storage.backend.StorageBackend.count_pages","title":"<code>count_pages(site_id, include_tombstones=False)</code>  <code>abstractmethod</code>","text":"<p>Count pages for a site.</p> Source code in <code>src/ragcrawl/storage/backend.py</code> <pre><code>@abstractmethod\ndef count_pages(self, site_id: str, include_tombstones: bool = False) -&gt; int:\n    \"\"\"Count pages for a site.\"\"\"\n    ...\n</code></pre>"},{"location":"reference/ragcrawl/storage/backend/#ragcrawl.storage.backend.StorageBackend.delete_site","title":"<code>delete_site(site_id)</code>  <code>abstractmethod</code>","text":"<p>Delete a site and all associated data.</p> Source code in <code>src/ragcrawl/storage/backend.py</code> <pre><code>@abstractmethod\ndef delete_site(self, site_id: str) -&gt; bool:\n    \"\"\"Delete a site and all associated data.\"\"\"\n    ...\n</code></pre>"},{"location":"reference/ragcrawl/storage/backend/#ragcrawl.storage.backend.StorageBackend.get_current_version","title":"<code>get_current_version(page_id)</code>  <code>abstractmethod</code>","text":"<p>Get the current version for a page.</p> Source code in <code>src/ragcrawl/storage/backend.py</code> <pre><code>@abstractmethod\ndef get_current_version(self, page_id: str) -&gt; PageVersion | None:\n    \"\"\"Get the current version for a page.\"\"\"\n    ...\n</code></pre>"},{"location":"reference/ragcrawl/storage/backend/#ragcrawl.storage.backend.StorageBackend.get_frontier_items","title":"<code>get_frontier_items(run_id, status=None, limit=1000)</code>  <code>abstractmethod</code>","text":"<p>Get frontier items for a run.</p> Source code in <code>src/ragcrawl/storage/backend.py</code> <pre><code>@abstractmethod\ndef get_frontier_items(\n    self,\n    run_id: str,\n    status: str | None = None,\n    limit: int = 1000,\n) -&gt; list[FrontierItem]:\n    \"\"\"Get frontier items for a run.\"\"\"\n    ...\n</code></pre>"},{"location":"reference/ragcrawl/storage/backend/#ragcrawl.storage.backend.StorageBackend.get_latest_run","title":"<code>get_latest_run(site_id)</code>  <code>abstractmethod</code>","text":"<p>Get the latest crawl run for a site.</p> Source code in <code>src/ragcrawl/storage/backend.py</code> <pre><code>@abstractmethod\ndef get_latest_run(self, site_id: str) -&gt; CrawlRun | None:\n    \"\"\"Get the latest crawl run for a site.\"\"\"\n    ...\n</code></pre>"},{"location":"reference/ragcrawl/storage/backend/#ragcrawl.storage.backend.StorageBackend.get_page","title":"<code>get_page(page_id)</code>  <code>abstractmethod</code>","text":"<p>Get a page by ID.</p> Source code in <code>src/ragcrawl/storage/backend.py</code> <pre><code>@abstractmethod\ndef get_page(self, page_id: str) -&gt; Page | None:\n    \"\"\"Get a page by ID.\"\"\"\n    ...\n</code></pre>"},{"location":"reference/ragcrawl/storage/backend/#ragcrawl.storage.backend.StorageBackend.get_page_by_url","title":"<code>get_page_by_url(site_id, url)</code>  <code>abstractmethod</code>","text":"<p>Get a page by normalized URL.</p> Source code in <code>src/ragcrawl/storage/backend.py</code> <pre><code>@abstractmethod\ndef get_page_by_url(self, site_id: str, url: str) -&gt; Page | None:\n    \"\"\"Get a page by normalized URL.\"\"\"\n    ...\n</code></pre>"},{"location":"reference/ragcrawl/storage/backend/#ragcrawl.storage.backend.StorageBackend.get_pages_needing_recrawl","title":"<code>get_pages_needing_recrawl(site_id, max_age_hours=None, limit=1000)</code>  <code>abstractmethod</code>","text":"<p>Get pages that need to be re-crawled.</p> Source code in <code>src/ragcrawl/storage/backend.py</code> <pre><code>@abstractmethod\ndef get_pages_needing_recrawl(\n    self,\n    site_id: str,\n    max_age_hours: float | None = None,\n    limit: int = 1000,\n) -&gt; list[Page]:\n    \"\"\"Get pages that need to be re-crawled.\"\"\"\n    ...\n</code></pre>"},{"location":"reference/ragcrawl/storage/backend/#ragcrawl.storage.backend.StorageBackend.get_run","title":"<code>get_run(run_id)</code>  <code>abstractmethod</code>","text":"<p>Get a crawl run by ID.</p> Source code in <code>src/ragcrawl/storage/backend.py</code> <pre><code>@abstractmethod\ndef get_run(self, run_id: str) -&gt; CrawlRun | None:\n    \"\"\"Get a crawl run by ID.\"\"\"\n    ...\n</code></pre>"},{"location":"reference/ragcrawl/storage/backend/#ragcrawl.storage.backend.StorageBackend.get_site","title":"<code>get_site(site_id)</code>  <code>abstractmethod</code>","text":"<p>Get a site by ID.</p> Source code in <code>src/ragcrawl/storage/backend.py</code> <pre><code>@abstractmethod\ndef get_site(self, site_id: str) -&gt; Site | None:\n    \"\"\"Get a site by ID.\"\"\"\n    ...\n</code></pre>"},{"location":"reference/ragcrawl/storage/backend/#ragcrawl.storage.backend.StorageBackend.get_version","title":"<code>get_version(version_id)</code>  <code>abstractmethod</code>","text":"<p>Get a page version by ID.</p> Source code in <code>src/ragcrawl/storage/backend.py</code> <pre><code>@abstractmethod\ndef get_version(self, version_id: str) -&gt; PageVersion | None:\n    \"\"\"Get a page version by ID.\"\"\"\n    ...\n</code></pre>"},{"location":"reference/ragcrawl/storage/backend/#ragcrawl.storage.backend.StorageBackend.health_check","title":"<code>health_check()</code>  <code>abstractmethod</code>","text":"<p>Check if the backend is healthy/available.</p> Source code in <code>src/ragcrawl/storage/backend.py</code> <pre><code>@abstractmethod\ndef health_check(self) -&gt; bool:\n    \"\"\"Check if the backend is healthy/available.\"\"\"\n    ...\n</code></pre>"},{"location":"reference/ragcrawl/storage/backend/#ragcrawl.storage.backend.StorageBackend.initialize","title":"<code>initialize()</code>  <code>abstractmethod</code>","text":"<p>Initialize the storage backend (create tables, etc.).</p> Source code in <code>src/ragcrawl/storage/backend.py</code> <pre><code>@abstractmethod\ndef initialize(self) -&gt; None:\n    \"\"\"Initialize the storage backend (create tables, etc.).\"\"\"\n    ...\n</code></pre>"},{"location":"reference/ragcrawl/storage/backend/#ragcrawl.storage.backend.StorageBackend.list_pages","title":"<code>list_pages(site_id, limit=1000, offset=0, include_tombstones=False)</code>  <code>abstractmethod</code>","text":"<p>List pages for a site.</p> Source code in <code>src/ragcrawl/storage/backend.py</code> <pre><code>@abstractmethod\ndef list_pages(\n    self,\n    site_id: str,\n    limit: int = 1000,\n    offset: int = 0,\n    include_tombstones: bool = False,\n) -&gt; list[Page]:\n    \"\"\"List pages for a site.\"\"\"\n    ...\n</code></pre>"},{"location":"reference/ragcrawl/storage/backend/#ragcrawl.storage.backend.StorageBackend.list_runs","title":"<code>list_runs(site_id, limit=100, offset=0)</code>  <code>abstractmethod</code>","text":"<p>List crawl runs for a site.</p> Source code in <code>src/ragcrawl/storage/backend.py</code> <pre><code>@abstractmethod\ndef list_runs(\n    self,\n    site_id: str,\n    limit: int = 100,\n    offset: int = 0,\n) -&gt; list[CrawlRun]:\n    \"\"\"List crawl runs for a site.\"\"\"\n    ...\n</code></pre>"},{"location":"reference/ragcrawl/storage/backend/#ragcrawl.storage.backend.StorageBackend.list_sites","title":"<code>list_sites()</code>  <code>abstractmethod</code>","text":"<p>List all sites.</p> Source code in <code>src/ragcrawl/storage/backend.py</code> <pre><code>@abstractmethod\ndef list_sites(self) -&gt; list[Site]:\n    \"\"\"List all sites.\"\"\"\n    ...\n</code></pre>"},{"location":"reference/ragcrawl/storage/backend/#ragcrawl.storage.backend.StorageBackend.list_versions","title":"<code>list_versions(page_id, limit=100)</code>  <code>abstractmethod</code>","text":"<p>List versions for a page.</p> Source code in <code>src/ragcrawl/storage/backend.py</code> <pre><code>@abstractmethod\ndef list_versions(\n    self,\n    page_id: str,\n    limit: int = 100,\n) -&gt; list[PageVersion]:\n    \"\"\"List versions for a page.\"\"\"\n    ...\n</code></pre>"},{"location":"reference/ragcrawl/storage/backend/#ragcrawl.storage.backend.StorageBackend.save_frontier_item","title":"<code>save_frontier_item(item)</code>  <code>abstractmethod</code>","text":"<p>Save a frontier item.</p> Source code in <code>src/ragcrawl/storage/backend.py</code> <pre><code>@abstractmethod\ndef save_frontier_item(self, item: FrontierItem) -&gt; None:\n    \"\"\"Save a frontier item.\"\"\"\n    ...\n</code></pre>"},{"location":"reference/ragcrawl/storage/backend/#ragcrawl.storage.backend.StorageBackend.save_page","title":"<code>save_page(page)</code>  <code>abstractmethod</code>","text":"<p>Save or update a page.</p> Source code in <code>src/ragcrawl/storage/backend.py</code> <pre><code>@abstractmethod\ndef save_page(self, page: Page) -&gt; None:\n    \"\"\"Save or update a page.\"\"\"\n    ...\n</code></pre>"},{"location":"reference/ragcrawl/storage/backend/#ragcrawl.storage.backend.StorageBackend.save_pages_bulk","title":"<code>save_pages_bulk(pages)</code>  <code>abstractmethod</code>","text":"<p>Bulk save pages. Returns count saved.</p> Source code in <code>src/ragcrawl/storage/backend.py</code> <pre><code>@abstractmethod\ndef save_pages_bulk(self, pages: list[Page]) -&gt; int:\n    \"\"\"Bulk save pages. Returns count saved.\"\"\"\n    ...\n</code></pre>"},{"location":"reference/ragcrawl/storage/backend/#ragcrawl.storage.backend.StorageBackend.save_run","title":"<code>save_run(run)</code>  <code>abstractmethod</code>","text":"<p>Save or update a crawl run.</p> Source code in <code>src/ragcrawl/storage/backend.py</code> <pre><code>@abstractmethod\ndef save_run(self, run: CrawlRun) -&gt; None:\n    \"\"\"Save or update a crawl run.\"\"\"\n    ...\n</code></pre>"},{"location":"reference/ragcrawl/storage/backend/#ragcrawl.storage.backend.StorageBackend.save_site","title":"<code>save_site(site)</code>  <code>abstractmethod</code>","text":"<p>Save or update a site.</p> Source code in <code>src/ragcrawl/storage/backend.py</code> <pre><code>@abstractmethod\ndef save_site(self, site: Site) -&gt; None:\n    \"\"\"Save or update a site.\"\"\"\n    ...\n</code></pre>"},{"location":"reference/ragcrawl/storage/backend/#ragcrawl.storage.backend.StorageBackend.save_version","title":"<code>save_version(version)</code>  <code>abstractmethod</code>","text":"<p>Save a page version.</p> Source code in <code>src/ragcrawl/storage/backend.py</code> <pre><code>@abstractmethod\ndef save_version(self, version: PageVersion) -&gt; None:\n    \"\"\"Save a page version.\"\"\"\n    ...\n</code></pre>"},{"location":"reference/ragcrawl/storage/backend/#ragcrawl.storage.backend.StorageBackend.save_versions_bulk","title":"<code>save_versions_bulk(versions)</code>  <code>abstractmethod</code>","text":"<p>Bulk save versions. Returns count saved.</p> Source code in <code>src/ragcrawl/storage/backend.py</code> <pre><code>@abstractmethod\ndef save_versions_bulk(self, versions: list[PageVersion]) -&gt; int:\n    \"\"\"Bulk save versions. Returns count saved.\"\"\"\n    ...\n</code></pre>"},{"location":"reference/ragcrawl/storage/backend/#ragcrawl.storage.backend.StorageBackend.update_frontier_status","title":"<code>update_frontier_status(item_id, status, error=None)</code>  <code>abstractmethod</code>","text":"<p>Update frontier item status.</p> Source code in <code>src/ragcrawl/storage/backend.py</code> <pre><code>@abstractmethod\ndef update_frontier_status(\n    self,\n    item_id: str,\n    status: str,\n    error: str | None = None,\n) -&gt; None:\n    \"\"\"Update frontier item status.\"\"\"\n    ...\n</code></pre>"},{"location":"reference/ragcrawl/storage/backend/#ragcrawl.storage.backend.create_storage_backend","title":"<code>create_storage_backend(config)</code>","text":"<p>Create a storage backend from configuration.</p> <p>Falls back to DuckDB if the configured backend is unavailable and fail_if_unavailable is False.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>StorageConfig</code> <p>Storage configuration.</p> required <p>Returns:</p> Type Description <code>StorageBackend</code> <p>A StorageBackend instance.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If backend unavailable and fail_if_unavailable is True.</p> Source code in <code>src/ragcrawl/storage/backend.py</code> <pre><code>def create_storage_backend(config: StorageConfig) -&gt; StorageBackend:\n    \"\"\"\n    Create a storage backend from configuration.\n\n    Falls back to DuckDB if the configured backend is unavailable\n    and fail_if_unavailable is False.\n\n    Args:\n        config: Storage configuration.\n\n    Returns:\n        A StorageBackend instance.\n\n    Raises:\n        RuntimeError: If backend unavailable and fail_if_unavailable is True.\n    \"\"\"\n    if config.storage_type == StorageType.DYNAMODB:\n        try:\n            from ragcrawl.storage.dynamodb.backend import DynamoDBBackend\n\n            assert isinstance(config.backend, DynamoDBConfig)\n            backend = DynamoDBBackend(config.backend)\n\n            if backend.health_check():\n                logger.info(\"Using DynamoDB storage backend\")\n                return backend\n            else:\n                raise RuntimeError(\"DynamoDB health check failed\")\n\n        except Exception as e:\n            if config.fail_if_unavailable:\n                raise RuntimeError(f\"DynamoDB unavailable: {e}\") from e\n\n            logger.warning(\n                \"DynamoDB unavailable, falling back to DuckDB\",\n                error=str(e),\n            )\n\n    # Default to DuckDB\n    from ragcrawl.storage.duckdb.backend import DuckDBBackend\n\n    if isinstance(config.backend, DuckDBConfig):\n        db_config = config.backend\n    else:\n        # Fallback config\n        db_config = DuckDBConfig()\n\n    logger.info(\"Using DuckDB storage backend\", path=str(db_config.path))\n    return DuckDBBackend(db_config)\n</code></pre>"},{"location":"reference/ragcrawl/storage/duckdb/","title":"Index","text":""},{"location":"reference/ragcrawl/storage/duckdb/#ragcrawl.storage.duckdb","title":"<code>duckdb</code>","text":"<p>DuckDB storage backend for ragcrawl.</p>"},{"location":"reference/ragcrawl/storage/duckdb/#ragcrawl.storage.duckdb.DuckDBBackend","title":"<code>DuckDBBackend(config)</code>","text":"<p>               Bases: <code>StorageBackend</code></p> <p>DuckDB storage backend implementation.</p> <p>Provides local file-based storage with SQL capabilities.</p> <p>Initialize DuckDB backend.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>DuckDBConfig</code> <p>DuckDB configuration.</p> required Source code in <code>src/ragcrawl/storage/duckdb/backend.py</code> <pre><code>def __init__(self, config: DuckDBConfig) -&gt; None:\n    \"\"\"\n    Initialize DuckDB backend.\n\n    Args:\n        config: DuckDB configuration.\n    \"\"\"\n    self.config = config\n    self.db_path = Path(config.path)\n    self._conn: duckdb.DuckDBPyConnection | None = None\n</code></pre>"},{"location":"reference/ragcrawl/storage/duckdb/#ragcrawl.storage.duckdb.DuckDBBackend.conn","title":"<code>conn</code>  <code>property</code>","text":"<p>Get or create database connection.</p>"},{"location":"reference/ragcrawl/storage/duckdb/#ragcrawl.storage.duckdb.DuckDBBackend.clear_frontier","title":"<code>clear_frontier(run_id)</code>","text":"<p>Clear all frontier items for a run.</p> Source code in <code>src/ragcrawl/storage/duckdb/backend.py</code> <pre><code>def clear_frontier(self, run_id: str) -&gt; int:\n    \"\"\"Clear all frontier items for a run.\"\"\"\n    result = self.conn.execute(\n        \"DELETE FROM frontier_items WHERE run_id = ?\", [run_id]\n    )\n    return result.rowcount if hasattr(result, \"rowcount\") else 0\n</code></pre>"},{"location":"reference/ragcrawl/storage/duckdb/#ragcrawl.storage.duckdb.DuckDBBackend.close","title":"<code>close()</code>","text":"<p>Close the database connection.</p> Source code in <code>src/ragcrawl/storage/duckdb/backend.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Close the database connection.\"\"\"\n    if self._conn is not None:\n        self._conn.close()\n        self._conn = None\n</code></pre>"},{"location":"reference/ragcrawl/storage/duckdb/#ragcrawl.storage.duckdb.DuckDBBackend.count_pages","title":"<code>count_pages(site_id, include_tombstones=False)</code>","text":"<p>Count pages for a site.</p> Source code in <code>src/ragcrawl/storage/duckdb/backend.py</code> <pre><code>def count_pages(self, site_id: str, include_tombstones: bool = False) -&gt; int:\n    \"\"\"Count pages for a site.\"\"\"\n    query = \"SELECT COUNT(*) FROM pages WHERE site_id = ?\"\n    params: list[Any] = [site_id]\n\n    if not include_tombstones:\n        query += \" AND is_tombstone = FALSE\"\n\n    result = self.conn.execute(query, params).fetchone()\n    return result[0] if result else 0\n</code></pre>"},{"location":"reference/ragcrawl/storage/duckdb/#ragcrawl.storage.duckdb.DuckDBBackend.delete_site","title":"<code>delete_site(site_id)</code>","text":"<p>Delete a site and all associated data.</p> Source code in <code>src/ragcrawl/storage/duckdb/backend.py</code> <pre><code>def delete_site(self, site_id: str) -&gt; bool:\n    \"\"\"Delete a site and all associated data.\"\"\"\n    # Delete in order of dependencies\n    self.conn.execute(\"DELETE FROM frontier_items WHERE site_id = ?\", [site_id])\n    self.conn.execute(\"DELETE FROM page_versions WHERE site_id = ?\", [site_id])\n    self.conn.execute(\"DELETE FROM pages WHERE site_id = ?\", [site_id])\n    self.conn.execute(\"DELETE FROM crawl_runs WHERE site_id = ?\", [site_id])\n    result = self.conn.execute(\"DELETE FROM sites WHERE site_id = ?\", [site_id])\n\n    return result.rowcount &gt; 0 if hasattr(result, \"rowcount\") else True\n</code></pre>"},{"location":"reference/ragcrawl/storage/duckdb/#ragcrawl.storage.duckdb.DuckDBBackend.get_current_version","title":"<code>get_current_version(page_id)</code>","text":"<p>Get the current version for a page.</p> Source code in <code>src/ragcrawl/storage/duckdb/backend.py</code> <pre><code>def get_current_version(self, page_id: str) -&gt; PageVersion | None:\n    \"\"\"Get the current version for a page.\"\"\"\n    result = self.conn.execute(\n        \"\"\"\n        SELECT pv.* FROM page_versions pv\n        JOIN pages p ON p.current_version_id = pv.version_id\n        WHERE p.page_id = ?\n        \"\"\",\n        [page_id],\n    ).fetchone()\n\n    if result is None:\n        return None\n\n    return self._row_to_version(result)\n</code></pre>"},{"location":"reference/ragcrawl/storage/duckdb/#ragcrawl.storage.duckdb.DuckDBBackend.get_frontier_items","title":"<code>get_frontier_items(run_id, status=None, limit=1000)</code>","text":"<p>Get frontier items for a run.</p> Source code in <code>src/ragcrawl/storage/duckdb/backend.py</code> <pre><code>def get_frontier_items(\n    self,\n    run_id: str,\n    status: str | None = None,\n    limit: int = 1000,\n) -&gt; list[FrontierItem]:\n    \"\"\"Get frontier items for a run.\"\"\"\n    query = \"SELECT * FROM frontier_items WHERE run_id = ?\"\n    params: list[Any] = [run_id]\n\n    if status:\n        query += \" AND status = ?\"\n        params.append(status)\n\n    query += \" ORDER BY priority DESC, discovered_at ASC LIMIT ?\"\n    params.append(limit)\n\n    results = self.conn.execute(query, params).fetchall()\n    return [self._row_to_frontier_item(row) for row in results]\n</code></pre>"},{"location":"reference/ragcrawl/storage/duckdb/#ragcrawl.storage.duckdb.DuckDBBackend.get_latest_run","title":"<code>get_latest_run(site_id)</code>","text":"<p>Get the latest crawl run for a site.</p> Source code in <code>src/ragcrawl/storage/duckdb/backend.py</code> <pre><code>def get_latest_run(self, site_id: str) -&gt; CrawlRun | None:\n    \"\"\"Get the latest crawl run for a site.\"\"\"\n    result = self.conn.execute(\n        \"\"\"\n        SELECT * FROM crawl_runs\n        WHERE site_id = ?\n        ORDER BY created_at DESC\n        LIMIT 1\n        \"\"\",\n        [site_id],\n    ).fetchone()\n\n    if result is None:\n        return None\n\n    return self._row_to_run(result)\n</code></pre>"},{"location":"reference/ragcrawl/storage/duckdb/#ragcrawl.storage.duckdb.DuckDBBackend.get_page","title":"<code>get_page(page_id)</code>","text":"<p>Get a page by ID.</p> Source code in <code>src/ragcrawl/storage/duckdb/backend.py</code> <pre><code>def get_page(self, page_id: str) -&gt; Page | None:\n    \"\"\"Get a page by ID.\"\"\"\n    result = self.conn.execute(\n        \"SELECT * FROM pages WHERE page_id = ?\", [page_id]\n    ).fetchone()\n\n    if result is None:\n        return None\n\n    return self._row_to_page(result)\n</code></pre>"},{"location":"reference/ragcrawl/storage/duckdb/#ragcrawl.storage.duckdb.DuckDBBackend.get_page_by_url","title":"<code>get_page_by_url(site_id, url)</code>","text":"<p>Get a page by normalized URL.</p> Source code in <code>src/ragcrawl/storage/duckdb/backend.py</code> <pre><code>def get_page_by_url(self, site_id: str, url: str) -&gt; Page | None:\n    \"\"\"Get a page by normalized URL.\"\"\"\n    result = self.conn.execute(\n        \"SELECT * FROM pages WHERE site_id = ? AND url = ?\", [site_id, url]\n    ).fetchone()\n\n    if result is None:\n        return None\n\n    return self._row_to_page(result)\n</code></pre>"},{"location":"reference/ragcrawl/storage/duckdb/#ragcrawl.storage.duckdb.DuckDBBackend.get_pages_needing_recrawl","title":"<code>get_pages_needing_recrawl(site_id, max_age_hours=None, limit=1000)</code>","text":"<p>Get pages that need to be re-crawled.</p> Source code in <code>src/ragcrawl/storage/duckdb/backend.py</code> <pre><code>def get_pages_needing_recrawl(\n    self,\n    site_id: str,\n    max_age_hours: float | None = None,\n    limit: int = 1000,\n) -&gt; list[Page]:\n    \"\"\"Get pages that need to be re-crawled.\"\"\"\n    query = \"\"\"\n        SELECT * FROM pages\n        WHERE site_id = ? AND is_tombstone = FALSE\n    \"\"\"\n    params: list[Any] = [site_id]\n\n    if max_age_hours is not None:\n        query += \"\"\"\n            AND (last_crawled IS NULL OR\n                 last_crawled &lt; CURRENT_TIMESTAMP - INTERVAL ? HOUR)\n        \"\"\"\n        params.append(max_age_hours)\n\n    query += \" ORDER BY last_crawled ASC NULLS FIRST LIMIT ?\"\n    params.append(limit)\n\n    results = self.conn.execute(query, params).fetchall()\n    return [self._row_to_page(row) for row in results]\n</code></pre>"},{"location":"reference/ragcrawl/storage/duckdb/#ragcrawl.storage.duckdb.DuckDBBackend.get_run","title":"<code>get_run(run_id)</code>","text":"<p>Get a crawl run by ID.</p> Source code in <code>src/ragcrawl/storage/duckdb/backend.py</code> <pre><code>def get_run(self, run_id: str) -&gt; CrawlRun | None:\n    \"\"\"Get a crawl run by ID.\"\"\"\n    result = self.conn.execute(\n        \"SELECT * FROM crawl_runs WHERE run_id = ?\", [run_id]\n    ).fetchone()\n\n    if result is None:\n        return None\n\n    return self._row_to_run(result)\n</code></pre>"},{"location":"reference/ragcrawl/storage/duckdb/#ragcrawl.storage.duckdb.DuckDBBackend.get_site","title":"<code>get_site(site_id)</code>","text":"<p>Get a site by ID.</p> Source code in <code>src/ragcrawl/storage/duckdb/backend.py</code> <pre><code>def get_site(self, site_id: str) -&gt; Site | None:\n    \"\"\"Get a site by ID.\"\"\"\n    result = self.conn.execute(\n        \"SELECT * FROM sites WHERE site_id = ?\", [site_id]\n    ).fetchone()\n\n    if result is None:\n        return None\n\n    return self._row_to_site(result)\n</code></pre>"},{"location":"reference/ragcrawl/storage/duckdb/#ragcrawl.storage.duckdb.DuckDBBackend.get_version","title":"<code>get_version(version_id)</code>","text":"<p>Get a page version by ID.</p> Source code in <code>src/ragcrawl/storage/duckdb/backend.py</code> <pre><code>def get_version(self, version_id: str) -&gt; PageVersion | None:\n    \"\"\"Get a page version by ID.\"\"\"\n    result = self.conn.execute(\n        \"SELECT * FROM page_versions WHERE version_id = ?\", [version_id]\n    ).fetchone()\n\n    if result is None:\n        return None\n\n    return self._row_to_version(result)\n</code></pre>"},{"location":"reference/ragcrawl/storage/duckdb/#ragcrawl.storage.duckdb.DuckDBBackend.health_check","title":"<code>health_check()</code>","text":"<p>Check if the database is accessible.</p> Source code in <code>src/ragcrawl/storage/duckdb/backend.py</code> <pre><code>def health_check(self) -&gt; bool:\n    \"\"\"Check if the database is accessible.\"\"\"\n    try:\n        self.conn.execute(\"SELECT 1\").fetchone()\n        return True\n    except Exception:\n        return False\n</code></pre>"},{"location":"reference/ragcrawl/storage/duckdb/#ragcrawl.storage.duckdb.DuckDBBackend.initialize","title":"<code>initialize()</code>","text":"<p>Initialize the database schema.</p> Source code in <code>src/ragcrawl/storage/duckdb/backend.py</code> <pre><code>def initialize(self) -&gt; None:\n    \"\"\"Initialize the database schema.\"\"\"\n    for schema_sql in get_all_schemas():\n        self.conn.execute(schema_sql)\n</code></pre>"},{"location":"reference/ragcrawl/storage/duckdb/#ragcrawl.storage.duckdb.DuckDBBackend.list_pages","title":"<code>list_pages(site_id, limit=1000, offset=0, include_tombstones=False)</code>","text":"<p>List pages for a site.</p> Source code in <code>src/ragcrawl/storage/duckdb/backend.py</code> <pre><code>def list_pages(\n    self,\n    site_id: str,\n    limit: int = 1000,\n    offset: int = 0,\n    include_tombstones: bool = False,\n) -&gt; list[Page]:\n    \"\"\"List pages for a site.\"\"\"\n    query = \"\"\"\n        SELECT * FROM pages\n        WHERE site_id = ?\n    \"\"\"\n    params: list[Any] = [site_id]\n\n    if not include_tombstones:\n        query += \" AND is_tombstone = FALSE\"\n\n    query += \" ORDER BY last_seen DESC LIMIT ? OFFSET ?\"\n    params.extend([limit, offset])\n\n    results = self.conn.execute(query, params).fetchall()\n    return [self._row_to_page(row) for row in results]\n</code></pre>"},{"location":"reference/ragcrawl/storage/duckdb/#ragcrawl.storage.duckdb.DuckDBBackend.list_runs","title":"<code>list_runs(site_id, limit=100, offset=0)</code>","text":"<p>List crawl runs for a site.</p> Source code in <code>src/ragcrawl/storage/duckdb/backend.py</code> <pre><code>def list_runs(\n    self,\n    site_id: str,\n    limit: int = 100,\n    offset: int = 0,\n) -&gt; list[CrawlRun]:\n    \"\"\"List crawl runs for a site.\"\"\"\n    results = self.conn.execute(\n        \"\"\"\n        SELECT * FROM crawl_runs\n        WHERE site_id = ?\n        ORDER BY created_at DESC\n        LIMIT ? OFFSET ?\n        \"\"\",\n        [site_id, limit, offset],\n    ).fetchall()\n\n    return [self._row_to_run(row) for row in results]\n</code></pre>"},{"location":"reference/ragcrawl/storage/duckdb/#ragcrawl.storage.duckdb.DuckDBBackend.list_sites","title":"<code>list_sites()</code>","text":"<p>List all sites.</p> Source code in <code>src/ragcrawl/storage/duckdb/backend.py</code> <pre><code>def list_sites(self) -&gt; list[Site]:\n    \"\"\"List all sites.\"\"\"\n    results = self.conn.execute(\n        \"SELECT * FROM sites ORDER BY created_at DESC\"\n    ).fetchall()\n\n    return [self._row_to_site(row) for row in results]\n</code></pre>"},{"location":"reference/ragcrawl/storage/duckdb/#ragcrawl.storage.duckdb.DuckDBBackend.list_versions","title":"<code>list_versions(page_id, limit=100)</code>","text":"<p>List versions for a page.</p> Source code in <code>src/ragcrawl/storage/duckdb/backend.py</code> <pre><code>def list_versions(\n    self,\n    page_id: str,\n    limit: int = 100,\n) -&gt; list[PageVersion]:\n    \"\"\"List versions for a page.\"\"\"\n    results = self.conn.execute(\n        \"\"\"\n        SELECT * FROM page_versions\n        WHERE page_id = ?\n        ORDER BY crawled_at DESC\n        LIMIT ?\n        \"\"\",\n        [page_id, limit],\n    ).fetchall()\n\n    return [self._row_to_version(row) for row in results]\n</code></pre>"},{"location":"reference/ragcrawl/storage/duckdb/#ragcrawl.storage.duckdb.DuckDBBackend.save_frontier_item","title":"<code>save_frontier_item(item)</code>","text":"<p>Save a frontier item.</p> Source code in <code>src/ragcrawl/storage/duckdb/backend.py</code> <pre><code>def save_frontier_item(self, item: FrontierItem) -&gt; None:\n    \"\"\"Save a frontier item.\"\"\"\n    self.conn.execute(\n        \"\"\"\n        INSERT OR REPLACE INTO frontier_items (\n            item_id, run_id, site_id, url, normalized_url, url_hash,\n            depth, referrer_url, priority, status, retry_count, last_error,\n            discovered_at, scheduled_at, started_at, completed_at, domain\n        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n        \"\"\",\n        [\n            item.item_id,\n            item.run_id,\n            item.site_id,\n            item.url,\n            item.normalized_url,\n            item.url_hash,\n            item.depth,\n            item.referrer_url,\n            item.priority,\n            item.status.value,\n            item.retry_count,\n            item.last_error,\n            item.discovered_at,\n            item.scheduled_at,\n            item.started_at,\n            item.completed_at,\n            item.domain,\n        ],\n    )\n</code></pre>"},{"location":"reference/ragcrawl/storage/duckdb/#ragcrawl.storage.duckdb.DuckDBBackend.save_page","title":"<code>save_page(page)</code>","text":"<p>Save or update a page.</p> Source code in <code>src/ragcrawl/storage/duckdb/backend.py</code> <pre><code>def save_page(self, page: Page) -&gt; None:\n    \"\"\"Save or update a page.\"\"\"\n    # Use INSERT ... ON CONFLICT instead of INSERT OR REPLACE\n    # due to a DuckDB bug with boolean columns and INSERT OR REPLACE\n    self.conn.execute(\n        \"\"\"\n        INSERT INTO pages (\n            page_id, site_id, url, canonical_url, current_version_id,\n            content_hash, etag, last_modified, first_seen, last_seen,\n            last_crawled, last_changed, depth, referrer_url, status_code,\n            is_tombstone, error_count, last_error, version_count\n        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n        ON CONFLICT (page_id) DO UPDATE SET\n            site_id = excluded.site_id,\n            url = excluded.url,\n            canonical_url = excluded.canonical_url,\n            current_version_id = excluded.current_version_id,\n            content_hash = excluded.content_hash,\n            etag = excluded.etag,\n            last_modified = excluded.last_modified,\n            first_seen = excluded.first_seen,\n            last_seen = excluded.last_seen,\n            last_crawled = excluded.last_crawled,\n            last_changed = excluded.last_changed,\n            depth = excluded.depth,\n            referrer_url = excluded.referrer_url,\n            status_code = excluded.status_code,\n            is_tombstone = excluded.is_tombstone,\n            error_count = excluded.error_count,\n            last_error = excluded.last_error,\n            version_count = excluded.version_count\n        \"\"\",\n        [\n            page.page_id,\n            page.site_id,\n            page.url,\n            page.canonical_url,\n            page.current_version_id,\n            page.content_hash,\n            page.etag,\n            page.last_modified,\n            page.first_seen,\n            page.last_seen,\n            page.last_crawled,\n            page.last_changed,\n            page.depth,\n            page.referrer_url,\n            page.status_code,\n            page.is_tombstone,\n            page.error_count,\n            page.last_error,\n            page.version_count,\n        ],\n    )\n</code></pre>"},{"location":"reference/ragcrawl/storage/duckdb/#ragcrawl.storage.duckdb.DuckDBBackend.save_pages_bulk","title":"<code>save_pages_bulk(pages)</code>","text":"<p>Bulk save pages.</p> Source code in <code>src/ragcrawl/storage/duckdb/backend.py</code> <pre><code>def save_pages_bulk(self, pages: list[Page]) -&gt; int:\n    \"\"\"Bulk save pages.\"\"\"\n    for page in pages:\n        self.save_page(page)\n    return len(pages)\n</code></pre>"},{"location":"reference/ragcrawl/storage/duckdb/#ragcrawl.storage.duckdb.DuckDBBackend.save_run","title":"<code>save_run(run)</code>","text":"<p>Save or update a crawl run.</p> Source code in <code>src/ragcrawl/storage/duckdb/backend.py</code> <pre><code>def save_run(self, run: CrawlRun) -&gt; None:\n    \"\"\"Save or update a crawl run.\"\"\"\n    self.conn.execute(\n        \"\"\"\n        INSERT OR REPLACE INTO crawl_runs (\n            run_id, site_id, status, error_message, created_at, started_at,\n            completed_at, config_snapshot, seeds, is_sync, parent_run_id,\n            stats, frontier_size, max_depth_reached\n        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n        \"\"\",\n        [\n            run.run_id,\n            run.site_id,\n            run.status.value,\n            run.error_message,\n            run.created_at,\n            run.started_at,\n            run.completed_at,\n            json.dumps(run.config_snapshot, default=self._json_serializer),\n            json.dumps(run.seeds),\n            run.is_sync,\n            run.parent_run_id,\n            json.dumps(run.stats.model_dump(), default=self._json_serializer),\n            run.frontier_size,\n            run.max_depth_reached,\n        ],\n    )\n</code></pre>"},{"location":"reference/ragcrawl/storage/duckdb/#ragcrawl.storage.duckdb.DuckDBBackend.save_site","title":"<code>save_site(site)</code>","text":"<p>Save or update a site.</p> Source code in <code>src/ragcrawl/storage/duckdb/backend.py</code> <pre><code>def save_site(self, site: Site) -&gt; None:\n    \"\"\"Save or update a site.\"\"\"\n    self.conn.execute(\n        \"\"\"\n        INSERT OR REPLACE INTO sites (\n            site_id, name, seeds, allowed_domains, allowed_subdomains,\n            config, created_at, updated_at, last_crawl_at, last_sync_at,\n            total_pages, total_runs, is_active\n        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n        \"\"\",\n        [\n            site.site_id,\n            site.name,\n            json.dumps(site.seeds),\n            json.dumps(site.allowed_domains),\n            site.allowed_subdomains,\n            json.dumps(site.config, default=self._json_serializer),\n            site.created_at,\n            site.updated_at,\n            site.last_crawl_at,\n            site.last_sync_at,\n            site.total_pages,\n            site.total_runs,\n            site.is_active,\n        ],\n    )\n</code></pre>"},{"location":"reference/ragcrawl/storage/duckdb/#ragcrawl.storage.duckdb.DuckDBBackend.save_version","title":"<code>save_version(version)</code>","text":"<p>Save a page version.</p> Source code in <code>src/ragcrawl/storage/duckdb/backend.py</code> <pre><code>def save_version(self, version: PageVersion) -&gt; None:\n    \"\"\"Save a page version.\"\"\"\n    self.conn.execute(\n        \"\"\"\n        INSERT OR REPLACE INTO page_versions (\n            version_id, page_id, site_id, run_id, markdown, html, plain_text,\n            content_hash, raw_hash, url, canonical_url, title, description,\n            content_type, status_code, language, headings_outline, word_count,\n            char_count, outlinks, internal_link_count, external_link_count,\n            etag, last_modified, crawled_at, created_at, fetch_latency_ms,\n            extraction_latency_ms, is_tombstone, extra\n        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n        \"\"\",\n        [\n            version.version_id,\n            version.page_id,\n            version.site_id,\n            version.run_id,\n            version.markdown,\n            version.html,\n            version.plain_text,\n            version.content_hash,\n            version.raw_hash,\n            version.url,\n            version.canonical_url,\n            version.title,\n            version.description,\n            version.content_type,\n            version.status_code,\n            version.language,\n            json.dumps(version.headings_outline),\n            version.word_count,\n            version.char_count,\n            json.dumps(version.outlinks),\n            version.internal_link_count,\n            version.external_link_count,\n            version.etag,\n            version.last_modified,\n            version.crawled_at,\n            version.created_at,\n            version.fetch_latency_ms,\n            version.extraction_latency_ms,\n            version.is_tombstone,\n            json.dumps(version.extra),\n        ],\n    )\n</code></pre>"},{"location":"reference/ragcrawl/storage/duckdb/#ragcrawl.storage.duckdb.DuckDBBackend.save_versions_bulk","title":"<code>save_versions_bulk(versions)</code>","text":"<p>Bulk save versions.</p> Source code in <code>src/ragcrawl/storage/duckdb/backend.py</code> <pre><code>def save_versions_bulk(self, versions: list[PageVersion]) -&gt; int:\n    \"\"\"Bulk save versions.\"\"\"\n    for version in versions:\n        self.save_version(version)\n    return len(versions)\n</code></pre>"},{"location":"reference/ragcrawl/storage/duckdb/#ragcrawl.storage.duckdb.DuckDBBackend.update_frontier_status","title":"<code>update_frontier_status(item_id, status, error=None)</code>","text":"<p>Update frontier item status.</p> Source code in <code>src/ragcrawl/storage/duckdb/backend.py</code> <pre><code>def update_frontier_status(\n    self,\n    item_id: str,\n    status: str,\n    error: str | None = None,\n) -&gt; None:\n    \"\"\"Update frontier item status.\"\"\"\n    if error:\n        self.conn.execute(\n            \"\"\"\n            UPDATE frontier_items\n            SET status = ?, last_error = ?, completed_at = CURRENT_TIMESTAMP\n            WHERE item_id = ?\n            \"\"\",\n            [status, error, item_id],\n        )\n    else:\n        self.conn.execute(\n            \"\"\"\n            UPDATE frontier_items\n            SET status = ?, completed_at = CURRENT_TIMESTAMP\n            WHERE item_id = ?\n            \"\"\",\n            [status, item_id],\n        )\n</code></pre>"},{"location":"reference/ragcrawl/storage/duckdb/backend/","title":"Backend","text":""},{"location":"reference/ragcrawl/storage/duckdb/backend/#ragcrawl.storage.duckdb.backend","title":"<code>backend</code>","text":"<p>DuckDB storage backend implementation.</p>"},{"location":"reference/ragcrawl/storage/duckdb/backend/#ragcrawl.storage.duckdb.backend.DuckDBBackend","title":"<code>DuckDBBackend(config)</code>","text":"<p>               Bases: <code>StorageBackend</code></p> <p>DuckDB storage backend implementation.</p> <p>Provides local file-based storage with SQL capabilities.</p> <p>Initialize DuckDB backend.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>DuckDBConfig</code> <p>DuckDB configuration.</p> required Source code in <code>src/ragcrawl/storage/duckdb/backend.py</code> <pre><code>def __init__(self, config: DuckDBConfig) -&gt; None:\n    \"\"\"\n    Initialize DuckDB backend.\n\n    Args:\n        config: DuckDB configuration.\n    \"\"\"\n    self.config = config\n    self.db_path = Path(config.path)\n    self._conn: duckdb.DuckDBPyConnection | None = None\n</code></pre>"},{"location":"reference/ragcrawl/storage/duckdb/backend/#ragcrawl.storage.duckdb.backend.DuckDBBackend.conn","title":"<code>conn</code>  <code>property</code>","text":"<p>Get or create database connection.</p>"},{"location":"reference/ragcrawl/storage/duckdb/backend/#ragcrawl.storage.duckdb.backend.DuckDBBackend.clear_frontier","title":"<code>clear_frontier(run_id)</code>","text":"<p>Clear all frontier items for a run.</p> Source code in <code>src/ragcrawl/storage/duckdb/backend.py</code> <pre><code>def clear_frontier(self, run_id: str) -&gt; int:\n    \"\"\"Clear all frontier items for a run.\"\"\"\n    result = self.conn.execute(\n        \"DELETE FROM frontier_items WHERE run_id = ?\", [run_id]\n    )\n    return result.rowcount if hasattr(result, \"rowcount\") else 0\n</code></pre>"},{"location":"reference/ragcrawl/storage/duckdb/backend/#ragcrawl.storage.duckdb.backend.DuckDBBackend.close","title":"<code>close()</code>","text":"<p>Close the database connection.</p> Source code in <code>src/ragcrawl/storage/duckdb/backend.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Close the database connection.\"\"\"\n    if self._conn is not None:\n        self._conn.close()\n        self._conn = None\n</code></pre>"},{"location":"reference/ragcrawl/storage/duckdb/backend/#ragcrawl.storage.duckdb.backend.DuckDBBackend.count_pages","title":"<code>count_pages(site_id, include_tombstones=False)</code>","text":"<p>Count pages for a site.</p> Source code in <code>src/ragcrawl/storage/duckdb/backend.py</code> <pre><code>def count_pages(self, site_id: str, include_tombstones: bool = False) -&gt; int:\n    \"\"\"Count pages for a site.\"\"\"\n    query = \"SELECT COUNT(*) FROM pages WHERE site_id = ?\"\n    params: list[Any] = [site_id]\n\n    if not include_tombstones:\n        query += \" AND is_tombstone = FALSE\"\n\n    result = self.conn.execute(query, params).fetchone()\n    return result[0] if result else 0\n</code></pre>"},{"location":"reference/ragcrawl/storage/duckdb/backend/#ragcrawl.storage.duckdb.backend.DuckDBBackend.delete_site","title":"<code>delete_site(site_id)</code>","text":"<p>Delete a site and all associated data.</p> Source code in <code>src/ragcrawl/storage/duckdb/backend.py</code> <pre><code>def delete_site(self, site_id: str) -&gt; bool:\n    \"\"\"Delete a site and all associated data.\"\"\"\n    # Delete in order of dependencies\n    self.conn.execute(\"DELETE FROM frontier_items WHERE site_id = ?\", [site_id])\n    self.conn.execute(\"DELETE FROM page_versions WHERE site_id = ?\", [site_id])\n    self.conn.execute(\"DELETE FROM pages WHERE site_id = ?\", [site_id])\n    self.conn.execute(\"DELETE FROM crawl_runs WHERE site_id = ?\", [site_id])\n    result = self.conn.execute(\"DELETE FROM sites WHERE site_id = ?\", [site_id])\n\n    return result.rowcount &gt; 0 if hasattr(result, \"rowcount\") else True\n</code></pre>"},{"location":"reference/ragcrawl/storage/duckdb/backend/#ragcrawl.storage.duckdb.backend.DuckDBBackend.get_current_version","title":"<code>get_current_version(page_id)</code>","text":"<p>Get the current version for a page.</p> Source code in <code>src/ragcrawl/storage/duckdb/backend.py</code> <pre><code>def get_current_version(self, page_id: str) -&gt; PageVersion | None:\n    \"\"\"Get the current version for a page.\"\"\"\n    result = self.conn.execute(\n        \"\"\"\n        SELECT pv.* FROM page_versions pv\n        JOIN pages p ON p.current_version_id = pv.version_id\n        WHERE p.page_id = ?\n        \"\"\",\n        [page_id],\n    ).fetchone()\n\n    if result is None:\n        return None\n\n    return self._row_to_version(result)\n</code></pre>"},{"location":"reference/ragcrawl/storage/duckdb/backend/#ragcrawl.storage.duckdb.backend.DuckDBBackend.get_frontier_items","title":"<code>get_frontier_items(run_id, status=None, limit=1000)</code>","text":"<p>Get frontier items for a run.</p> Source code in <code>src/ragcrawl/storage/duckdb/backend.py</code> <pre><code>def get_frontier_items(\n    self,\n    run_id: str,\n    status: str | None = None,\n    limit: int = 1000,\n) -&gt; list[FrontierItem]:\n    \"\"\"Get frontier items for a run.\"\"\"\n    query = \"SELECT * FROM frontier_items WHERE run_id = ?\"\n    params: list[Any] = [run_id]\n\n    if status:\n        query += \" AND status = ?\"\n        params.append(status)\n\n    query += \" ORDER BY priority DESC, discovered_at ASC LIMIT ?\"\n    params.append(limit)\n\n    results = self.conn.execute(query, params).fetchall()\n    return [self._row_to_frontier_item(row) for row in results]\n</code></pre>"},{"location":"reference/ragcrawl/storage/duckdb/backend/#ragcrawl.storage.duckdb.backend.DuckDBBackend.get_latest_run","title":"<code>get_latest_run(site_id)</code>","text":"<p>Get the latest crawl run for a site.</p> Source code in <code>src/ragcrawl/storage/duckdb/backend.py</code> <pre><code>def get_latest_run(self, site_id: str) -&gt; CrawlRun | None:\n    \"\"\"Get the latest crawl run for a site.\"\"\"\n    result = self.conn.execute(\n        \"\"\"\n        SELECT * FROM crawl_runs\n        WHERE site_id = ?\n        ORDER BY created_at DESC\n        LIMIT 1\n        \"\"\",\n        [site_id],\n    ).fetchone()\n\n    if result is None:\n        return None\n\n    return self._row_to_run(result)\n</code></pre>"},{"location":"reference/ragcrawl/storage/duckdb/backend/#ragcrawl.storage.duckdb.backend.DuckDBBackend.get_page","title":"<code>get_page(page_id)</code>","text":"<p>Get a page by ID.</p> Source code in <code>src/ragcrawl/storage/duckdb/backend.py</code> <pre><code>def get_page(self, page_id: str) -&gt; Page | None:\n    \"\"\"Get a page by ID.\"\"\"\n    result = self.conn.execute(\n        \"SELECT * FROM pages WHERE page_id = ?\", [page_id]\n    ).fetchone()\n\n    if result is None:\n        return None\n\n    return self._row_to_page(result)\n</code></pre>"},{"location":"reference/ragcrawl/storage/duckdb/backend/#ragcrawl.storage.duckdb.backend.DuckDBBackend.get_page_by_url","title":"<code>get_page_by_url(site_id, url)</code>","text":"<p>Get a page by normalized URL.</p> Source code in <code>src/ragcrawl/storage/duckdb/backend.py</code> <pre><code>def get_page_by_url(self, site_id: str, url: str) -&gt; Page | None:\n    \"\"\"Get a page by normalized URL.\"\"\"\n    result = self.conn.execute(\n        \"SELECT * FROM pages WHERE site_id = ? AND url = ?\", [site_id, url]\n    ).fetchone()\n\n    if result is None:\n        return None\n\n    return self._row_to_page(result)\n</code></pre>"},{"location":"reference/ragcrawl/storage/duckdb/backend/#ragcrawl.storage.duckdb.backend.DuckDBBackend.get_pages_needing_recrawl","title":"<code>get_pages_needing_recrawl(site_id, max_age_hours=None, limit=1000)</code>","text":"<p>Get pages that need to be re-crawled.</p> Source code in <code>src/ragcrawl/storage/duckdb/backend.py</code> <pre><code>def get_pages_needing_recrawl(\n    self,\n    site_id: str,\n    max_age_hours: float | None = None,\n    limit: int = 1000,\n) -&gt; list[Page]:\n    \"\"\"Get pages that need to be re-crawled.\"\"\"\n    query = \"\"\"\n        SELECT * FROM pages\n        WHERE site_id = ? AND is_tombstone = FALSE\n    \"\"\"\n    params: list[Any] = [site_id]\n\n    if max_age_hours is not None:\n        query += \"\"\"\n            AND (last_crawled IS NULL OR\n                 last_crawled &lt; CURRENT_TIMESTAMP - INTERVAL ? HOUR)\n        \"\"\"\n        params.append(max_age_hours)\n\n    query += \" ORDER BY last_crawled ASC NULLS FIRST LIMIT ?\"\n    params.append(limit)\n\n    results = self.conn.execute(query, params).fetchall()\n    return [self._row_to_page(row) for row in results]\n</code></pre>"},{"location":"reference/ragcrawl/storage/duckdb/backend/#ragcrawl.storage.duckdb.backend.DuckDBBackend.get_run","title":"<code>get_run(run_id)</code>","text":"<p>Get a crawl run by ID.</p> Source code in <code>src/ragcrawl/storage/duckdb/backend.py</code> <pre><code>def get_run(self, run_id: str) -&gt; CrawlRun | None:\n    \"\"\"Get a crawl run by ID.\"\"\"\n    result = self.conn.execute(\n        \"SELECT * FROM crawl_runs WHERE run_id = ?\", [run_id]\n    ).fetchone()\n\n    if result is None:\n        return None\n\n    return self._row_to_run(result)\n</code></pre>"},{"location":"reference/ragcrawl/storage/duckdb/backend/#ragcrawl.storage.duckdb.backend.DuckDBBackend.get_site","title":"<code>get_site(site_id)</code>","text":"<p>Get a site by ID.</p> Source code in <code>src/ragcrawl/storage/duckdb/backend.py</code> <pre><code>def get_site(self, site_id: str) -&gt; Site | None:\n    \"\"\"Get a site by ID.\"\"\"\n    result = self.conn.execute(\n        \"SELECT * FROM sites WHERE site_id = ?\", [site_id]\n    ).fetchone()\n\n    if result is None:\n        return None\n\n    return self._row_to_site(result)\n</code></pre>"},{"location":"reference/ragcrawl/storage/duckdb/backend/#ragcrawl.storage.duckdb.backend.DuckDBBackend.get_version","title":"<code>get_version(version_id)</code>","text":"<p>Get a page version by ID.</p> Source code in <code>src/ragcrawl/storage/duckdb/backend.py</code> <pre><code>def get_version(self, version_id: str) -&gt; PageVersion | None:\n    \"\"\"Get a page version by ID.\"\"\"\n    result = self.conn.execute(\n        \"SELECT * FROM page_versions WHERE version_id = ?\", [version_id]\n    ).fetchone()\n\n    if result is None:\n        return None\n\n    return self._row_to_version(result)\n</code></pre>"},{"location":"reference/ragcrawl/storage/duckdb/backend/#ragcrawl.storage.duckdb.backend.DuckDBBackend.health_check","title":"<code>health_check()</code>","text":"<p>Check if the database is accessible.</p> Source code in <code>src/ragcrawl/storage/duckdb/backend.py</code> <pre><code>def health_check(self) -&gt; bool:\n    \"\"\"Check if the database is accessible.\"\"\"\n    try:\n        self.conn.execute(\"SELECT 1\").fetchone()\n        return True\n    except Exception:\n        return False\n</code></pre>"},{"location":"reference/ragcrawl/storage/duckdb/backend/#ragcrawl.storage.duckdb.backend.DuckDBBackend.initialize","title":"<code>initialize()</code>","text":"<p>Initialize the database schema.</p> Source code in <code>src/ragcrawl/storage/duckdb/backend.py</code> <pre><code>def initialize(self) -&gt; None:\n    \"\"\"Initialize the database schema.\"\"\"\n    for schema_sql in get_all_schemas():\n        self.conn.execute(schema_sql)\n</code></pre>"},{"location":"reference/ragcrawl/storage/duckdb/backend/#ragcrawl.storage.duckdb.backend.DuckDBBackend.list_pages","title":"<code>list_pages(site_id, limit=1000, offset=0, include_tombstones=False)</code>","text":"<p>List pages for a site.</p> Source code in <code>src/ragcrawl/storage/duckdb/backend.py</code> <pre><code>def list_pages(\n    self,\n    site_id: str,\n    limit: int = 1000,\n    offset: int = 0,\n    include_tombstones: bool = False,\n) -&gt; list[Page]:\n    \"\"\"List pages for a site.\"\"\"\n    query = \"\"\"\n        SELECT * FROM pages\n        WHERE site_id = ?\n    \"\"\"\n    params: list[Any] = [site_id]\n\n    if not include_tombstones:\n        query += \" AND is_tombstone = FALSE\"\n\n    query += \" ORDER BY last_seen DESC LIMIT ? OFFSET ?\"\n    params.extend([limit, offset])\n\n    results = self.conn.execute(query, params).fetchall()\n    return [self._row_to_page(row) for row in results]\n</code></pre>"},{"location":"reference/ragcrawl/storage/duckdb/backend/#ragcrawl.storage.duckdb.backend.DuckDBBackend.list_runs","title":"<code>list_runs(site_id, limit=100, offset=0)</code>","text":"<p>List crawl runs for a site.</p> Source code in <code>src/ragcrawl/storage/duckdb/backend.py</code> <pre><code>def list_runs(\n    self,\n    site_id: str,\n    limit: int = 100,\n    offset: int = 0,\n) -&gt; list[CrawlRun]:\n    \"\"\"List crawl runs for a site.\"\"\"\n    results = self.conn.execute(\n        \"\"\"\n        SELECT * FROM crawl_runs\n        WHERE site_id = ?\n        ORDER BY created_at DESC\n        LIMIT ? OFFSET ?\n        \"\"\",\n        [site_id, limit, offset],\n    ).fetchall()\n\n    return [self._row_to_run(row) for row in results]\n</code></pre>"},{"location":"reference/ragcrawl/storage/duckdb/backend/#ragcrawl.storage.duckdb.backend.DuckDBBackend.list_sites","title":"<code>list_sites()</code>","text":"<p>List all sites.</p> Source code in <code>src/ragcrawl/storage/duckdb/backend.py</code> <pre><code>def list_sites(self) -&gt; list[Site]:\n    \"\"\"List all sites.\"\"\"\n    results = self.conn.execute(\n        \"SELECT * FROM sites ORDER BY created_at DESC\"\n    ).fetchall()\n\n    return [self._row_to_site(row) for row in results]\n</code></pre>"},{"location":"reference/ragcrawl/storage/duckdb/backend/#ragcrawl.storage.duckdb.backend.DuckDBBackend.list_versions","title":"<code>list_versions(page_id, limit=100)</code>","text":"<p>List versions for a page.</p> Source code in <code>src/ragcrawl/storage/duckdb/backend.py</code> <pre><code>def list_versions(\n    self,\n    page_id: str,\n    limit: int = 100,\n) -&gt; list[PageVersion]:\n    \"\"\"List versions for a page.\"\"\"\n    results = self.conn.execute(\n        \"\"\"\n        SELECT * FROM page_versions\n        WHERE page_id = ?\n        ORDER BY crawled_at DESC\n        LIMIT ?\n        \"\"\",\n        [page_id, limit],\n    ).fetchall()\n\n    return [self._row_to_version(row) for row in results]\n</code></pre>"},{"location":"reference/ragcrawl/storage/duckdb/backend/#ragcrawl.storage.duckdb.backend.DuckDBBackend.save_frontier_item","title":"<code>save_frontier_item(item)</code>","text":"<p>Save a frontier item.</p> Source code in <code>src/ragcrawl/storage/duckdb/backend.py</code> <pre><code>def save_frontier_item(self, item: FrontierItem) -&gt; None:\n    \"\"\"Save a frontier item.\"\"\"\n    self.conn.execute(\n        \"\"\"\n        INSERT OR REPLACE INTO frontier_items (\n            item_id, run_id, site_id, url, normalized_url, url_hash,\n            depth, referrer_url, priority, status, retry_count, last_error,\n            discovered_at, scheduled_at, started_at, completed_at, domain\n        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n        \"\"\",\n        [\n            item.item_id,\n            item.run_id,\n            item.site_id,\n            item.url,\n            item.normalized_url,\n            item.url_hash,\n            item.depth,\n            item.referrer_url,\n            item.priority,\n            item.status.value,\n            item.retry_count,\n            item.last_error,\n            item.discovered_at,\n            item.scheduled_at,\n            item.started_at,\n            item.completed_at,\n            item.domain,\n        ],\n    )\n</code></pre>"},{"location":"reference/ragcrawl/storage/duckdb/backend/#ragcrawl.storage.duckdb.backend.DuckDBBackend.save_page","title":"<code>save_page(page)</code>","text":"<p>Save or update a page.</p> Source code in <code>src/ragcrawl/storage/duckdb/backend.py</code> <pre><code>def save_page(self, page: Page) -&gt; None:\n    \"\"\"Save or update a page.\"\"\"\n    # Use INSERT ... ON CONFLICT instead of INSERT OR REPLACE\n    # due to a DuckDB bug with boolean columns and INSERT OR REPLACE\n    self.conn.execute(\n        \"\"\"\n        INSERT INTO pages (\n            page_id, site_id, url, canonical_url, current_version_id,\n            content_hash, etag, last_modified, first_seen, last_seen,\n            last_crawled, last_changed, depth, referrer_url, status_code,\n            is_tombstone, error_count, last_error, version_count\n        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n        ON CONFLICT (page_id) DO UPDATE SET\n            site_id = excluded.site_id,\n            url = excluded.url,\n            canonical_url = excluded.canonical_url,\n            current_version_id = excluded.current_version_id,\n            content_hash = excluded.content_hash,\n            etag = excluded.etag,\n            last_modified = excluded.last_modified,\n            first_seen = excluded.first_seen,\n            last_seen = excluded.last_seen,\n            last_crawled = excluded.last_crawled,\n            last_changed = excluded.last_changed,\n            depth = excluded.depth,\n            referrer_url = excluded.referrer_url,\n            status_code = excluded.status_code,\n            is_tombstone = excluded.is_tombstone,\n            error_count = excluded.error_count,\n            last_error = excluded.last_error,\n            version_count = excluded.version_count\n        \"\"\",\n        [\n            page.page_id,\n            page.site_id,\n            page.url,\n            page.canonical_url,\n            page.current_version_id,\n            page.content_hash,\n            page.etag,\n            page.last_modified,\n            page.first_seen,\n            page.last_seen,\n            page.last_crawled,\n            page.last_changed,\n            page.depth,\n            page.referrer_url,\n            page.status_code,\n            page.is_tombstone,\n            page.error_count,\n            page.last_error,\n            page.version_count,\n        ],\n    )\n</code></pre>"},{"location":"reference/ragcrawl/storage/duckdb/backend/#ragcrawl.storage.duckdb.backend.DuckDBBackend.save_pages_bulk","title":"<code>save_pages_bulk(pages)</code>","text":"<p>Bulk save pages.</p> Source code in <code>src/ragcrawl/storage/duckdb/backend.py</code> <pre><code>def save_pages_bulk(self, pages: list[Page]) -&gt; int:\n    \"\"\"Bulk save pages.\"\"\"\n    for page in pages:\n        self.save_page(page)\n    return len(pages)\n</code></pre>"},{"location":"reference/ragcrawl/storage/duckdb/backend/#ragcrawl.storage.duckdb.backend.DuckDBBackend.save_run","title":"<code>save_run(run)</code>","text":"<p>Save or update a crawl run.</p> Source code in <code>src/ragcrawl/storage/duckdb/backend.py</code> <pre><code>def save_run(self, run: CrawlRun) -&gt; None:\n    \"\"\"Save or update a crawl run.\"\"\"\n    self.conn.execute(\n        \"\"\"\n        INSERT OR REPLACE INTO crawl_runs (\n            run_id, site_id, status, error_message, created_at, started_at,\n            completed_at, config_snapshot, seeds, is_sync, parent_run_id,\n            stats, frontier_size, max_depth_reached\n        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n        \"\"\",\n        [\n            run.run_id,\n            run.site_id,\n            run.status.value,\n            run.error_message,\n            run.created_at,\n            run.started_at,\n            run.completed_at,\n            json.dumps(run.config_snapshot, default=self._json_serializer),\n            json.dumps(run.seeds),\n            run.is_sync,\n            run.parent_run_id,\n            json.dumps(run.stats.model_dump(), default=self._json_serializer),\n            run.frontier_size,\n            run.max_depth_reached,\n        ],\n    )\n</code></pre>"},{"location":"reference/ragcrawl/storage/duckdb/backend/#ragcrawl.storage.duckdb.backend.DuckDBBackend.save_site","title":"<code>save_site(site)</code>","text":"<p>Save or update a site.</p> Source code in <code>src/ragcrawl/storage/duckdb/backend.py</code> <pre><code>def save_site(self, site: Site) -&gt; None:\n    \"\"\"Save or update a site.\"\"\"\n    self.conn.execute(\n        \"\"\"\n        INSERT OR REPLACE INTO sites (\n            site_id, name, seeds, allowed_domains, allowed_subdomains,\n            config, created_at, updated_at, last_crawl_at, last_sync_at,\n            total_pages, total_runs, is_active\n        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n        \"\"\",\n        [\n            site.site_id,\n            site.name,\n            json.dumps(site.seeds),\n            json.dumps(site.allowed_domains),\n            site.allowed_subdomains,\n            json.dumps(site.config, default=self._json_serializer),\n            site.created_at,\n            site.updated_at,\n            site.last_crawl_at,\n            site.last_sync_at,\n            site.total_pages,\n            site.total_runs,\n            site.is_active,\n        ],\n    )\n</code></pre>"},{"location":"reference/ragcrawl/storage/duckdb/backend/#ragcrawl.storage.duckdb.backend.DuckDBBackend.save_version","title":"<code>save_version(version)</code>","text":"<p>Save a page version.</p> Source code in <code>src/ragcrawl/storage/duckdb/backend.py</code> <pre><code>def save_version(self, version: PageVersion) -&gt; None:\n    \"\"\"Save a page version.\"\"\"\n    self.conn.execute(\n        \"\"\"\n        INSERT OR REPLACE INTO page_versions (\n            version_id, page_id, site_id, run_id, markdown, html, plain_text,\n            content_hash, raw_hash, url, canonical_url, title, description,\n            content_type, status_code, language, headings_outline, word_count,\n            char_count, outlinks, internal_link_count, external_link_count,\n            etag, last_modified, crawled_at, created_at, fetch_latency_ms,\n            extraction_latency_ms, is_tombstone, extra\n        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n        \"\"\",\n        [\n            version.version_id,\n            version.page_id,\n            version.site_id,\n            version.run_id,\n            version.markdown,\n            version.html,\n            version.plain_text,\n            version.content_hash,\n            version.raw_hash,\n            version.url,\n            version.canonical_url,\n            version.title,\n            version.description,\n            version.content_type,\n            version.status_code,\n            version.language,\n            json.dumps(version.headings_outline),\n            version.word_count,\n            version.char_count,\n            json.dumps(version.outlinks),\n            version.internal_link_count,\n            version.external_link_count,\n            version.etag,\n            version.last_modified,\n            version.crawled_at,\n            version.created_at,\n            version.fetch_latency_ms,\n            version.extraction_latency_ms,\n            version.is_tombstone,\n            json.dumps(version.extra),\n        ],\n    )\n</code></pre>"},{"location":"reference/ragcrawl/storage/duckdb/backend/#ragcrawl.storage.duckdb.backend.DuckDBBackend.save_versions_bulk","title":"<code>save_versions_bulk(versions)</code>","text":"<p>Bulk save versions.</p> Source code in <code>src/ragcrawl/storage/duckdb/backend.py</code> <pre><code>def save_versions_bulk(self, versions: list[PageVersion]) -&gt; int:\n    \"\"\"Bulk save versions.\"\"\"\n    for version in versions:\n        self.save_version(version)\n    return len(versions)\n</code></pre>"},{"location":"reference/ragcrawl/storage/duckdb/backend/#ragcrawl.storage.duckdb.backend.DuckDBBackend.update_frontier_status","title":"<code>update_frontier_status(item_id, status, error=None)</code>","text":"<p>Update frontier item status.</p> Source code in <code>src/ragcrawl/storage/duckdb/backend.py</code> <pre><code>def update_frontier_status(\n    self,\n    item_id: str,\n    status: str,\n    error: str | None = None,\n) -&gt; None:\n    \"\"\"Update frontier item status.\"\"\"\n    if error:\n        self.conn.execute(\n            \"\"\"\n            UPDATE frontier_items\n            SET status = ?, last_error = ?, completed_at = CURRENT_TIMESTAMP\n            WHERE item_id = ?\n            \"\"\",\n            [status, error, item_id],\n        )\n    else:\n        self.conn.execute(\n            \"\"\"\n            UPDATE frontier_items\n            SET status = ?, completed_at = CURRENT_TIMESTAMP\n            WHERE item_id = ?\n            \"\"\",\n            [status, item_id],\n        )\n</code></pre>"},{"location":"reference/ragcrawl/storage/duckdb/schema/","title":"Schema","text":""},{"location":"reference/ragcrawl/storage/duckdb/schema/#ragcrawl.storage.duckdb.schema","title":"<code>schema</code>","text":"<p>DuckDB schema definitions.</p>"},{"location":"reference/ragcrawl/storage/duckdb/schema/#ragcrawl.storage.duckdb.schema.get_all_schemas","title":"<code>get_all_schemas()</code>","text":"<p>Get all schema creation SQL statements.</p> Source code in <code>src/ragcrawl/storage/duckdb/schema.py</code> <pre><code>def get_all_schemas() -&gt; list[str]:\n    \"\"\"Get all schema creation SQL statements.\"\"\"\n    return ALL_SCHEMAS\n</code></pre>"},{"location":"reference/ragcrawl/storage/dynamodb/","title":"Index","text":""},{"location":"reference/ragcrawl/storage/dynamodb/#ragcrawl.storage.dynamodb","title":"<code>dynamodb</code>","text":"<p>DynamoDB storage backend for ragcrawl.</p>"},{"location":"reference/ragcrawl/storage/dynamodb/#ragcrawl.storage.dynamodb.CrawlRunModel","title":"<code>CrawlRunModel</code>","text":"<p>               Bases: <code>Model</code></p> <p>DynamoDB model for CrawlRun.</p>"},{"location":"reference/ragcrawl/storage/dynamodb/#ragcrawl.storage.dynamodb.DynamoDBBackend","title":"<code>DynamoDBBackend(config)</code>","text":"<p>               Bases: <code>StorageBackend</code></p> <p>DynamoDB storage backend implementation using PynamoDB.</p> <p>Initialize DynamoDB backend.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>DynamoDBConfig</code> <p>DynamoDB configuration.</p> required Source code in <code>src/ragcrawl/storage/dynamodb/backend.py</code> <pre><code>def __init__(self, config: DynamoDBConfig) -&gt; None:\n    \"\"\"\n    Initialize DynamoDB backend.\n\n    Args:\n        config: DynamoDB configuration.\n    \"\"\"\n    self.config = config\n    self._configure_models()\n</code></pre>"},{"location":"reference/ragcrawl/storage/dynamodb/#ragcrawl.storage.dynamodb.DynamoDBBackend.clear_frontier","title":"<code>clear_frontier(run_id)</code>","text":"<p>Clear all frontier items for a run.</p> Source code in <code>src/ragcrawl/storage/dynamodb/backend.py</code> <pre><code>def clear_frontier(self, run_id: str) -&gt; int:\n    \"\"\"Clear all frontier items for a run.\"\"\"\n    count = 0\n    for item in FrontierItemModel.run_index.query(run_id):\n        item.delete()\n        count += 1\n    return count\n</code></pre>"},{"location":"reference/ragcrawl/storage/dynamodb/#ragcrawl.storage.dynamodb.DynamoDBBackend.close","title":"<code>close()</code>","text":"<p>Close connections (no-op for DynamoDB).</p> Source code in <code>src/ragcrawl/storage/dynamodb/backend.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Close connections (no-op for DynamoDB).\"\"\"\n    pass\n</code></pre>"},{"location":"reference/ragcrawl/storage/dynamodb/#ragcrawl.storage.dynamodb.DynamoDBBackend.count_pages","title":"<code>count_pages(site_id, include_tombstones=False)</code>","text":"<p>Count pages for a site.</p> Source code in <code>src/ragcrawl/storage/dynamodb/backend.py</code> <pre><code>def count_pages(self, site_id: str, include_tombstones: bool = False) -&gt; int:\n    \"\"\"Count pages for a site.\"\"\"\n    filter_condition = PageModel.site_id == site_id\n    if not include_tombstones:\n        filter_condition &amp;= PageModel.is_tombstone == False\n\n    return PageModel.count(filter_condition)\n</code></pre>"},{"location":"reference/ragcrawl/storage/dynamodb/#ragcrawl.storage.dynamodb.DynamoDBBackend.delete_site","title":"<code>delete_site(site_id)</code>","text":"<p>Delete a site and all associated data.</p> Source code in <code>src/ragcrawl/storage/dynamodb/backend.py</code> <pre><code>def delete_site(self, site_id: str) -&gt; bool:\n    \"\"\"Delete a site and all associated data.\"\"\"\n    try:\n        # Delete associated data\n        for item in FrontierItemModel.scan(FrontierItemModel.site_id == site_id):\n            item.delete()\n        for item in PageVersionModel.scan(PageVersionModel.site_id == site_id):\n            item.delete()\n        for item in PageModel.scan(PageModel.site_id == site_id):\n            item.delete()\n        for item in CrawlRunModel.scan(CrawlRunModel.site_id == site_id):\n            item.delete()\n\n        # Delete site\n        site = SiteModel.get(site_id)\n        site.delete()\n        return True\n    except Exception:\n        return False\n</code></pre>"},{"location":"reference/ragcrawl/storage/dynamodb/#ragcrawl.storage.dynamodb.DynamoDBBackend.get_current_version","title":"<code>get_current_version(page_id)</code>","text":"<p>Get the current version for a page.</p> Source code in <code>src/ragcrawl/storage/dynamodb/backend.py</code> <pre><code>def get_current_version(self, page_id: str) -&gt; PageVersion | None:\n    \"\"\"Get the current version for a page.\"\"\"\n    page = self.get_page(page_id)\n    if page and page.current_version_id:\n        return self.get_version(page.current_version_id)\n    return None\n</code></pre>"},{"location":"reference/ragcrawl/storage/dynamodb/#ragcrawl.storage.dynamodb.DynamoDBBackend.get_frontier_items","title":"<code>get_frontier_items(run_id, status=None, limit=1000)</code>","text":"<p>Get frontier items for a run.</p> Source code in <code>src/ragcrawl/storage/dynamodb/backend.py</code> <pre><code>def get_frontier_items(\n    self,\n    run_id: str,\n    status: str | None = None,\n    limit: int = 1000,\n) -&gt; list[FrontierItem]:\n    \"\"\"Get frontier items for a run.\"\"\"\n    results = FrontierItemModel.run_index.query(\n        run_id,\n        scan_index_forward=False,\n        limit=limit,\n    )\n\n    items = [self._model_to_frontier_item(m) for m in results]\n\n    if status:\n        items = [i for i in items if i.status.value == status]\n\n    return items\n</code></pre>"},{"location":"reference/ragcrawl/storage/dynamodb/#ragcrawl.storage.dynamodb.DynamoDBBackend.get_latest_run","title":"<code>get_latest_run(site_id)</code>","text":"<p>Get the latest crawl run for a site.</p> Source code in <code>src/ragcrawl/storage/dynamodb/backend.py</code> <pre><code>def get_latest_run(self, site_id: str) -&gt; CrawlRun | None:\n    \"\"\"Get the latest crawl run for a site.\"\"\"\n    results = list(CrawlRunModel.site_index.query(\n        site_id,\n        scan_index_forward=False,\n        limit=1,\n    ))\n    return self._model_to_run(results[0]) if results else None\n</code></pre>"},{"location":"reference/ragcrawl/storage/dynamodb/#ragcrawl.storage.dynamodb.DynamoDBBackend.get_page","title":"<code>get_page(page_id)</code>","text":"<p>Get a page by ID.</p> Source code in <code>src/ragcrawl/storage/dynamodb/backend.py</code> <pre><code>def get_page(self, page_id: str) -&gt; Page | None:\n    \"\"\"Get a page by ID.\"\"\"\n    try:\n        model = PageModel.get(page_id)\n        return self._model_to_page(model)\n    except PageModel.DoesNotExist:\n        return None\n</code></pre>"},{"location":"reference/ragcrawl/storage/dynamodb/#ragcrawl.storage.dynamodb.DynamoDBBackend.get_page_by_url","title":"<code>get_page_by_url(site_id, url)</code>","text":"<p>Get a page by normalized URL.</p> Source code in <code>src/ragcrawl/storage/dynamodb/backend.py</code> <pre><code>def get_page_by_url(self, site_id: str, url: str) -&gt; Page | None:\n    \"\"\"Get a page by normalized URL.\"\"\"\n    # Need to scan with filter - consider adding GSI for URL lookups\n    for model in PageModel.scan((PageModel.site_id == site_id) &amp; (PageModel.url == url)):\n        return self._model_to_page(model)\n    return None\n</code></pre>"},{"location":"reference/ragcrawl/storage/dynamodb/#ragcrawl.storage.dynamodb.DynamoDBBackend.get_pages_needing_recrawl","title":"<code>get_pages_needing_recrawl(site_id, max_age_hours=None, limit=1000)</code>","text":"<p>Get pages that need to be re-crawled.</p> Source code in <code>src/ragcrawl/storage/dynamodb/backend.py</code> <pre><code>def get_pages_needing_recrawl(\n    self,\n    site_id: str,\n    max_age_hours: float | None = None,\n    limit: int = 1000,\n) -&gt; list[Page]:\n    \"\"\"Get pages that need to be re-crawled.\"\"\"\n    filter_condition = (PageModel.site_id == site_id) &amp; (PageModel.is_tombstone == False)\n\n    if max_age_hours is not None:\n        from datetime import timedelta\n        cutoff = datetime.now() - timedelta(hours=max_age_hours)\n        filter_condition &amp;= (PageModel.last_crawled &lt; cutoff) | (PageModel.last_crawled == None)\n\n    results = PageModel.scan(filter_condition, limit=limit)\n    return [self._model_to_page(m) for m in results]\n</code></pre>"},{"location":"reference/ragcrawl/storage/dynamodb/#ragcrawl.storage.dynamodb.DynamoDBBackend.get_run","title":"<code>get_run(run_id)</code>","text":"<p>Get a crawl run by ID.</p> Source code in <code>src/ragcrawl/storage/dynamodb/backend.py</code> <pre><code>def get_run(self, run_id: str) -&gt; CrawlRun | None:\n    \"\"\"Get a crawl run by ID.\"\"\"\n    try:\n        model = CrawlRunModel.get(run_id)\n        return self._model_to_run(model)\n    except CrawlRunModel.DoesNotExist:\n        return None\n</code></pre>"},{"location":"reference/ragcrawl/storage/dynamodb/#ragcrawl.storage.dynamodb.DynamoDBBackend.get_site","title":"<code>get_site(site_id)</code>","text":"<p>Get a site by ID.</p> Source code in <code>src/ragcrawl/storage/dynamodb/backend.py</code> <pre><code>def get_site(self, site_id: str) -&gt; Site | None:\n    \"\"\"Get a site by ID.\"\"\"\n    try:\n        model = SiteModel.get(site_id)\n        return self._model_to_site(model)\n    except SiteModel.DoesNotExist:\n        return None\n</code></pre>"},{"location":"reference/ragcrawl/storage/dynamodb/#ragcrawl.storage.dynamodb.DynamoDBBackend.get_version","title":"<code>get_version(version_id)</code>","text":"<p>Get a page version by ID.</p> Source code in <code>src/ragcrawl/storage/dynamodb/backend.py</code> <pre><code>def get_version(self, version_id: str) -&gt; PageVersion | None:\n    \"\"\"Get a page version by ID.\"\"\"\n    try:\n        model = PageVersionModel.get(version_id)\n        return self._model_to_version(model)\n    except PageVersionModel.DoesNotExist:\n        return None\n</code></pre>"},{"location":"reference/ragcrawl/storage/dynamodb/#ragcrawl.storage.dynamodb.DynamoDBBackend.health_check","title":"<code>health_check()</code>","text":"<p>Check if DynamoDB is accessible.</p> Source code in <code>src/ragcrawl/storage/dynamodb/backend.py</code> <pre><code>def health_check(self) -&gt; bool:\n    \"\"\"Check if DynamoDB is accessible.\"\"\"\n    try:\n        # Try to describe one table\n        SiteModel.exists()\n        return True\n    except Exception as e:\n        logger.error(\"DynamoDB health check failed\", error=str(e))\n        return False\n</code></pre>"},{"location":"reference/ragcrawl/storage/dynamodb/#ragcrawl.storage.dynamodb.DynamoDBBackend.initialize","title":"<code>initialize()</code>","text":"<p>Create tables if they don't exist.</p> Source code in <code>src/ragcrawl/storage/dynamodb/backend.py</code> <pre><code>def initialize(self) -&gt; None:\n    \"\"\"Create tables if they don't exist.\"\"\"\n    for model_class in [\n        SiteModel,\n        CrawlRunModel,\n        PageModel,\n        PageVersionModel,\n        FrontierItemModel,\n    ]:\n        if not model_class.exists():\n            model_class.create_table(\n                read_capacity_units=self.config.read_capacity_units,\n                write_capacity_units=self.config.write_capacity_units,\n                wait=True,\n            )\n            logger.info(f\"Created table: {model_class.Meta.table_name}\")\n</code></pre>"},{"location":"reference/ragcrawl/storage/dynamodb/#ragcrawl.storage.dynamodb.DynamoDBBackend.list_pages","title":"<code>list_pages(site_id, limit=1000, offset=0, include_tombstones=False)</code>","text":"<p>List pages for a site.</p> Source code in <code>src/ragcrawl/storage/dynamodb/backend.py</code> <pre><code>def list_pages(\n    self,\n    site_id: str,\n    limit: int = 1000,\n    offset: int = 0,\n    include_tombstones: bool = False,\n) -&gt; list[Page]:\n    \"\"\"List pages for a site.\"\"\"\n    filter_condition = PageModel.site_id == site_id\n    if not include_tombstones:\n        filter_condition &amp;= PageModel.is_tombstone == False\n\n    results = PageModel.scan(filter_condition, limit=limit)\n    return [self._model_to_page(m) for m in results]\n</code></pre>"},{"location":"reference/ragcrawl/storage/dynamodb/#ragcrawl.storage.dynamodb.DynamoDBBackend.list_runs","title":"<code>list_runs(site_id, limit=100, offset=0)</code>","text":"<p>List crawl runs for a site.</p> Source code in <code>src/ragcrawl/storage/dynamodb/backend.py</code> <pre><code>def list_runs(\n    self,\n    site_id: str,\n    limit: int = 100,\n    offset: int = 0,\n) -&gt; list[CrawlRun]:\n    \"\"\"List crawl runs for a site.\"\"\"\n    # Use GSI to query by site_id\n    results = CrawlRunModel.site_index.query(\n        site_id,\n        scan_index_forward=False,\n        limit=limit,\n    )\n    return [self._model_to_run(m) for m in results]\n</code></pre>"},{"location":"reference/ragcrawl/storage/dynamodb/#ragcrawl.storage.dynamodb.DynamoDBBackend.list_sites","title":"<code>list_sites()</code>","text":"<p>List all sites.</p> Source code in <code>src/ragcrawl/storage/dynamodb/backend.py</code> <pre><code>def list_sites(self) -&gt; list[Site]:\n    \"\"\"List all sites.\"\"\"\n    return [self._model_to_site(m) for m in SiteModel.scan()]\n</code></pre>"},{"location":"reference/ragcrawl/storage/dynamodb/#ragcrawl.storage.dynamodb.DynamoDBBackend.list_versions","title":"<code>list_versions(page_id, limit=100)</code>","text":"<p>List versions for a page.</p> Source code in <code>src/ragcrawl/storage/dynamodb/backend.py</code> <pre><code>def list_versions(\n    self,\n    page_id: str,\n    limit: int = 100,\n) -&gt; list[PageVersion]:\n    \"\"\"List versions for a page.\"\"\"\n    results = PageVersionModel.page_index.query(\n        page_id,\n        scan_index_forward=False,\n        limit=limit,\n    )\n    return [self._model_to_version(m) for m in results]\n</code></pre>"},{"location":"reference/ragcrawl/storage/dynamodb/#ragcrawl.storage.dynamodb.DynamoDBBackend.save_frontier_item","title":"<code>save_frontier_item(item)</code>","text":"<p>Save a frontier item.</p> Source code in <code>src/ragcrawl/storage/dynamodb/backend.py</code> <pre><code>def save_frontier_item(self, item: FrontierItem) -&gt; None:\n    \"\"\"Save a frontier item.\"\"\"\n    model = FrontierItemModel(\n        item_id=item.item_id,\n        run_id=item.run_id,\n        site_id=item.site_id,\n        url=item.url,\n        normalized_url=item.normalized_url,\n        url_hash=item.url_hash,\n        depth=item.depth,\n        referrer_url=item.referrer_url,\n        priority=item.priority,\n        status=item.status.value,\n        retry_count=item.retry_count,\n        last_error=item.last_error,\n        discovered_at=item.discovered_at,\n        scheduled_at=item.scheduled_at,\n        started_at=item.started_at,\n        completed_at=item.completed_at,\n        domain=item.domain,\n    )\n    model.save()\n</code></pre>"},{"location":"reference/ragcrawl/storage/dynamodb/#ragcrawl.storage.dynamodb.DynamoDBBackend.save_page","title":"<code>save_page(page)</code>","text":"<p>Save or update a page.</p> Source code in <code>src/ragcrawl/storage/dynamodb/backend.py</code> <pre><code>def save_page(self, page: Page) -&gt; None:\n    \"\"\"Save or update a page.\"\"\"\n    model = PageModel(\n        page_id=page.page_id,\n        site_id=page.site_id,\n        url=page.url,\n        canonical_url=page.canonical_url,\n        current_version_id=page.current_version_id,\n        content_hash=page.content_hash,\n        etag=page.etag,\n        last_modified=page.last_modified,\n        first_seen=page.first_seen,\n        last_seen=page.last_seen,\n        last_crawled=page.last_crawled,\n        last_changed=page.last_changed,\n        depth=page.depth,\n        referrer_url=page.referrer_url,\n        status_code=page.status_code,\n        is_tombstone=page.is_tombstone,\n        error_count=page.error_count,\n        last_error=page.last_error,\n        version_count=page.version_count,\n    )\n    model.save()\n</code></pre>"},{"location":"reference/ragcrawl/storage/dynamodb/#ragcrawl.storage.dynamodb.DynamoDBBackend.save_pages_bulk","title":"<code>save_pages_bulk(pages)</code>","text":"<p>Bulk save pages.</p> Source code in <code>src/ragcrawl/storage/dynamodb/backend.py</code> <pre><code>def save_pages_bulk(self, pages: list[Page]) -&gt; int:\n    \"\"\"Bulk save pages.\"\"\"\n    with PageModel.batch_write() as batch:\n        for page in pages:\n            model = PageModel(\n                page_id=page.page_id,\n                site_id=page.site_id,\n                url=page.url,\n                # ... other fields\n            )\n            batch.save(model)\n    return len(pages)\n</code></pre>"},{"location":"reference/ragcrawl/storage/dynamodb/#ragcrawl.storage.dynamodb.DynamoDBBackend.save_run","title":"<code>save_run(run)</code>","text":"<p>Save or update a crawl run.</p> Source code in <code>src/ragcrawl/storage/dynamodb/backend.py</code> <pre><code>def save_run(self, run: CrawlRun) -&gt; None:\n    \"\"\"Save or update a crawl run.\"\"\"\n    model = CrawlRunModel(\n        run_id=run.run_id,\n        site_id=run.site_id,\n        status=run.status.value,\n        error_message=run.error_message,\n        created_at=run.created_at,\n        started_at=run.started_at,\n        completed_at=run.completed_at,\n        config_snapshot=run.config_snapshot,\n        seeds=run.seeds,\n        is_sync=run.is_sync,\n        parent_run_id=run.parent_run_id,\n        stats=run.stats.model_dump() if run.stats else None,\n        frontier_size=run.frontier_size,\n        max_depth_reached=run.max_depth_reached,\n    )\n    model.save()\n</code></pre>"},{"location":"reference/ragcrawl/storage/dynamodb/#ragcrawl.storage.dynamodb.DynamoDBBackend.save_site","title":"<code>save_site(site)</code>","text":"<p>Save or update a site.</p> Source code in <code>src/ragcrawl/storage/dynamodb/backend.py</code> <pre><code>def save_site(self, site: Site) -&gt; None:\n    \"\"\"Save or update a site.\"\"\"\n    model = SiteModel(\n        site_id=site.site_id,\n        name=site.name,\n        seeds=site.seeds,\n        allowed_domains=site.allowed_domains,\n        allowed_subdomains=site.allowed_subdomains,\n        config=site.config,\n        created_at=site.created_at,\n        updated_at=site.updated_at,\n        last_crawl_at=site.last_crawl_at,\n        last_sync_at=site.last_sync_at,\n        total_pages=site.total_pages,\n        total_runs=site.total_runs,\n        is_active=site.is_active,\n    )\n    model.save()\n</code></pre>"},{"location":"reference/ragcrawl/storage/dynamodb/#ragcrawl.storage.dynamodb.DynamoDBBackend.save_version","title":"<code>save_version(version)</code>","text":"<p>Save a page version.</p> Source code in <code>src/ragcrawl/storage/dynamodb/backend.py</code> <pre><code>def save_version(self, version: PageVersion) -&gt; None:\n    \"\"\"Save a page version.\"\"\"\n    model = PageVersionModel(\n        version_id=version.version_id,\n        page_id=version.page_id,\n        site_id=version.site_id,\n        run_id=version.run_id,\n        markdown=version.markdown,\n        html=version.html,\n        plain_text=version.plain_text,\n        content_hash=version.content_hash,\n        raw_hash=version.raw_hash,\n        url=version.url,\n        canonical_url=version.canonical_url,\n        title=version.title,\n        description=version.description,\n        content_type=version.content_type,\n        status_code=version.status_code,\n        language=version.language,\n        headings_outline=version.headings_outline,\n        word_count=version.word_count,\n        char_count=version.char_count,\n        outlinks=version.outlinks,\n        internal_link_count=version.internal_link_count,\n        external_link_count=version.external_link_count,\n        etag=version.etag,\n        last_modified_header=version.last_modified,\n        crawled_at=version.crawled_at,\n        created_at=version.created_at,\n        fetch_latency_ms=version.fetch_latency_ms,\n        extraction_latency_ms=version.extraction_latency_ms,\n        is_tombstone=version.is_tombstone,\n        extra=version.extra,\n    )\n    model.save()\n</code></pre>"},{"location":"reference/ragcrawl/storage/dynamodb/#ragcrawl.storage.dynamodb.DynamoDBBackend.save_versions_bulk","title":"<code>save_versions_bulk(versions)</code>","text":"<p>Bulk save versions.</p> Source code in <code>src/ragcrawl/storage/dynamodb/backend.py</code> <pre><code>def save_versions_bulk(self, versions: list[PageVersion]) -&gt; int:\n    \"\"\"Bulk save versions.\"\"\"\n    with PageVersionModel.batch_write() as batch:\n        for version in versions:\n            model = PageVersionModel(\n                version_id=version.version_id,\n                page_id=version.page_id,\n                # ... other fields\n            )\n            batch.save(model)\n    return len(versions)\n</code></pre>"},{"location":"reference/ragcrawl/storage/dynamodb/#ragcrawl.storage.dynamodb.DynamoDBBackend.update_frontier_status","title":"<code>update_frontier_status(item_id, status, error=None)</code>","text":"<p>Update frontier item status.</p> Source code in <code>src/ragcrawl/storage/dynamodb/backend.py</code> <pre><code>def update_frontier_status(\n    self,\n    item_id: str,\n    status: str,\n    error: str | None = None,\n) -&gt; None:\n    \"\"\"Update frontier item status.\"\"\"\n    try:\n        model = FrontierItemModel.get(item_id)\n        model.status = status\n        if error:\n            model.last_error = error\n        model.completed_at = datetime.now()\n        model.save()\n    except FrontierItemModel.DoesNotExist:\n        pass\n</code></pre>"},{"location":"reference/ragcrawl/storage/dynamodb/#ragcrawl.storage.dynamodb.FrontierItemModel","title":"<code>FrontierItemModel</code>","text":"<p>               Bases: <code>Model</code></p> <p>DynamoDB model for FrontierItem.</p>"},{"location":"reference/ragcrawl/storage/dynamodb/#ragcrawl.storage.dynamodb.PageModel","title":"<code>PageModel</code>","text":"<p>               Bases: <code>Model</code></p> <p>DynamoDB model for Page.</p>"},{"location":"reference/ragcrawl/storage/dynamodb/#ragcrawl.storage.dynamodb.PageVersionModel","title":"<code>PageVersionModel</code>","text":"<p>               Bases: <code>Model</code></p> <p>DynamoDB model for PageVersion.</p>"},{"location":"reference/ragcrawl/storage/dynamodb/#ragcrawl.storage.dynamodb.SiteModel","title":"<code>SiteModel</code>","text":"<p>               Bases: <code>Model</code></p> <p>DynamoDB model for Site.</p>"},{"location":"reference/ragcrawl/storage/dynamodb/backend/","title":"Backend","text":""},{"location":"reference/ragcrawl/storage/dynamodb/backend/#ragcrawl.storage.dynamodb.backend","title":"<code>backend</code>","text":"<p>DynamoDB storage backend implementation.</p>"},{"location":"reference/ragcrawl/storage/dynamodb/backend/#ragcrawl.storage.dynamodb.backend.DynamoDBBackend","title":"<code>DynamoDBBackend(config)</code>","text":"<p>               Bases: <code>StorageBackend</code></p> <p>DynamoDB storage backend implementation using PynamoDB.</p> <p>Initialize DynamoDB backend.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>DynamoDBConfig</code> <p>DynamoDB configuration.</p> required Source code in <code>src/ragcrawl/storage/dynamodb/backend.py</code> <pre><code>def __init__(self, config: DynamoDBConfig) -&gt; None:\n    \"\"\"\n    Initialize DynamoDB backend.\n\n    Args:\n        config: DynamoDB configuration.\n    \"\"\"\n    self.config = config\n    self._configure_models()\n</code></pre>"},{"location":"reference/ragcrawl/storage/dynamodb/backend/#ragcrawl.storage.dynamodb.backend.DynamoDBBackend.clear_frontier","title":"<code>clear_frontier(run_id)</code>","text":"<p>Clear all frontier items for a run.</p> Source code in <code>src/ragcrawl/storage/dynamodb/backend.py</code> <pre><code>def clear_frontier(self, run_id: str) -&gt; int:\n    \"\"\"Clear all frontier items for a run.\"\"\"\n    count = 0\n    for item in FrontierItemModel.run_index.query(run_id):\n        item.delete()\n        count += 1\n    return count\n</code></pre>"},{"location":"reference/ragcrawl/storage/dynamodb/backend/#ragcrawl.storage.dynamodb.backend.DynamoDBBackend.close","title":"<code>close()</code>","text":"<p>Close connections (no-op for DynamoDB).</p> Source code in <code>src/ragcrawl/storage/dynamodb/backend.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Close connections (no-op for DynamoDB).\"\"\"\n    pass\n</code></pre>"},{"location":"reference/ragcrawl/storage/dynamodb/backend/#ragcrawl.storage.dynamodb.backend.DynamoDBBackend.count_pages","title":"<code>count_pages(site_id, include_tombstones=False)</code>","text":"<p>Count pages for a site.</p> Source code in <code>src/ragcrawl/storage/dynamodb/backend.py</code> <pre><code>def count_pages(self, site_id: str, include_tombstones: bool = False) -&gt; int:\n    \"\"\"Count pages for a site.\"\"\"\n    filter_condition = PageModel.site_id == site_id\n    if not include_tombstones:\n        filter_condition &amp;= PageModel.is_tombstone == False\n\n    return PageModel.count(filter_condition)\n</code></pre>"},{"location":"reference/ragcrawl/storage/dynamodb/backend/#ragcrawl.storage.dynamodb.backend.DynamoDBBackend.delete_site","title":"<code>delete_site(site_id)</code>","text":"<p>Delete a site and all associated data.</p> Source code in <code>src/ragcrawl/storage/dynamodb/backend.py</code> <pre><code>def delete_site(self, site_id: str) -&gt; bool:\n    \"\"\"Delete a site and all associated data.\"\"\"\n    try:\n        # Delete associated data\n        for item in FrontierItemModel.scan(FrontierItemModel.site_id == site_id):\n            item.delete()\n        for item in PageVersionModel.scan(PageVersionModel.site_id == site_id):\n            item.delete()\n        for item in PageModel.scan(PageModel.site_id == site_id):\n            item.delete()\n        for item in CrawlRunModel.scan(CrawlRunModel.site_id == site_id):\n            item.delete()\n\n        # Delete site\n        site = SiteModel.get(site_id)\n        site.delete()\n        return True\n    except Exception:\n        return False\n</code></pre>"},{"location":"reference/ragcrawl/storage/dynamodb/backend/#ragcrawl.storage.dynamodb.backend.DynamoDBBackend.get_current_version","title":"<code>get_current_version(page_id)</code>","text":"<p>Get the current version for a page.</p> Source code in <code>src/ragcrawl/storage/dynamodb/backend.py</code> <pre><code>def get_current_version(self, page_id: str) -&gt; PageVersion | None:\n    \"\"\"Get the current version for a page.\"\"\"\n    page = self.get_page(page_id)\n    if page and page.current_version_id:\n        return self.get_version(page.current_version_id)\n    return None\n</code></pre>"},{"location":"reference/ragcrawl/storage/dynamodb/backend/#ragcrawl.storage.dynamodb.backend.DynamoDBBackend.get_frontier_items","title":"<code>get_frontier_items(run_id, status=None, limit=1000)</code>","text":"<p>Get frontier items for a run.</p> Source code in <code>src/ragcrawl/storage/dynamodb/backend.py</code> <pre><code>def get_frontier_items(\n    self,\n    run_id: str,\n    status: str | None = None,\n    limit: int = 1000,\n) -&gt; list[FrontierItem]:\n    \"\"\"Get frontier items for a run.\"\"\"\n    results = FrontierItemModel.run_index.query(\n        run_id,\n        scan_index_forward=False,\n        limit=limit,\n    )\n\n    items = [self._model_to_frontier_item(m) for m in results]\n\n    if status:\n        items = [i for i in items if i.status.value == status]\n\n    return items\n</code></pre>"},{"location":"reference/ragcrawl/storage/dynamodb/backend/#ragcrawl.storage.dynamodb.backend.DynamoDBBackend.get_latest_run","title":"<code>get_latest_run(site_id)</code>","text":"<p>Get the latest crawl run for a site.</p> Source code in <code>src/ragcrawl/storage/dynamodb/backend.py</code> <pre><code>def get_latest_run(self, site_id: str) -&gt; CrawlRun | None:\n    \"\"\"Get the latest crawl run for a site.\"\"\"\n    results = list(CrawlRunModel.site_index.query(\n        site_id,\n        scan_index_forward=False,\n        limit=1,\n    ))\n    return self._model_to_run(results[0]) if results else None\n</code></pre>"},{"location":"reference/ragcrawl/storage/dynamodb/backend/#ragcrawl.storage.dynamodb.backend.DynamoDBBackend.get_page","title":"<code>get_page(page_id)</code>","text":"<p>Get a page by ID.</p> Source code in <code>src/ragcrawl/storage/dynamodb/backend.py</code> <pre><code>def get_page(self, page_id: str) -&gt; Page | None:\n    \"\"\"Get a page by ID.\"\"\"\n    try:\n        model = PageModel.get(page_id)\n        return self._model_to_page(model)\n    except PageModel.DoesNotExist:\n        return None\n</code></pre>"},{"location":"reference/ragcrawl/storage/dynamodb/backend/#ragcrawl.storage.dynamodb.backend.DynamoDBBackend.get_page_by_url","title":"<code>get_page_by_url(site_id, url)</code>","text":"<p>Get a page by normalized URL.</p> Source code in <code>src/ragcrawl/storage/dynamodb/backend.py</code> <pre><code>def get_page_by_url(self, site_id: str, url: str) -&gt; Page | None:\n    \"\"\"Get a page by normalized URL.\"\"\"\n    # Need to scan with filter - consider adding GSI for URL lookups\n    for model in PageModel.scan((PageModel.site_id == site_id) &amp; (PageModel.url == url)):\n        return self._model_to_page(model)\n    return None\n</code></pre>"},{"location":"reference/ragcrawl/storage/dynamodb/backend/#ragcrawl.storage.dynamodb.backend.DynamoDBBackend.get_pages_needing_recrawl","title":"<code>get_pages_needing_recrawl(site_id, max_age_hours=None, limit=1000)</code>","text":"<p>Get pages that need to be re-crawled.</p> Source code in <code>src/ragcrawl/storage/dynamodb/backend.py</code> <pre><code>def get_pages_needing_recrawl(\n    self,\n    site_id: str,\n    max_age_hours: float | None = None,\n    limit: int = 1000,\n) -&gt; list[Page]:\n    \"\"\"Get pages that need to be re-crawled.\"\"\"\n    filter_condition = (PageModel.site_id == site_id) &amp; (PageModel.is_tombstone == False)\n\n    if max_age_hours is not None:\n        from datetime import timedelta\n        cutoff = datetime.now() - timedelta(hours=max_age_hours)\n        filter_condition &amp;= (PageModel.last_crawled &lt; cutoff) | (PageModel.last_crawled == None)\n\n    results = PageModel.scan(filter_condition, limit=limit)\n    return [self._model_to_page(m) for m in results]\n</code></pre>"},{"location":"reference/ragcrawl/storage/dynamodb/backend/#ragcrawl.storage.dynamodb.backend.DynamoDBBackend.get_run","title":"<code>get_run(run_id)</code>","text":"<p>Get a crawl run by ID.</p> Source code in <code>src/ragcrawl/storage/dynamodb/backend.py</code> <pre><code>def get_run(self, run_id: str) -&gt; CrawlRun | None:\n    \"\"\"Get a crawl run by ID.\"\"\"\n    try:\n        model = CrawlRunModel.get(run_id)\n        return self._model_to_run(model)\n    except CrawlRunModel.DoesNotExist:\n        return None\n</code></pre>"},{"location":"reference/ragcrawl/storage/dynamodb/backend/#ragcrawl.storage.dynamodb.backend.DynamoDBBackend.get_site","title":"<code>get_site(site_id)</code>","text":"<p>Get a site by ID.</p> Source code in <code>src/ragcrawl/storage/dynamodb/backend.py</code> <pre><code>def get_site(self, site_id: str) -&gt; Site | None:\n    \"\"\"Get a site by ID.\"\"\"\n    try:\n        model = SiteModel.get(site_id)\n        return self._model_to_site(model)\n    except SiteModel.DoesNotExist:\n        return None\n</code></pre>"},{"location":"reference/ragcrawl/storage/dynamodb/backend/#ragcrawl.storage.dynamodb.backend.DynamoDBBackend.get_version","title":"<code>get_version(version_id)</code>","text":"<p>Get a page version by ID.</p> Source code in <code>src/ragcrawl/storage/dynamodb/backend.py</code> <pre><code>def get_version(self, version_id: str) -&gt; PageVersion | None:\n    \"\"\"Get a page version by ID.\"\"\"\n    try:\n        model = PageVersionModel.get(version_id)\n        return self._model_to_version(model)\n    except PageVersionModel.DoesNotExist:\n        return None\n</code></pre>"},{"location":"reference/ragcrawl/storage/dynamodb/backend/#ragcrawl.storage.dynamodb.backend.DynamoDBBackend.health_check","title":"<code>health_check()</code>","text":"<p>Check if DynamoDB is accessible.</p> Source code in <code>src/ragcrawl/storage/dynamodb/backend.py</code> <pre><code>def health_check(self) -&gt; bool:\n    \"\"\"Check if DynamoDB is accessible.\"\"\"\n    try:\n        # Try to describe one table\n        SiteModel.exists()\n        return True\n    except Exception as e:\n        logger.error(\"DynamoDB health check failed\", error=str(e))\n        return False\n</code></pre>"},{"location":"reference/ragcrawl/storage/dynamodb/backend/#ragcrawl.storage.dynamodb.backend.DynamoDBBackend.initialize","title":"<code>initialize()</code>","text":"<p>Create tables if they don't exist.</p> Source code in <code>src/ragcrawl/storage/dynamodb/backend.py</code> <pre><code>def initialize(self) -&gt; None:\n    \"\"\"Create tables if they don't exist.\"\"\"\n    for model_class in [\n        SiteModel,\n        CrawlRunModel,\n        PageModel,\n        PageVersionModel,\n        FrontierItemModel,\n    ]:\n        if not model_class.exists():\n            model_class.create_table(\n                read_capacity_units=self.config.read_capacity_units,\n                write_capacity_units=self.config.write_capacity_units,\n                wait=True,\n            )\n            logger.info(f\"Created table: {model_class.Meta.table_name}\")\n</code></pre>"},{"location":"reference/ragcrawl/storage/dynamodb/backend/#ragcrawl.storage.dynamodb.backend.DynamoDBBackend.list_pages","title":"<code>list_pages(site_id, limit=1000, offset=0, include_tombstones=False)</code>","text":"<p>List pages for a site.</p> Source code in <code>src/ragcrawl/storage/dynamodb/backend.py</code> <pre><code>def list_pages(\n    self,\n    site_id: str,\n    limit: int = 1000,\n    offset: int = 0,\n    include_tombstones: bool = False,\n) -&gt; list[Page]:\n    \"\"\"List pages for a site.\"\"\"\n    filter_condition = PageModel.site_id == site_id\n    if not include_tombstones:\n        filter_condition &amp;= PageModel.is_tombstone == False\n\n    results = PageModel.scan(filter_condition, limit=limit)\n    return [self._model_to_page(m) for m in results]\n</code></pre>"},{"location":"reference/ragcrawl/storage/dynamodb/backend/#ragcrawl.storage.dynamodb.backend.DynamoDBBackend.list_runs","title":"<code>list_runs(site_id, limit=100, offset=0)</code>","text":"<p>List crawl runs for a site.</p> Source code in <code>src/ragcrawl/storage/dynamodb/backend.py</code> <pre><code>def list_runs(\n    self,\n    site_id: str,\n    limit: int = 100,\n    offset: int = 0,\n) -&gt; list[CrawlRun]:\n    \"\"\"List crawl runs for a site.\"\"\"\n    # Use GSI to query by site_id\n    results = CrawlRunModel.site_index.query(\n        site_id,\n        scan_index_forward=False,\n        limit=limit,\n    )\n    return [self._model_to_run(m) for m in results]\n</code></pre>"},{"location":"reference/ragcrawl/storage/dynamodb/backend/#ragcrawl.storage.dynamodb.backend.DynamoDBBackend.list_sites","title":"<code>list_sites()</code>","text":"<p>List all sites.</p> Source code in <code>src/ragcrawl/storage/dynamodb/backend.py</code> <pre><code>def list_sites(self) -&gt; list[Site]:\n    \"\"\"List all sites.\"\"\"\n    return [self._model_to_site(m) for m in SiteModel.scan()]\n</code></pre>"},{"location":"reference/ragcrawl/storage/dynamodb/backend/#ragcrawl.storage.dynamodb.backend.DynamoDBBackend.list_versions","title":"<code>list_versions(page_id, limit=100)</code>","text":"<p>List versions for a page.</p> Source code in <code>src/ragcrawl/storage/dynamodb/backend.py</code> <pre><code>def list_versions(\n    self,\n    page_id: str,\n    limit: int = 100,\n) -&gt; list[PageVersion]:\n    \"\"\"List versions for a page.\"\"\"\n    results = PageVersionModel.page_index.query(\n        page_id,\n        scan_index_forward=False,\n        limit=limit,\n    )\n    return [self._model_to_version(m) for m in results]\n</code></pre>"},{"location":"reference/ragcrawl/storage/dynamodb/backend/#ragcrawl.storage.dynamodb.backend.DynamoDBBackend.save_frontier_item","title":"<code>save_frontier_item(item)</code>","text":"<p>Save a frontier item.</p> Source code in <code>src/ragcrawl/storage/dynamodb/backend.py</code> <pre><code>def save_frontier_item(self, item: FrontierItem) -&gt; None:\n    \"\"\"Save a frontier item.\"\"\"\n    model = FrontierItemModel(\n        item_id=item.item_id,\n        run_id=item.run_id,\n        site_id=item.site_id,\n        url=item.url,\n        normalized_url=item.normalized_url,\n        url_hash=item.url_hash,\n        depth=item.depth,\n        referrer_url=item.referrer_url,\n        priority=item.priority,\n        status=item.status.value,\n        retry_count=item.retry_count,\n        last_error=item.last_error,\n        discovered_at=item.discovered_at,\n        scheduled_at=item.scheduled_at,\n        started_at=item.started_at,\n        completed_at=item.completed_at,\n        domain=item.domain,\n    )\n    model.save()\n</code></pre>"},{"location":"reference/ragcrawl/storage/dynamodb/backend/#ragcrawl.storage.dynamodb.backend.DynamoDBBackend.save_page","title":"<code>save_page(page)</code>","text":"<p>Save or update a page.</p> Source code in <code>src/ragcrawl/storage/dynamodb/backend.py</code> <pre><code>def save_page(self, page: Page) -&gt; None:\n    \"\"\"Save or update a page.\"\"\"\n    model = PageModel(\n        page_id=page.page_id,\n        site_id=page.site_id,\n        url=page.url,\n        canonical_url=page.canonical_url,\n        current_version_id=page.current_version_id,\n        content_hash=page.content_hash,\n        etag=page.etag,\n        last_modified=page.last_modified,\n        first_seen=page.first_seen,\n        last_seen=page.last_seen,\n        last_crawled=page.last_crawled,\n        last_changed=page.last_changed,\n        depth=page.depth,\n        referrer_url=page.referrer_url,\n        status_code=page.status_code,\n        is_tombstone=page.is_tombstone,\n        error_count=page.error_count,\n        last_error=page.last_error,\n        version_count=page.version_count,\n    )\n    model.save()\n</code></pre>"},{"location":"reference/ragcrawl/storage/dynamodb/backend/#ragcrawl.storage.dynamodb.backend.DynamoDBBackend.save_pages_bulk","title":"<code>save_pages_bulk(pages)</code>","text":"<p>Bulk save pages.</p> Source code in <code>src/ragcrawl/storage/dynamodb/backend.py</code> <pre><code>def save_pages_bulk(self, pages: list[Page]) -&gt; int:\n    \"\"\"Bulk save pages.\"\"\"\n    with PageModel.batch_write() as batch:\n        for page in pages:\n            model = PageModel(\n                page_id=page.page_id,\n                site_id=page.site_id,\n                url=page.url,\n                # ... other fields\n            )\n            batch.save(model)\n    return len(pages)\n</code></pre>"},{"location":"reference/ragcrawl/storage/dynamodb/backend/#ragcrawl.storage.dynamodb.backend.DynamoDBBackend.save_run","title":"<code>save_run(run)</code>","text":"<p>Save or update a crawl run.</p> Source code in <code>src/ragcrawl/storage/dynamodb/backend.py</code> <pre><code>def save_run(self, run: CrawlRun) -&gt; None:\n    \"\"\"Save or update a crawl run.\"\"\"\n    model = CrawlRunModel(\n        run_id=run.run_id,\n        site_id=run.site_id,\n        status=run.status.value,\n        error_message=run.error_message,\n        created_at=run.created_at,\n        started_at=run.started_at,\n        completed_at=run.completed_at,\n        config_snapshot=run.config_snapshot,\n        seeds=run.seeds,\n        is_sync=run.is_sync,\n        parent_run_id=run.parent_run_id,\n        stats=run.stats.model_dump() if run.stats else None,\n        frontier_size=run.frontier_size,\n        max_depth_reached=run.max_depth_reached,\n    )\n    model.save()\n</code></pre>"},{"location":"reference/ragcrawl/storage/dynamodb/backend/#ragcrawl.storage.dynamodb.backend.DynamoDBBackend.save_site","title":"<code>save_site(site)</code>","text":"<p>Save or update a site.</p> Source code in <code>src/ragcrawl/storage/dynamodb/backend.py</code> <pre><code>def save_site(self, site: Site) -&gt; None:\n    \"\"\"Save or update a site.\"\"\"\n    model = SiteModel(\n        site_id=site.site_id,\n        name=site.name,\n        seeds=site.seeds,\n        allowed_domains=site.allowed_domains,\n        allowed_subdomains=site.allowed_subdomains,\n        config=site.config,\n        created_at=site.created_at,\n        updated_at=site.updated_at,\n        last_crawl_at=site.last_crawl_at,\n        last_sync_at=site.last_sync_at,\n        total_pages=site.total_pages,\n        total_runs=site.total_runs,\n        is_active=site.is_active,\n    )\n    model.save()\n</code></pre>"},{"location":"reference/ragcrawl/storage/dynamodb/backend/#ragcrawl.storage.dynamodb.backend.DynamoDBBackend.save_version","title":"<code>save_version(version)</code>","text":"<p>Save a page version.</p> Source code in <code>src/ragcrawl/storage/dynamodb/backend.py</code> <pre><code>def save_version(self, version: PageVersion) -&gt; None:\n    \"\"\"Save a page version.\"\"\"\n    model = PageVersionModel(\n        version_id=version.version_id,\n        page_id=version.page_id,\n        site_id=version.site_id,\n        run_id=version.run_id,\n        markdown=version.markdown,\n        html=version.html,\n        plain_text=version.plain_text,\n        content_hash=version.content_hash,\n        raw_hash=version.raw_hash,\n        url=version.url,\n        canonical_url=version.canonical_url,\n        title=version.title,\n        description=version.description,\n        content_type=version.content_type,\n        status_code=version.status_code,\n        language=version.language,\n        headings_outline=version.headings_outline,\n        word_count=version.word_count,\n        char_count=version.char_count,\n        outlinks=version.outlinks,\n        internal_link_count=version.internal_link_count,\n        external_link_count=version.external_link_count,\n        etag=version.etag,\n        last_modified_header=version.last_modified,\n        crawled_at=version.crawled_at,\n        created_at=version.created_at,\n        fetch_latency_ms=version.fetch_latency_ms,\n        extraction_latency_ms=version.extraction_latency_ms,\n        is_tombstone=version.is_tombstone,\n        extra=version.extra,\n    )\n    model.save()\n</code></pre>"},{"location":"reference/ragcrawl/storage/dynamodb/backend/#ragcrawl.storage.dynamodb.backend.DynamoDBBackend.save_versions_bulk","title":"<code>save_versions_bulk(versions)</code>","text":"<p>Bulk save versions.</p> Source code in <code>src/ragcrawl/storage/dynamodb/backend.py</code> <pre><code>def save_versions_bulk(self, versions: list[PageVersion]) -&gt; int:\n    \"\"\"Bulk save versions.\"\"\"\n    with PageVersionModel.batch_write() as batch:\n        for version in versions:\n            model = PageVersionModel(\n                version_id=version.version_id,\n                page_id=version.page_id,\n                # ... other fields\n            )\n            batch.save(model)\n    return len(versions)\n</code></pre>"},{"location":"reference/ragcrawl/storage/dynamodb/backend/#ragcrawl.storage.dynamodb.backend.DynamoDBBackend.update_frontier_status","title":"<code>update_frontier_status(item_id, status, error=None)</code>","text":"<p>Update frontier item status.</p> Source code in <code>src/ragcrawl/storage/dynamodb/backend.py</code> <pre><code>def update_frontier_status(\n    self,\n    item_id: str,\n    status: str,\n    error: str | None = None,\n) -&gt; None:\n    \"\"\"Update frontier item status.\"\"\"\n    try:\n        model = FrontierItemModel.get(item_id)\n        model.status = status\n        if error:\n            model.last_error = error\n        model.completed_at = datetime.now()\n        model.save()\n    except FrontierItemModel.DoesNotExist:\n        pass\n</code></pre>"},{"location":"reference/ragcrawl/storage/dynamodb/models/","title":"Models","text":""},{"location":"reference/ragcrawl/storage/dynamodb/models/#ragcrawl.storage.dynamodb.models","title":"<code>models</code>","text":"<p>PynamoDB models for DynamoDB storage.</p>"},{"location":"reference/ragcrawl/storage/dynamodb/models/#ragcrawl.storage.dynamodb.models.CrawlRunModel","title":"<code>CrawlRunModel</code>","text":"<p>               Bases: <code>Model</code></p> <p>DynamoDB model for CrawlRun.</p>"},{"location":"reference/ragcrawl/storage/dynamodb/models/#ragcrawl.storage.dynamodb.models.FrontierItemModel","title":"<code>FrontierItemModel</code>","text":"<p>               Bases: <code>Model</code></p> <p>DynamoDB model for FrontierItem.</p>"},{"location":"reference/ragcrawl/storage/dynamodb/models/#ragcrawl.storage.dynamodb.models.FrontierRunIndex","title":"<code>FrontierRunIndex</code>","text":"<p>               Bases: <code>GlobalSecondaryIndex</code></p> <p>GSI for querying frontier by run.</p>"},{"location":"reference/ragcrawl/storage/dynamodb/models/#ragcrawl.storage.dynamodb.models.PageModel","title":"<code>PageModel</code>","text":"<p>               Bases: <code>Model</code></p> <p>DynamoDB model for Page.</p>"},{"location":"reference/ragcrawl/storage/dynamodb/models/#ragcrawl.storage.dynamodb.models.PageSiteIndex","title":"<code>PageSiteIndex</code>","text":"<p>               Bases: <code>GlobalSecondaryIndex</code></p> <p>GSI for querying pages by site.</p>"},{"location":"reference/ragcrawl/storage/dynamodb/models/#ragcrawl.storage.dynamodb.models.PageVersionModel","title":"<code>PageVersionModel</code>","text":"<p>               Bases: <code>Model</code></p> <p>DynamoDB model for PageVersion.</p>"},{"location":"reference/ragcrawl/storage/dynamodb/models/#ragcrawl.storage.dynamodb.models.RunSiteIndex","title":"<code>RunSiteIndex</code>","text":"<p>               Bases: <code>GlobalSecondaryIndex</code></p> <p>GSI for querying runs by site.</p>"},{"location":"reference/ragcrawl/storage/dynamodb/models/#ragcrawl.storage.dynamodb.models.SiteModel","title":"<code>SiteModel</code>","text":"<p>               Bases: <code>Model</code></p> <p>DynamoDB model for Site.</p>"},{"location":"reference/ragcrawl/storage/dynamodb/models/#ragcrawl.storage.dynamodb.models.SiteUrlIndex","title":"<code>SiteUrlIndex</code>","text":"<p>               Bases: <code>GlobalSecondaryIndex</code></p> <p>GSI for querying sites by URL.</p>"},{"location":"reference/ragcrawl/storage/dynamodb/models/#ragcrawl.storage.dynamodb.models.VersionPageIndex","title":"<code>VersionPageIndex</code>","text":"<p>               Bases: <code>GlobalSecondaryIndex</code></p> <p>GSI for querying versions by page.</p>"},{"location":"reference/ragcrawl/sync/","title":"Index","text":""},{"location":"reference/ragcrawl/sync/#ragcrawl.sync","title":"<code>sync</code>","text":"<p>Sync and change detection for ragcrawl.</p>"},{"location":"reference/ragcrawl/sync/#ragcrawl.sync.ChangeDetector","title":"<code>ChangeDetector(normalize=True, noise_patterns=None)</code>","text":"<p>Detects content changes using hash comparison.</p> <p>Supports noise reduction to minimize false positives from dynamic content like dates, timestamps, etc.</p> <p>Initialize change detector.</p> <p>Parameters:</p> Name Type Description Default <code>normalize</code> <code>bool</code> <p>Whether to normalize content before hashing.</p> <code>True</code> <code>noise_patterns</code> <code>list[str] | None</code> <p>Regex patterns for noise to strip before hashing.</p> <code>None</code> Source code in <code>src/ragcrawl/sync/change_detector.py</code> <pre><code>def __init__(\n    self,\n    normalize: bool = True,\n    noise_patterns: list[str] | None = None,\n) -&gt; None:\n    \"\"\"\n    Initialize change detector.\n\n    Args:\n        normalize: Whether to normalize content before hashing.\n        noise_patterns: Regex patterns for noise to strip before hashing.\n    \"\"\"\n    self.normalize = normalize\n    self.noise_patterns = self._compile_patterns(noise_patterns or [])\n</code></pre>"},{"location":"reference/ragcrawl/sync/#ragcrawl.sync.ChangeDetector.compute_hash","title":"<code>compute_hash(content)</code>","text":"<p>Compute content hash with optional normalization.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>Content to hash.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Content hash.</p> Source code in <code>src/ragcrawl/sync/change_detector.py</code> <pre><code>def compute_hash(self, content: str) -&gt; str:\n    \"\"\"\n    Compute content hash with optional normalization.\n\n    Args:\n        content: Content to hash.\n\n    Returns:\n        Content hash.\n    \"\"\"\n    processed = content\n\n    # Apply noise reduction\n    for pattern in self.noise_patterns:\n        processed = pattern.sub(\"\", processed)\n\n    # Compute hash\n    return compute_content_hash(processed, normalize=self.normalize)\n</code></pre>"},{"location":"reference/ragcrawl/sync/#ragcrawl.sync.ChangeDetector.get_diff_ratio","title":"<code>get_diff_ratio(old_content, new_content)</code>","text":"<p>Calculate similarity ratio between old and new content.</p> <p>Parameters:</p> Name Type Description Default <code>old_content</code> <code>str</code> <p>Previous content.</p> required <code>new_content</code> <code>str</code> <p>New content.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Ratio between 0 (completely different) and 1 (identical).</p> Source code in <code>src/ragcrawl/sync/change_detector.py</code> <pre><code>def get_diff_ratio(self, old_content: str, new_content: str) -&gt; float:\n    \"\"\"\n    Calculate similarity ratio between old and new content.\n\n    Args:\n        old_content: Previous content.\n        new_content: New content.\n\n    Returns:\n        Ratio between 0 (completely different) and 1 (identical).\n    \"\"\"\n    from difflib import SequenceMatcher\n\n    # Normalize both\n    if self.normalize:\n        old_content = re.sub(r\"\\s+\", \" \", old_content.strip())\n        new_content = re.sub(r\"\\s+\", \" \", new_content.strip())\n\n    return SequenceMatcher(None, old_content, new_content).ratio()\n</code></pre>"},{"location":"reference/ragcrawl/sync/#ragcrawl.sync.ChangeDetector.has_changed","title":"<code>has_changed(old_hash, new_hash)</code>","text":"<p>Check if content has changed.</p> <p>Parameters:</p> Name Type Description Default <code>old_hash</code> <code>str | None</code> <p>Previous content hash (None if new page).</p> required <code>new_hash</code> <code>str</code> <p>New content hash.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if content has changed.</p> Source code in <code>src/ragcrawl/sync/change_detector.py</code> <pre><code>def has_changed(self, old_hash: str | None, new_hash: str) -&gt; bool:\n    \"\"\"\n    Check if content has changed.\n\n    Args:\n        old_hash: Previous content hash (None if new page).\n        new_hash: New content hash.\n\n    Returns:\n        True if content has changed.\n    \"\"\"\n    if old_hash is None:\n        return True\n\n    return old_hash != new_hash\n</code></pre>"},{"location":"reference/ragcrawl/sync/#ragcrawl.sync.ChangeDetector.is_significant_change","title":"<code>is_significant_change(old_content, new_content, threshold=0.1)</code>","text":"<p>Check if change is significant (not just noise).</p> <p>Parameters:</p> Name Type Description Default <code>old_content</code> <code>str</code> <p>Previous content.</p> required <code>new_content</code> <code>str</code> <p>New content.</p> required <code>threshold</code> <code>float</code> <p>Minimum change ratio to be significant.</p> <code>0.1</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if change is significant.</p> Source code in <code>src/ragcrawl/sync/change_detector.py</code> <pre><code>def is_significant_change(\n    self,\n    old_content: str,\n    new_content: str,\n    threshold: float = 0.1,\n) -&gt; bool:\n    \"\"\"\n    Check if change is significant (not just noise).\n\n    Args:\n        old_content: Previous content.\n        new_content: New content.\n        threshold: Minimum change ratio to be significant.\n\n    Returns:\n        True if change is significant.\n    \"\"\"\n    ratio = self.get_diff_ratio(old_content, new_content)\n    change_ratio = 1 - ratio\n    return change_ratio &gt;= threshold\n</code></pre>"},{"location":"reference/ragcrawl/sync/#ragcrawl.sync.SitemapEntry","title":"<code>SitemapEntry(loc, lastmod=None, changefreq=None, priority=None)</code>  <code>dataclass</code>","text":"<p>Entry from a sitemap.</p>"},{"location":"reference/ragcrawl/sync/#ragcrawl.sync.SitemapParser","title":"<code>SitemapParser(user_agent='ragcrawl/0.1', timeout=30)</code>","text":"<p>Parses XML sitemaps for URL discovery and change prioritization.</p> <p>Supports: - Standard sitemap.xml - Sitemap index files - lastmod for change detection</p> <p>Initialize sitemap parser.</p> <p>Parameters:</p> Name Type Description Default <code>user_agent</code> <code>str</code> <p>User agent for fetching sitemaps.</p> <code>'ragcrawl/0.1'</code> <code>timeout</code> <code>int</code> <p>Request timeout in seconds.</p> <code>30</code> Source code in <code>src/ragcrawl/sync/sitemap_parser.py</code> <pre><code>def __init__(\n    self,\n    user_agent: str = \"ragcrawl/0.1\",\n    timeout: int = 30,\n) -&gt; None:\n    \"\"\"\n    Initialize sitemap parser.\n\n    Args:\n        user_agent: User agent for fetching sitemaps.\n        timeout: Request timeout in seconds.\n    \"\"\"\n    self.user_agent = user_agent\n    self.timeout = timeout\n</code></pre>"},{"location":"reference/ragcrawl/sync/#ragcrawl.sync.SitemapParser.discover_sitemaps","title":"<code>discover_sitemaps(base_url)</code>  <code>async</code>","text":"<p>Discover sitemap URLs for a site.</p> <p>Checks common locations: - /sitemap.xml - /sitemap_index.xml - robots.txt Sitemap directives</p> <p>Parameters:</p> Name Type Description Default <code>base_url</code> <code>str</code> <p>Base URL of the site.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List of discovered sitemap URLs.</p> Source code in <code>src/ragcrawl/sync/sitemap_parser.py</code> <pre><code>async def discover_sitemaps(self, base_url: str) -&gt; list[str]:\n    \"\"\"\n    Discover sitemap URLs for a site.\n\n    Checks common locations:\n    - /sitemap.xml\n    - /sitemap_index.xml\n    - robots.txt Sitemap directives\n\n    Args:\n        base_url: Base URL of the site.\n\n    Returns:\n        List of discovered sitemap URLs.\n    \"\"\"\n    sitemaps = []\n\n    # Try common locations\n    common_paths = [\n        \"/sitemap.xml\",\n        \"/sitemap_index.xml\",\n        \"/sitemap-index.xml\",\n        \"/sitemaps.xml\",\n    ]\n\n    for path in common_paths:\n        sitemap_url = urljoin(base_url, path)\n        if await self._exists(sitemap_url):\n            sitemaps.append(sitemap_url)\n            break  # Usually only one is needed\n\n    # Check robots.txt\n    robots_url = urljoin(base_url, \"/robots.txt\")\n    robots_sitemaps = await self._parse_robots_sitemaps(robots_url)\n    sitemaps.extend(s for s in robots_sitemaps if s not in sitemaps)\n\n    return sitemaps\n</code></pre>"},{"location":"reference/ragcrawl/sync/#ragcrawl.sync.SitemapParser.parse","title":"<code>parse(sitemap_url)</code>  <code>async</code>","text":"<p>Parse a sitemap URL.</p> <p>Handles both regular sitemaps and sitemap indexes.</p> <p>Parameters:</p> Name Type Description Default <code>sitemap_url</code> <code>str</code> <p>URL of the sitemap.</p> required <p>Returns:</p> Type Description <code>list[SitemapEntry]</code> <p>List of sitemap entries.</p> Source code in <code>src/ragcrawl/sync/sitemap_parser.py</code> <pre><code>async def parse(self, sitemap_url: str) -&gt; list[SitemapEntry]:\n    \"\"\"\n    Parse a sitemap URL.\n\n    Handles both regular sitemaps and sitemap indexes.\n\n    Args:\n        sitemap_url: URL of the sitemap.\n\n    Returns:\n        List of sitemap entries.\n    \"\"\"\n    try:\n        content = await self._fetch(sitemap_url)\n        if not content:\n            return []\n\n        # Check if it's a sitemap index\n        if \"&lt;sitemapindex\" in content:\n            return await self._parse_sitemap_index(content, sitemap_url)\n        else:\n            return self._parse_sitemap(content)\n\n    except Exception as e:\n        logger.error(\"Failed to parse sitemap\", url=sitemap_url, error=str(e))\n        return []\n</code></pre>"},{"location":"reference/ragcrawl/sync/change_detector/","title":"Change detector","text":""},{"location":"reference/ragcrawl/sync/change_detector/#ragcrawl.sync.change_detector","title":"<code>change_detector</code>","text":"<p>Content change detection for incremental sync.</p>"},{"location":"reference/ragcrawl/sync/change_detector/#ragcrawl.sync.change_detector.ChangeDetector","title":"<code>ChangeDetector(normalize=True, noise_patterns=None)</code>","text":"<p>Detects content changes using hash comparison.</p> <p>Supports noise reduction to minimize false positives from dynamic content like dates, timestamps, etc.</p> <p>Initialize change detector.</p> <p>Parameters:</p> Name Type Description Default <code>normalize</code> <code>bool</code> <p>Whether to normalize content before hashing.</p> <code>True</code> <code>noise_patterns</code> <code>list[str] | None</code> <p>Regex patterns for noise to strip before hashing.</p> <code>None</code> Source code in <code>src/ragcrawl/sync/change_detector.py</code> <pre><code>def __init__(\n    self,\n    normalize: bool = True,\n    noise_patterns: list[str] | None = None,\n) -&gt; None:\n    \"\"\"\n    Initialize change detector.\n\n    Args:\n        normalize: Whether to normalize content before hashing.\n        noise_patterns: Regex patterns for noise to strip before hashing.\n    \"\"\"\n    self.normalize = normalize\n    self.noise_patterns = self._compile_patterns(noise_patterns or [])\n</code></pre>"},{"location":"reference/ragcrawl/sync/change_detector/#ragcrawl.sync.change_detector.ChangeDetector.compute_hash","title":"<code>compute_hash(content)</code>","text":"<p>Compute content hash with optional normalization.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>Content to hash.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Content hash.</p> Source code in <code>src/ragcrawl/sync/change_detector.py</code> <pre><code>def compute_hash(self, content: str) -&gt; str:\n    \"\"\"\n    Compute content hash with optional normalization.\n\n    Args:\n        content: Content to hash.\n\n    Returns:\n        Content hash.\n    \"\"\"\n    processed = content\n\n    # Apply noise reduction\n    for pattern in self.noise_patterns:\n        processed = pattern.sub(\"\", processed)\n\n    # Compute hash\n    return compute_content_hash(processed, normalize=self.normalize)\n</code></pre>"},{"location":"reference/ragcrawl/sync/change_detector/#ragcrawl.sync.change_detector.ChangeDetector.get_diff_ratio","title":"<code>get_diff_ratio(old_content, new_content)</code>","text":"<p>Calculate similarity ratio between old and new content.</p> <p>Parameters:</p> Name Type Description Default <code>old_content</code> <code>str</code> <p>Previous content.</p> required <code>new_content</code> <code>str</code> <p>New content.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Ratio between 0 (completely different) and 1 (identical).</p> Source code in <code>src/ragcrawl/sync/change_detector.py</code> <pre><code>def get_diff_ratio(self, old_content: str, new_content: str) -&gt; float:\n    \"\"\"\n    Calculate similarity ratio between old and new content.\n\n    Args:\n        old_content: Previous content.\n        new_content: New content.\n\n    Returns:\n        Ratio between 0 (completely different) and 1 (identical).\n    \"\"\"\n    from difflib import SequenceMatcher\n\n    # Normalize both\n    if self.normalize:\n        old_content = re.sub(r\"\\s+\", \" \", old_content.strip())\n        new_content = re.sub(r\"\\s+\", \" \", new_content.strip())\n\n    return SequenceMatcher(None, old_content, new_content).ratio()\n</code></pre>"},{"location":"reference/ragcrawl/sync/change_detector/#ragcrawl.sync.change_detector.ChangeDetector.has_changed","title":"<code>has_changed(old_hash, new_hash)</code>","text":"<p>Check if content has changed.</p> <p>Parameters:</p> Name Type Description Default <code>old_hash</code> <code>str | None</code> <p>Previous content hash (None if new page).</p> required <code>new_hash</code> <code>str</code> <p>New content hash.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if content has changed.</p> Source code in <code>src/ragcrawl/sync/change_detector.py</code> <pre><code>def has_changed(self, old_hash: str | None, new_hash: str) -&gt; bool:\n    \"\"\"\n    Check if content has changed.\n\n    Args:\n        old_hash: Previous content hash (None if new page).\n        new_hash: New content hash.\n\n    Returns:\n        True if content has changed.\n    \"\"\"\n    if old_hash is None:\n        return True\n\n    return old_hash != new_hash\n</code></pre>"},{"location":"reference/ragcrawl/sync/change_detector/#ragcrawl.sync.change_detector.ChangeDetector.is_significant_change","title":"<code>is_significant_change(old_content, new_content, threshold=0.1)</code>","text":"<p>Check if change is significant (not just noise).</p> <p>Parameters:</p> Name Type Description Default <code>old_content</code> <code>str</code> <p>Previous content.</p> required <code>new_content</code> <code>str</code> <p>New content.</p> required <code>threshold</code> <code>float</code> <p>Minimum change ratio to be significant.</p> <code>0.1</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if change is significant.</p> Source code in <code>src/ragcrawl/sync/change_detector.py</code> <pre><code>def is_significant_change(\n    self,\n    old_content: str,\n    new_content: str,\n    threshold: float = 0.1,\n) -&gt; bool:\n    \"\"\"\n    Check if change is significant (not just noise).\n\n    Args:\n        old_content: Previous content.\n        new_content: New content.\n        threshold: Minimum change ratio to be significant.\n\n    Returns:\n        True if change is significant.\n    \"\"\"\n    ratio = self.get_diff_ratio(old_content, new_content)\n    change_ratio = 1 - ratio\n    return change_ratio &gt;= threshold\n</code></pre>"},{"location":"reference/ragcrawl/sync/change_detector/#ragcrawl.sync.change_detector.ChangeDetectorProtocol","title":"<code>ChangeDetectorProtocol</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for change detectors.</p>"},{"location":"reference/ragcrawl/sync/change_detector/#ragcrawl.sync.change_detector.ChangeDetectorProtocol.compute_hash","title":"<code>compute_hash(content)</code>","text":"<p>Compute content hash.</p> Source code in <code>src/ragcrawl/sync/change_detector.py</code> <pre><code>def compute_hash(self, content: str) -&gt; str:\n    \"\"\"Compute content hash.\"\"\"\n    ...\n</code></pre>"},{"location":"reference/ragcrawl/sync/change_detector/#ragcrawl.sync.change_detector.ChangeDetectorProtocol.has_changed","title":"<code>has_changed(old_hash, new_hash)</code>","text":"<p>Check if content has changed.</p> Source code in <code>src/ragcrawl/sync/change_detector.py</code> <pre><code>def has_changed(self, old_hash: str | None, new_hash: str) -&gt; bool:\n    \"\"\"Check if content has changed.\"\"\"\n    ...\n</code></pre>"},{"location":"reference/ragcrawl/sync/change_detector/#ragcrawl.sync.change_detector.ContentNormalizer","title":"<code>ContentNormalizer(strip_whitespace=True, lowercase=False, strip_patterns=None)</code>","text":"<p>Normalizes content for consistent change detection.</p> <p>Initialize normalizer.</p> <p>Parameters:</p> Name Type Description Default <code>strip_whitespace</code> <code>bool</code> <p>Normalize whitespace.</p> <code>True</code> <code>lowercase</code> <code>bool</code> <p>Convert to lowercase.</p> <code>False</code> <code>strip_patterns</code> <code>list[str] | None</code> <p>Patterns to strip.</p> <code>None</code> Source code in <code>src/ragcrawl/sync/change_detector.py</code> <pre><code>def __init__(\n    self,\n    strip_whitespace: bool = True,\n    lowercase: bool = False,\n    strip_patterns: list[str] | None = None,\n) -&gt; None:\n    \"\"\"\n    Initialize normalizer.\n\n    Args:\n        strip_whitespace: Normalize whitespace.\n        lowercase: Convert to lowercase.\n        strip_patterns: Patterns to strip.\n    \"\"\"\n    self.strip_whitespace = strip_whitespace\n    self.lowercase = lowercase\n    self.strip_patterns = self._compile_patterns(strip_patterns or [])\n</code></pre>"},{"location":"reference/ragcrawl/sync/change_detector/#ragcrawl.sync.change_detector.ContentNormalizer.normalize","title":"<code>normalize(content)</code>","text":"<p>Normalize content.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>Content to normalize.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Normalized content.</p> Source code in <code>src/ragcrawl/sync/change_detector.py</code> <pre><code>def normalize(self, content: str) -&gt; str:\n    \"\"\"\n    Normalize content.\n\n    Args:\n        content: Content to normalize.\n\n    Returns:\n        Normalized content.\n    \"\"\"\n    result = content\n\n    # Strip patterns\n    for pattern in self.strip_patterns:\n        result = pattern.sub(\"\", result)\n\n    # Whitespace normalization\n    if self.strip_whitespace:\n        result = re.sub(r\"\\s+\", \" \", result)\n        result = result.strip()\n\n    # Lowercase\n    if self.lowercase:\n        result = result.lower()\n\n    return result\n</code></pre>"},{"location":"reference/ragcrawl/sync/sitemap_parser/","title":"Sitemap parser","text":""},{"location":"reference/ragcrawl/sync/sitemap_parser/#ragcrawl.sync.sitemap_parser","title":"<code>sitemap_parser</code>","text":"<p>Sitemap parsing for change prioritization.</p>"},{"location":"reference/ragcrawl/sync/sitemap_parser/#ragcrawl.sync.sitemap_parser.SitemapEntry","title":"<code>SitemapEntry(loc, lastmod=None, changefreq=None, priority=None)</code>  <code>dataclass</code>","text":"<p>Entry from a sitemap.</p>"},{"location":"reference/ragcrawl/sync/sitemap_parser/#ragcrawl.sync.sitemap_parser.SitemapParser","title":"<code>SitemapParser(user_agent='ragcrawl/0.1', timeout=30)</code>","text":"<p>Parses XML sitemaps for URL discovery and change prioritization.</p> <p>Supports: - Standard sitemap.xml - Sitemap index files - lastmod for change detection</p> <p>Initialize sitemap parser.</p> <p>Parameters:</p> Name Type Description Default <code>user_agent</code> <code>str</code> <p>User agent for fetching sitemaps.</p> <code>'ragcrawl/0.1'</code> <code>timeout</code> <code>int</code> <p>Request timeout in seconds.</p> <code>30</code> Source code in <code>src/ragcrawl/sync/sitemap_parser.py</code> <pre><code>def __init__(\n    self,\n    user_agent: str = \"ragcrawl/0.1\",\n    timeout: int = 30,\n) -&gt; None:\n    \"\"\"\n    Initialize sitemap parser.\n\n    Args:\n        user_agent: User agent for fetching sitemaps.\n        timeout: Request timeout in seconds.\n    \"\"\"\n    self.user_agent = user_agent\n    self.timeout = timeout\n</code></pre>"},{"location":"reference/ragcrawl/sync/sitemap_parser/#ragcrawl.sync.sitemap_parser.SitemapParser.discover_sitemaps","title":"<code>discover_sitemaps(base_url)</code>  <code>async</code>","text":"<p>Discover sitemap URLs for a site.</p> <p>Checks common locations: - /sitemap.xml - /sitemap_index.xml - robots.txt Sitemap directives</p> <p>Parameters:</p> Name Type Description Default <code>base_url</code> <code>str</code> <p>Base URL of the site.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List of discovered sitemap URLs.</p> Source code in <code>src/ragcrawl/sync/sitemap_parser.py</code> <pre><code>async def discover_sitemaps(self, base_url: str) -&gt; list[str]:\n    \"\"\"\n    Discover sitemap URLs for a site.\n\n    Checks common locations:\n    - /sitemap.xml\n    - /sitemap_index.xml\n    - robots.txt Sitemap directives\n\n    Args:\n        base_url: Base URL of the site.\n\n    Returns:\n        List of discovered sitemap URLs.\n    \"\"\"\n    sitemaps = []\n\n    # Try common locations\n    common_paths = [\n        \"/sitemap.xml\",\n        \"/sitemap_index.xml\",\n        \"/sitemap-index.xml\",\n        \"/sitemaps.xml\",\n    ]\n\n    for path in common_paths:\n        sitemap_url = urljoin(base_url, path)\n        if await self._exists(sitemap_url):\n            sitemaps.append(sitemap_url)\n            break  # Usually only one is needed\n\n    # Check robots.txt\n    robots_url = urljoin(base_url, \"/robots.txt\")\n    robots_sitemaps = await self._parse_robots_sitemaps(robots_url)\n    sitemaps.extend(s for s in robots_sitemaps if s not in sitemaps)\n\n    return sitemaps\n</code></pre>"},{"location":"reference/ragcrawl/sync/sitemap_parser/#ragcrawl.sync.sitemap_parser.SitemapParser.parse","title":"<code>parse(sitemap_url)</code>  <code>async</code>","text":"<p>Parse a sitemap URL.</p> <p>Handles both regular sitemaps and sitemap indexes.</p> <p>Parameters:</p> Name Type Description Default <code>sitemap_url</code> <code>str</code> <p>URL of the sitemap.</p> required <p>Returns:</p> Type Description <code>list[SitemapEntry]</code> <p>List of sitemap entries.</p> Source code in <code>src/ragcrawl/sync/sitemap_parser.py</code> <pre><code>async def parse(self, sitemap_url: str) -&gt; list[SitemapEntry]:\n    \"\"\"\n    Parse a sitemap URL.\n\n    Handles both regular sitemaps and sitemap indexes.\n\n    Args:\n        sitemap_url: URL of the sitemap.\n\n    Returns:\n        List of sitemap entries.\n    \"\"\"\n    try:\n        content = await self._fetch(sitemap_url)\n        if not content:\n            return []\n\n        # Check if it's a sitemap index\n        if \"&lt;sitemapindex\" in content:\n            return await self._parse_sitemap_index(content, sitemap_url)\n        else:\n            return self._parse_sitemap(content)\n\n    except Exception as e:\n        logger.error(\"Failed to parse sitemap\", url=sitemap_url, error=str(e))\n        return []\n</code></pre>"},{"location":"reference/ragcrawl/sync/strategies/","title":"Strategies","text":""},{"location":"reference/ragcrawl/sync/strategies/#ragcrawl.sync.strategies","title":"<code>strategies</code>","text":"<p>Sync strategy orchestration.</p>"},{"location":"reference/ragcrawl/sync/strategies/#ragcrawl.sync.strategies.StrategyResult","title":"<code>StrategyResult(strategy, should_fetch, reason, metadata=None)</code>  <code>dataclass</code>","text":"<p>Result from a sync strategy.</p>"},{"location":"reference/ragcrawl/sync/strategies/#ragcrawl.sync.strategies.SyncStrategyOrchestrator","title":"<code>SyncStrategyOrchestrator(strategies)</code>","text":"<p>Orchestrates multiple sync strategies.</p> <p>Tries strategies in order until one provides a definitive answer.</p> <p>Initialize orchestrator.</p> <p>Parameters:</p> Name Type Description Default <code>strategies</code> <code>list[SyncStrategy]</code> <p>Strategies to use, in order of preference.</p> required Source code in <code>src/ragcrawl/sync/strategies.py</code> <pre><code>def __init__(\n    self,\n    strategies: list[SyncStrategy],\n) -&gt; None:\n    \"\"\"\n    Initialize orchestrator.\n\n    Args:\n        strategies: Strategies to use, in order of preference.\n    \"\"\"\n    self.strategies = strategies\n</code></pre>"},{"location":"reference/ragcrawl/sync/strategies/#ragcrawl.sync.strategies.SyncStrategyOrchestrator.get_next_strategy","title":"<code>get_next_strategy(current)</code>","text":"<p>Get the next strategy to try after current.</p> Source code in <code>src/ragcrawl/sync/strategies.py</code> <pre><code>def get_next_strategy(\n    self,\n    current: SyncStrategy | None,\n) -&gt; SyncStrategy | None:\n    \"\"\"Get the next strategy to try after current.\"\"\"\n    if current is None:\n        return self.strategies[0] if self.strategies else None\n\n    try:\n        idx = self.strategies.index(current)\n        if idx + 1 &lt; len(self.strategies):\n            return self.strategies[idx + 1]\n    except ValueError:\n        pass\n\n    return None\n</code></pre>"},{"location":"reference/ragcrawl/sync/strategies/#ragcrawl.sync.strategies.SyncStrategyOrchestrator.get_strategy_order","title":"<code>get_strategy_order()</code>","text":"<p>Get the order of strategies to try.</p> Source code in <code>src/ragcrawl/sync/strategies.py</code> <pre><code>def get_strategy_order(self) -&gt; list[SyncStrategy]:\n    \"\"\"Get the order of strategies to try.\"\"\"\n    return self.strategies\n</code></pre>"},{"location":"reference/ragcrawl/sync/strategies/#ragcrawl.sync.strategies.SyncStrategyOrchestrator.should_try_strategy","title":"<code>should_try_strategy(strategy)</code>","text":"<p>Check if a strategy should be tried.</p> Source code in <code>src/ragcrawl/sync/strategies.py</code> <pre><code>def should_try_strategy(self, strategy: SyncStrategy) -&gt; bool:\n    \"\"\"Check if a strategy should be tried.\"\"\"\n    return strategy in self.strategies\n</code></pre>"},{"location":"reference/ragcrawl/utils/","title":"Index","text":""},{"location":"reference/ragcrawl/utils/#ragcrawl.utils","title":"<code>utils</code>","text":"<p>Utility functions for ragcrawl.</p>"},{"location":"reference/ragcrawl/utils/#ragcrawl.utils.CrawlMetrics","title":"<code>CrawlMetrics(pages_discovered=0, pages_crawled=0, pages_succeeded=0, pages_failed=0, pages_skipped=0, pages_changed=0, pages_unchanged=0, pages_new=0, pages_deleted=0, total_bytes=0, total_fetch_time_ms=0.0, total_extraction_time_ms=0.0, start_time=time.time(), end_time=None, domains=(lambda: defaultdict(DomainMetrics))(), errors_by_type=(lambda: defaultdict(int))(), status_codes=(lambda: defaultdict(int))())</code>  <code>dataclass</code>","text":"<p>Aggregated metrics for a crawl run.</p>"},{"location":"reference/ragcrawl/utils/#ragcrawl.utils.CrawlMetrics.avg_fetch_latency_ms","title":"<code>avg_fetch_latency_ms</code>  <code>property</code>","text":"<p>Average fetch latency.</p>"},{"location":"reference/ragcrawl/utils/#ragcrawl.utils.CrawlMetrics.duration_seconds","title":"<code>duration_seconds</code>  <code>property</code>","text":"<p>Total duration in seconds.</p>"},{"location":"reference/ragcrawl/utils/#ragcrawl.utils.CrawlMetrics.pages_per_second","title":"<code>pages_per_second</code>  <code>property</code>","text":"<p>Crawl throughput in pages per second.</p>"},{"location":"reference/ragcrawl/utils/#ragcrawl.utils.CrawlMetrics.to_dict","title":"<code>to_dict()</code>","text":"<p>Convert metrics to dictionary.</p> Source code in <code>src/ragcrawl/utils/metrics.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Convert metrics to dictionary.\"\"\"\n    return {\n        \"pages_discovered\": self.pages_discovered,\n        \"pages_crawled\": self.pages_crawled,\n        \"pages_succeeded\": self.pages_succeeded,\n        \"pages_failed\": self.pages_failed,\n        \"pages_skipped\": self.pages_skipped,\n        \"pages_changed\": self.pages_changed,\n        \"pages_unchanged\": self.pages_unchanged,\n        \"pages_new\": self.pages_new,\n        \"pages_deleted\": self.pages_deleted,\n        \"total_bytes\": self.total_bytes,\n        \"total_fetch_time_ms\": self.total_fetch_time_ms,\n        \"total_extraction_time_ms\": self.total_extraction_time_ms,\n        \"duration_seconds\": self.duration_seconds,\n        \"avg_fetch_latency_ms\": self.avg_fetch_latency_ms,\n        \"pages_per_second\": self.pages_per_second,\n        \"domains_crawled\": len(self.domains),\n        \"status_codes\": dict(self.status_codes),\n        \"errors_by_type\": dict(self.errors_by_type),\n    }\n</code></pre>"},{"location":"reference/ragcrawl/utils/#ragcrawl.utils.MetricsCollector","title":"<code>MetricsCollector()</code>","text":"<p>Collector for crawl metrics with thread-safe updates.</p> <p>Initialize the metrics collector.</p> Source code in <code>src/ragcrawl/utils/metrics.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize the metrics collector.\"\"\"\n    self.metrics = CrawlMetrics()\n    self._lock_placeholder = True  # Placeholder for thread safety if needed\n</code></pre>"},{"location":"reference/ragcrawl/utils/#ragcrawl.utils.MetricsCollector.finalize","title":"<code>finalize()</code>","text":"<p>Finalize metrics and return.</p> Source code in <code>src/ragcrawl/utils/metrics.py</code> <pre><code>def finalize(self) -&gt; CrawlMetrics:\n    \"\"\"Finalize metrics and return.\"\"\"\n    self.metrics.end_time = time.time()\n    return self.metrics\n</code></pre>"},{"location":"reference/ragcrawl/utils/#ragcrawl.utils.MetricsCollector.get_domain_stats","title":"<code>get_domain_stats()</code>","text":"<p>Get per-domain statistics.</p> Source code in <code>src/ragcrawl/utils/metrics.py</code> <pre><code>def get_domain_stats(self) -&gt; dict[str, dict[str, Any]]:\n    \"\"\"Get per-domain statistics.\"\"\"\n    return {\n        domain: {\n            \"requests\": dm.requests,\n            \"successes\": dm.successes,\n            \"failures\": dm.failures,\n            \"avg_latency_ms\": dm.avg_latency_ms,\n            \"total_bytes\": dm.total_bytes,\n            \"success_rate\": dm.success_rate,\n        }\n        for domain, dm in self.metrics.domains.items()\n    }\n</code></pre>"},{"location":"reference/ragcrawl/utils/#ragcrawl.utils.MetricsCollector.record_change","title":"<code>record_change(is_new=False)</code>","text":"<p>Record content change.</p> Source code in <code>src/ragcrawl/utils/metrics.py</code> <pre><code>def record_change(self, is_new: bool = False) -&gt; None:\n    \"\"\"Record content change.\"\"\"\n    self.metrics.pages_changed += 1\n    if is_new:\n        self.metrics.pages_new += 1\n</code></pre>"},{"location":"reference/ragcrawl/utils/#ragcrawl.utils.MetricsCollector.record_deletion","title":"<code>record_deletion()</code>","text":"<p>Record page deletion (tombstone).</p> Source code in <code>src/ragcrawl/utils/metrics.py</code> <pre><code>def record_deletion(self) -&gt; None:\n    \"\"\"Record page deletion (tombstone).\"\"\"\n    self.metrics.pages_deleted += 1\n</code></pre>"},{"location":"reference/ragcrawl/utils/#ragcrawl.utils.MetricsCollector.record_discovery","title":"<code>record_discovery(count=1)</code>","text":"<p>Record URL discovery.</p> Source code in <code>src/ragcrawl/utils/metrics.py</code> <pre><code>def record_discovery(self, count: int = 1) -&gt; None:\n    \"\"\"Record URL discovery.\"\"\"\n    self.metrics.pages_discovered += count\n</code></pre>"},{"location":"reference/ragcrawl/utils/#ragcrawl.utils.MetricsCollector.record_error","title":"<code>record_error(error_type, domain=None)</code>","text":"<p>Record an error.</p> Source code in <code>src/ragcrawl/utils/metrics.py</code> <pre><code>def record_error(self, error_type: str, domain: str | None = None) -&gt; None:\n    \"\"\"Record an error.\"\"\"\n    self.metrics.errors_by_type[error_type] += 1\n    if domain:\n        self.metrics.domains[domain].errors[error_type] += 1\n</code></pre>"},{"location":"reference/ragcrawl/utils/#ragcrawl.utils.MetricsCollector.record_extraction","title":"<code>record_extraction(latency_ms)</code>","text":"<p>Record extraction time.</p> Source code in <code>src/ragcrawl/utils/metrics.py</code> <pre><code>def record_extraction(self, latency_ms: float) -&gt; None:\n    \"\"\"Record extraction time.\"\"\"\n    self.metrics.total_extraction_time_ms += latency_ms\n</code></pre>"},{"location":"reference/ragcrawl/utils/#ragcrawl.utils.MetricsCollector.record_fetch","title":"<code>record_fetch(domain, status_code, latency_ms, bytes_downloaded, success=True)</code>","text":"<p>Record a fetch operation.</p> Source code in <code>src/ragcrawl/utils/metrics.py</code> <pre><code>def record_fetch(\n    self,\n    domain: str,\n    status_code: int,\n    latency_ms: float,\n    bytes_downloaded: int,\n    success: bool = True,\n) -&gt; None:\n    \"\"\"Record a fetch operation.\"\"\"\n    self.metrics.pages_crawled += 1\n    self.metrics.total_bytes += bytes_downloaded\n    self.metrics.total_fetch_time_ms += latency_ms\n    self.metrics.status_codes[status_code] += 1\n\n    if success:\n        self.metrics.pages_succeeded += 1\n    else:\n        self.metrics.pages_failed += 1\n\n    # Domain metrics\n    dm = self.metrics.domains[domain]\n    dm.requests += 1\n    dm.total_latency_ms += latency_ms\n    dm.total_bytes += bytes_downloaded\n    dm.status_codes[status_code] += 1\n    if success:\n        dm.successes += 1\n    else:\n        dm.failures += 1\n</code></pre>"},{"location":"reference/ragcrawl/utils/#ragcrawl.utils.MetricsCollector.record_skip","title":"<code>record_skip(reason='filtered')</code>","text":"<p>Record a skipped page.</p> Source code in <code>src/ragcrawl/utils/metrics.py</code> <pre><code>def record_skip(self, reason: str = \"filtered\") -&gt; None:\n    \"\"\"Record a skipped page.\"\"\"\n    self.metrics.pages_skipped += 1\n</code></pre>"},{"location":"reference/ragcrawl/utils/#ragcrawl.utils.MetricsCollector.record_unchanged","title":"<code>record_unchanged()</code>","text":"<p>Record unchanged page (304 or hash match).</p> Source code in <code>src/ragcrawl/utils/metrics.py</code> <pre><code>def record_unchanged(self) -&gt; None:\n    \"\"\"Record unchanged page (304 or hash match).\"\"\"\n    self.metrics.pages_unchanged += 1\n</code></pre>"},{"location":"reference/ragcrawl/utils/#ragcrawl.utils.compute_content_hash","title":"<code>compute_content_hash(content, normalize=True)</code>","text":"<p>Compute a hash of content for change detection.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str | bytes</code> <p>The content to hash (typically Markdown). Can be str or bytes.</p> required <code>normalize</code> <code>bool</code> <p>If True, normalize whitespace before hashing (only for str).</p> <code>True</code> <p>Returns:</p> Type Description <code>str</code> <p>A hex string hash of the content.</p> Source code in <code>src/ragcrawl/utils/hashing.py</code> <pre><code>def compute_content_hash(content: str | bytes, normalize: bool = True) -&gt; str:\n    \"\"\"\n    Compute a hash of content for change detection.\n\n    Args:\n        content: The content to hash (typically Markdown). Can be str or bytes.\n        normalize: If True, normalize whitespace before hashing (only for str).\n\n    Returns:\n        A hex string hash of the content.\n    \"\"\"\n    if isinstance(content, bytes):\n        return xxhash.xxh64(content).hexdigest()\n\n    if normalize:\n        # Normalize whitespace to reduce false positives\n        content = re.sub(r\"\\s+\", \" \", content.strip())\n\n    return xxhash.xxh64(content.encode(\"utf-8\")).hexdigest()\n</code></pre>"},{"location":"reference/ragcrawl/utils/#ragcrawl.utils.compute_doc_id","title":"<code>compute_doc_id(normalized_url)</code>","text":"<p>Compute a stable document ID from a normalized URL.</p> <p>Uses xxhash for fast, deterministic hashing.</p> <p>Parameters:</p> Name Type Description Default <code>normalized_url</code> <code>str</code> <p>The normalized URL string.</p> required <p>Returns:</p> Type Description <code>str</code> <p>A hex string ID that's stable across runs.</p> Source code in <code>src/ragcrawl/utils/hashing.py</code> <pre><code>def compute_doc_id(normalized_url: str) -&gt; str:\n    \"\"\"\n    Compute a stable document ID from a normalized URL.\n\n    Uses xxhash for fast, deterministic hashing.\n\n    Args:\n        normalized_url: The normalized URL string.\n\n    Returns:\n        A hex string ID that's stable across runs.\n    \"\"\"\n    return xxhash.xxh64(normalized_url.encode(\"utf-8\")).hexdigest()\n</code></pre>"},{"location":"reference/ragcrawl/utils/#ragcrawl.utils.generate_run_id","title":"<code>generate_run_id()</code>","text":"<p>Generate a unique run ID.</p> <p>Format: run_{timestamp}_{random}</p> <p>Returns:</p> Type Description <code>str</code> <p>A unique run identifier.</p> Source code in <code>src/ragcrawl/utils/hashing.py</code> <pre><code>def generate_run_id() -&gt; str:\n    \"\"\"\n    Generate a unique run ID.\n\n    Format: run_{timestamp}_{random}\n\n    Returns:\n        A unique run identifier.\n    \"\"\"\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    random_suffix = uuid.uuid4().hex[:8]\n    return f\"run_{timestamp}_{random_suffix}\"\n</code></pre>"},{"location":"reference/ragcrawl/utils/#ragcrawl.utils.generate_version_id","title":"<code>generate_version_id(content_hash, timestamp=None)</code>","text":"<p>Generate a version ID from content hash and timestamp.</p> <p>Parameters:</p> Name Type Description Default <code>content_hash</code> <code>str</code> <p>Hash of the content.</p> required <code>timestamp</code> <code>datetime | None</code> <p>Optional timestamp (defaults to now).</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>A version identifier.</p> Source code in <code>src/ragcrawl/utils/hashing.py</code> <pre><code>def generate_version_id(content_hash: str, timestamp: datetime | None = None) -&gt; str:\n    \"\"\"\n    Generate a version ID from content hash and timestamp.\n\n    Args:\n        content_hash: Hash of the content.\n        timestamp: Optional timestamp (defaults to now).\n\n    Returns:\n        A version identifier.\n    \"\"\"\n    if timestamp is None:\n        timestamp = datetime.now()\n\n    ts_str = timestamp.strftime(\"%Y%m%d%H%M%S\")\n    return f\"v_{content_hash[:12]}_{ts_str}\"\n</code></pre>"},{"location":"reference/ragcrawl/utils/#ragcrawl.utils.get_logger","title":"<code>get_logger(name, **initial_context)</code>","text":"<p>Get a structured logger instance.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Logger name (typically module name).</p> required <code>**initial_context</code> <code>Any</code> <p>Initial context to bind to the logger.</p> <code>{}</code> <p>Returns:</p> Type Description <code>BoundLogger</code> <p>A structlog BoundLogger instance.</p> Source code in <code>src/ragcrawl/utils/logging.py</code> <pre><code>def get_logger(name: str, **initial_context: Any) -&gt; structlog.stdlib.BoundLogger:\n    \"\"\"\n    Get a structured logger instance.\n\n    Args:\n        name: Logger name (typically module name).\n        **initial_context: Initial context to bind to the logger.\n\n    Returns:\n        A structlog BoundLogger instance.\n    \"\"\"\n    logger = structlog.get_logger(name)\n    if initial_context:\n        logger = logger.bind(**initial_context)\n    return logger\n</code></pre>"},{"location":"reference/ragcrawl/utils/#ragcrawl.utils.setup_logging","title":"<code>setup_logging(level=logging.INFO, json_format=False, log_file=None)</code>","text":"<p>Set up structured logging for the crawler.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>int</code> <p>Logging level (default: INFO).</p> <code>INFO</code> <code>json_format</code> <code>bool</code> <p>If True, output logs as JSON.</p> <code>False</code> <code>log_file</code> <code>str | None</code> <p>Optional file path to write logs to.</p> <code>None</code> Source code in <code>src/ragcrawl/utils/logging.py</code> <pre><code>def setup_logging(\n    level: int = logging.INFO,\n    json_format: bool = False,\n    log_file: str | None = None,\n) -&gt; None:\n    \"\"\"\n    Set up structured logging for the crawler.\n\n    Args:\n        level: Logging level (default: INFO).\n        json_format: If True, output logs as JSON.\n        log_file: Optional file path to write logs to.\n    \"\"\"\n    # Configure standard logging\n    logging.basicConfig(\n        format=\"%(message)s\",\n        stream=sys.stdout,\n        level=level,\n    )\n\n    # Configure structlog processors\n    shared_processors: list[structlog.typing.Processor] = [\n        structlog.stdlib.add_log_level,\n        structlog.stdlib.add_logger_name,\n        structlog.processors.TimeStamper(fmt=\"iso\"),\n        structlog.processors.StackInfoRenderer(),\n        structlog.processors.UnicodeDecoder(),\n    ]\n\n    if json_format:\n        # JSON output for production\n        processors = shared_processors + [\n            structlog.processors.format_exc_info,\n            structlog.processors.JSONRenderer(),\n        ]\n    else:\n        # Console output for development\n        processors = shared_processors + [\n            structlog.dev.ConsoleRenderer(colors=True),\n        ]\n\n    structlog.configure(\n        processors=processors,\n        wrapper_class=structlog.stdlib.BoundLogger,\n        context_class=dict,\n        logger_factory=structlog.stdlib.LoggerFactory(),\n        cache_logger_on_first_use=True,\n    )\n\n    # Add file handler if specified\n    if log_file:\n        file_handler = logging.FileHandler(log_file)\n        file_handler.setLevel(level)\n        logging.getLogger().addHandler(file_handler)\n</code></pre>"},{"location":"reference/ragcrawl/utils/hashing/","title":"Hashing","text":""},{"location":"reference/ragcrawl/utils/hashing/#ragcrawl.utils.hashing","title":"<code>hashing</code>","text":"<p>Hashing utilities for stable ID generation.</p>"},{"location":"reference/ragcrawl/utils/hashing/#ragcrawl.utils.hashing.compute_content_hash","title":"<code>compute_content_hash(content, normalize=True)</code>","text":"<p>Compute a hash of content for change detection.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str | bytes</code> <p>The content to hash (typically Markdown). Can be str or bytes.</p> required <code>normalize</code> <code>bool</code> <p>If True, normalize whitespace before hashing (only for str).</p> <code>True</code> <p>Returns:</p> Type Description <code>str</code> <p>A hex string hash of the content.</p> Source code in <code>src/ragcrawl/utils/hashing.py</code> <pre><code>def compute_content_hash(content: str | bytes, normalize: bool = True) -&gt; str:\n    \"\"\"\n    Compute a hash of content for change detection.\n\n    Args:\n        content: The content to hash (typically Markdown). Can be str or bytes.\n        normalize: If True, normalize whitespace before hashing (only for str).\n\n    Returns:\n        A hex string hash of the content.\n    \"\"\"\n    if isinstance(content, bytes):\n        return xxhash.xxh64(content).hexdigest()\n\n    if normalize:\n        # Normalize whitespace to reduce false positives\n        content = re.sub(r\"\\s+\", \" \", content.strip())\n\n    return xxhash.xxh64(content.encode(\"utf-8\")).hexdigest()\n</code></pre>"},{"location":"reference/ragcrawl/utils/hashing/#ragcrawl.utils.hashing.compute_doc_id","title":"<code>compute_doc_id(normalized_url)</code>","text":"<p>Compute a stable document ID from a normalized URL.</p> <p>Uses xxhash for fast, deterministic hashing.</p> <p>Parameters:</p> Name Type Description Default <code>normalized_url</code> <code>str</code> <p>The normalized URL string.</p> required <p>Returns:</p> Type Description <code>str</code> <p>A hex string ID that's stable across runs.</p> Source code in <code>src/ragcrawl/utils/hashing.py</code> <pre><code>def compute_doc_id(normalized_url: str) -&gt; str:\n    \"\"\"\n    Compute a stable document ID from a normalized URL.\n\n    Uses xxhash for fast, deterministic hashing.\n\n    Args:\n        normalized_url: The normalized URL string.\n\n    Returns:\n        A hex string ID that's stable across runs.\n    \"\"\"\n    return xxhash.xxh64(normalized_url.encode(\"utf-8\")).hexdigest()\n</code></pre>"},{"location":"reference/ragcrawl/utils/hashing/#ragcrawl.utils.hashing.compute_url_hash","title":"<code>compute_url_hash(url)</code>","text":"<p>Compute a hash of a URL.</p> <p>This is an alias for compute_doc_id for semantic clarity.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL to hash.</p> required <p>Returns:</p> Type Description <code>str</code> <p>A hex string hash of the URL.</p> Source code in <code>src/ragcrawl/utils/hashing.py</code> <pre><code>def compute_url_hash(url: str) -&gt; str:\n    \"\"\"\n    Compute a hash of a URL.\n\n    This is an alias for compute_doc_id for semantic clarity.\n\n    Args:\n        url: The URL to hash.\n\n    Returns:\n        A hex string hash of the URL.\n    \"\"\"\n    return compute_doc_id(url)\n</code></pre>"},{"location":"reference/ragcrawl/utils/hashing/#ragcrawl.utils.hashing.generate_chunk_id","title":"<code>generate_chunk_id(doc_id, chunk_index)</code>","text":"<p>Generate a chunk ID from document ID and index.</p> <p>Parameters:</p> Name Type Description Default <code>doc_id</code> <code>str</code> <p>Parent document ID.</p> required <code>chunk_index</code> <code>int</code> <p>Index of the chunk (0-based).</p> required <p>Returns:</p> Type Description <code>str</code> <p>A unique chunk identifier.</p> Source code in <code>src/ragcrawl/utils/hashing.py</code> <pre><code>def generate_chunk_id(doc_id: str, chunk_index: int) -&gt; str:\n    \"\"\"\n    Generate a chunk ID from document ID and index.\n\n    Args:\n        doc_id: Parent document ID.\n        chunk_index: Index of the chunk (0-based).\n\n    Returns:\n        A unique chunk identifier.\n    \"\"\"\n    return f\"{doc_id}_chunk_{chunk_index:04d}\"\n</code></pre>"},{"location":"reference/ragcrawl/utils/hashing/#ragcrawl.utils.hashing.generate_run_id","title":"<code>generate_run_id()</code>","text":"<p>Generate a unique run ID.</p> <p>Format: run_{timestamp}_{random}</p> <p>Returns:</p> Type Description <code>str</code> <p>A unique run identifier.</p> Source code in <code>src/ragcrawl/utils/hashing.py</code> <pre><code>def generate_run_id() -&gt; str:\n    \"\"\"\n    Generate a unique run ID.\n\n    Format: run_{timestamp}_{random}\n\n    Returns:\n        A unique run identifier.\n    \"\"\"\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    random_suffix = uuid.uuid4().hex[:8]\n    return f\"run_{timestamp}_{random_suffix}\"\n</code></pre>"},{"location":"reference/ragcrawl/utils/hashing/#ragcrawl.utils.hashing.generate_site_id","title":"<code>generate_site_id(seed_urls)</code>","text":"<p>Generate a site ID from seed URLs.</p> <p>Parameters:</p> Name Type Description Default <code>seed_urls</code> <code>list[str]</code> <p>List of seed URLs.</p> required <p>Returns:</p> Type Description <code>str</code> <p>A stable site identifier based on seeds.</p> Source code in <code>src/ragcrawl/utils/hashing.py</code> <pre><code>def generate_site_id(seed_urls: list[str]) -&gt; str:\n    \"\"\"\n    Generate a site ID from seed URLs.\n\n    Args:\n        seed_urls: List of seed URLs.\n\n    Returns:\n        A stable site identifier based on seeds.\n    \"\"\"\n    # Sort and join for determinism\n    seeds_str = \"|\".join(sorted(seed_urls))\n    hash_val = xxhash.xxh64(seeds_str.encode(\"utf-8\")).hexdigest()[:12]\n    return f\"site_{hash_val}\"\n</code></pre>"},{"location":"reference/ragcrawl/utils/hashing/#ragcrawl.utils.hashing.generate_version_id","title":"<code>generate_version_id(content_hash, timestamp=None)</code>","text":"<p>Generate a version ID from content hash and timestamp.</p> <p>Parameters:</p> Name Type Description Default <code>content_hash</code> <code>str</code> <p>Hash of the content.</p> required <code>timestamp</code> <code>datetime | None</code> <p>Optional timestamp (defaults to now).</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>A version identifier.</p> Source code in <code>src/ragcrawl/utils/hashing.py</code> <pre><code>def generate_version_id(content_hash: str, timestamp: datetime | None = None) -&gt; str:\n    \"\"\"\n    Generate a version ID from content hash and timestamp.\n\n    Args:\n        content_hash: Hash of the content.\n        timestamp: Optional timestamp (defaults to now).\n\n    Returns:\n        A version identifier.\n    \"\"\"\n    if timestamp is None:\n        timestamp = datetime.now()\n\n    ts_str = timestamp.strftime(\"%Y%m%d%H%M%S\")\n    return f\"v_{content_hash[:12]}_{ts_str}\"\n</code></pre>"},{"location":"reference/ragcrawl/utils/logging/","title":"Logging","text":""},{"location":"reference/ragcrawl/utils/logging/#ragcrawl.utils.logging","title":"<code>logging</code>","text":"<p>Structured logging setup for ragcrawl.</p>"},{"location":"reference/ragcrawl/utils/logging/#ragcrawl.utils.logging.CrawlLoggerAdapter","title":"<code>CrawlLoggerAdapter(run_id, site_id)</code>","text":"<p>Adapter for logging crawl events with consistent context.</p> <p>Initialize the logger adapter.</p> <p>Parameters:</p> Name Type Description Default <code>run_id</code> <code>str</code> <p>Current crawl run ID.</p> required <code>site_id</code> <code>str</code> <p>Site being crawled.</p> required Source code in <code>src/ragcrawl/utils/logging.py</code> <pre><code>def __init__(self, run_id: str, site_id: str) -&gt; None:\n    \"\"\"\n    Initialize the logger adapter.\n\n    Args:\n        run_id: Current crawl run ID.\n        site_id: Site being crawled.\n    \"\"\"\n    self.logger = get_logger(\n        \"ragcrawl\",\n        run_id=run_id,\n        site_id=site_id,\n    )\n</code></pre>"},{"location":"reference/ragcrawl/utils/logging/#ragcrawl.utils.logging.CrawlLoggerAdapter.content_changed","title":"<code>content_changed(url, old_hash, new_hash)</code>","text":"<p>Log content change detection.</p> Source code in <code>src/ragcrawl/utils/logging.py</code> <pre><code>def content_changed(self, url: str, old_hash: str | None, new_hash: str) -&gt; None:\n    \"\"\"Log content change detection.\"\"\"\n    self.logger.info(\n        \"Content changed\",\n        url=url,\n        old_hash=old_hash,\n        new_hash=new_hash,\n    )\n</code></pre>"},{"location":"reference/ragcrawl/utils/logging/#ragcrawl.utils.logging.CrawlLoggerAdapter.page_discovered","title":"<code>page_discovered(url, depth, referrer=None)</code>","text":"<p>Log page discovery.</p> Source code in <code>src/ragcrawl/utils/logging.py</code> <pre><code>def page_discovered(self, url: str, depth: int, referrer: str | None = None) -&gt; None:\n    \"\"\"Log page discovery.\"\"\"\n    self.logger.debug(\n        \"Page discovered\",\n        url=url,\n        depth=depth,\n        referrer=referrer,\n    )\n</code></pre>"},{"location":"reference/ragcrawl/utils/logging/#ragcrawl.utils.logging.CrawlLoggerAdapter.page_extracted","title":"<code>page_extracted(url, markdown_size, links_found, latency_ms)</code>","text":"<p>Log content extraction.</p> Source code in <code>src/ragcrawl/utils/logging.py</code> <pre><code>def page_extracted(\n    self,\n    url: str,\n    markdown_size: int,\n    links_found: int,\n    latency_ms: float,\n) -&gt; None:\n    \"\"\"Log content extraction.\"\"\"\n    self.logger.debug(\n        \"Content extracted\",\n        url=url,\n        markdown_size=markdown_size,\n        links_found=links_found,\n        latency_ms=round(latency_ms, 2),\n    )\n</code></pre>"},{"location":"reference/ragcrawl/utils/logging/#ragcrawl.utils.logging.CrawlLoggerAdapter.page_failed","title":"<code>page_failed(url, error, retry_count=0)</code>","text":"<p>Log page failure.</p> Source code in <code>src/ragcrawl/utils/logging.py</code> <pre><code>def page_failed(self, url: str, error: str, retry_count: int = 0) -&gt; None:\n    \"\"\"Log page failure.\"\"\"\n    self.logger.warning(\n        \"Page failed\",\n        url=url,\n        error=error,\n        retry_count=retry_count,\n    )\n</code></pre>"},{"location":"reference/ragcrawl/utils/logging/#ragcrawl.utils.logging.CrawlLoggerAdapter.page_fetched","title":"<code>page_fetched(url, status_code, latency_ms, size_bytes=None)</code>","text":"<p>Log page fetch completion.</p> Source code in <code>src/ragcrawl/utils/logging.py</code> <pre><code>def page_fetched(\n    self,\n    url: str,\n    status_code: int,\n    latency_ms: float,\n    size_bytes: int | None = None,\n) -&gt; None:\n    \"\"\"Log page fetch completion.\"\"\"\n    self.logger.info(\n        \"Page fetched\",\n        url=url,\n        status_code=status_code,\n        latency_ms=round(latency_ms, 2),\n        size_bytes=size_bytes,\n    )\n</code></pre>"},{"location":"reference/ragcrawl/utils/logging/#ragcrawl.utils.logging.CrawlLoggerAdapter.page_skipped","title":"<code>page_skipped(url, reason)</code>","text":"<p>Log page skip.</p> Source code in <code>src/ragcrawl/utils/logging.py</code> <pre><code>def page_skipped(self, url: str, reason: str) -&gt; None:\n    \"\"\"Log page skip.\"\"\"\n    self.logger.debug(\"Page skipped\", url=url, reason=reason)\n</code></pre>"},{"location":"reference/ragcrawl/utils/logging/#ragcrawl.utils.logging.CrawlLoggerAdapter.run_completed","title":"<code>run_completed(stats, duration_seconds)</code>","text":"<p>Log crawl run completion.</p> Source code in <code>src/ragcrawl/utils/logging.py</code> <pre><code>def run_completed(self, stats: dict[str, Any], duration_seconds: float) -&gt; None:\n    \"\"\"Log crawl run completion.\"\"\"\n    self.logger.info(\n        \"Crawl run completed\",\n        stats=stats,\n        duration_seconds=round(duration_seconds, 2),\n    )\n</code></pre>"},{"location":"reference/ragcrawl/utils/logging/#ragcrawl.utils.logging.CrawlLoggerAdapter.run_failed","title":"<code>run_failed(error)</code>","text":"<p>Log crawl run failure.</p> Source code in <code>src/ragcrawl/utils/logging.py</code> <pre><code>def run_failed(self, error: str) -&gt; None:\n    \"\"\"Log crawl run failure.\"\"\"\n    self.logger.error(\"Crawl run failed\", error=error)\n</code></pre>"},{"location":"reference/ragcrawl/utils/logging/#ragcrawl.utils.logging.CrawlLoggerAdapter.run_started","title":"<code>run_started(seeds, config_summary)</code>","text":"<p>Log crawl run start.</p> Source code in <code>src/ragcrawl/utils/logging.py</code> <pre><code>def run_started(self, seeds: list[str], config_summary: dict[str, Any]) -&gt; None:\n    \"\"\"Log crawl run start.\"\"\"\n    self.logger.info(\n        \"Crawl run started\",\n        seeds=seeds,\n        config=config_summary,\n    )\n</code></pre>"},{"location":"reference/ragcrawl/utils/logging/#ragcrawl.utils.logging.CrawlLoggerAdapter.tombstone_created","title":"<code>tombstone_created(url, status_code)</code>","text":"<p>Log tombstone creation for deleted page.</p> Source code in <code>src/ragcrawl/utils/logging.py</code> <pre><code>def tombstone_created(self, url: str, status_code: int) -&gt; None:\n    \"\"\"Log tombstone creation for deleted page.\"\"\"\n    self.logger.info(\n        \"Tombstone created\",\n        url=url,\n        status_code=status_code,\n    )\n</code></pre>"},{"location":"reference/ragcrawl/utils/logging/#ragcrawl.utils.logging.get_logger","title":"<code>get_logger(name, **initial_context)</code>","text":"<p>Get a structured logger instance.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Logger name (typically module name).</p> required <code>**initial_context</code> <code>Any</code> <p>Initial context to bind to the logger.</p> <code>{}</code> <p>Returns:</p> Type Description <code>BoundLogger</code> <p>A structlog BoundLogger instance.</p> Source code in <code>src/ragcrawl/utils/logging.py</code> <pre><code>def get_logger(name: str, **initial_context: Any) -&gt; structlog.stdlib.BoundLogger:\n    \"\"\"\n    Get a structured logger instance.\n\n    Args:\n        name: Logger name (typically module name).\n        **initial_context: Initial context to bind to the logger.\n\n    Returns:\n        A structlog BoundLogger instance.\n    \"\"\"\n    logger = structlog.get_logger(name)\n    if initial_context:\n        logger = logger.bind(**initial_context)\n    return logger\n</code></pre>"},{"location":"reference/ragcrawl/utils/logging/#ragcrawl.utils.logging.setup_logging","title":"<code>setup_logging(level=logging.INFO, json_format=False, log_file=None)</code>","text":"<p>Set up structured logging for the crawler.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>int</code> <p>Logging level (default: INFO).</p> <code>INFO</code> <code>json_format</code> <code>bool</code> <p>If True, output logs as JSON.</p> <code>False</code> <code>log_file</code> <code>str | None</code> <p>Optional file path to write logs to.</p> <code>None</code> Source code in <code>src/ragcrawl/utils/logging.py</code> <pre><code>def setup_logging(\n    level: int = logging.INFO,\n    json_format: bool = False,\n    log_file: str | None = None,\n) -&gt; None:\n    \"\"\"\n    Set up structured logging for the crawler.\n\n    Args:\n        level: Logging level (default: INFO).\n        json_format: If True, output logs as JSON.\n        log_file: Optional file path to write logs to.\n    \"\"\"\n    # Configure standard logging\n    logging.basicConfig(\n        format=\"%(message)s\",\n        stream=sys.stdout,\n        level=level,\n    )\n\n    # Configure structlog processors\n    shared_processors: list[structlog.typing.Processor] = [\n        structlog.stdlib.add_log_level,\n        structlog.stdlib.add_logger_name,\n        structlog.processors.TimeStamper(fmt=\"iso\"),\n        structlog.processors.StackInfoRenderer(),\n        structlog.processors.UnicodeDecoder(),\n    ]\n\n    if json_format:\n        # JSON output for production\n        processors = shared_processors + [\n            structlog.processors.format_exc_info,\n            structlog.processors.JSONRenderer(),\n        ]\n    else:\n        # Console output for development\n        processors = shared_processors + [\n            structlog.dev.ConsoleRenderer(colors=True),\n        ]\n\n    structlog.configure(\n        processors=processors,\n        wrapper_class=structlog.stdlib.BoundLogger,\n        context_class=dict,\n        logger_factory=structlog.stdlib.LoggerFactory(),\n        cache_logger_on_first_use=True,\n    )\n\n    # Add file handler if specified\n    if log_file:\n        file_handler = logging.FileHandler(log_file)\n        file_handler.setLevel(level)\n        logging.getLogger().addHandler(file_handler)\n</code></pre>"},{"location":"reference/ragcrawl/utils/metrics/","title":"Metrics","text":""},{"location":"reference/ragcrawl/utils/metrics/#ragcrawl.utils.metrics","title":"<code>metrics</code>","text":"<p>Metrics collection for crawl runs.</p>"},{"location":"reference/ragcrawl/utils/metrics/#ragcrawl.utils.metrics.CrawlMetrics","title":"<code>CrawlMetrics(pages_discovered=0, pages_crawled=0, pages_succeeded=0, pages_failed=0, pages_skipped=0, pages_changed=0, pages_unchanged=0, pages_new=0, pages_deleted=0, total_bytes=0, total_fetch_time_ms=0.0, total_extraction_time_ms=0.0, start_time=time.time(), end_time=None, domains=(lambda: defaultdict(DomainMetrics))(), errors_by_type=(lambda: defaultdict(int))(), status_codes=(lambda: defaultdict(int))())</code>  <code>dataclass</code>","text":"<p>Aggregated metrics for a crawl run.</p>"},{"location":"reference/ragcrawl/utils/metrics/#ragcrawl.utils.metrics.CrawlMetrics.avg_fetch_latency_ms","title":"<code>avg_fetch_latency_ms</code>  <code>property</code>","text":"<p>Average fetch latency.</p>"},{"location":"reference/ragcrawl/utils/metrics/#ragcrawl.utils.metrics.CrawlMetrics.duration_seconds","title":"<code>duration_seconds</code>  <code>property</code>","text":"<p>Total duration in seconds.</p>"},{"location":"reference/ragcrawl/utils/metrics/#ragcrawl.utils.metrics.CrawlMetrics.pages_per_second","title":"<code>pages_per_second</code>  <code>property</code>","text":"<p>Crawl throughput in pages per second.</p>"},{"location":"reference/ragcrawl/utils/metrics/#ragcrawl.utils.metrics.CrawlMetrics.to_dict","title":"<code>to_dict()</code>","text":"<p>Convert metrics to dictionary.</p> Source code in <code>src/ragcrawl/utils/metrics.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Convert metrics to dictionary.\"\"\"\n    return {\n        \"pages_discovered\": self.pages_discovered,\n        \"pages_crawled\": self.pages_crawled,\n        \"pages_succeeded\": self.pages_succeeded,\n        \"pages_failed\": self.pages_failed,\n        \"pages_skipped\": self.pages_skipped,\n        \"pages_changed\": self.pages_changed,\n        \"pages_unchanged\": self.pages_unchanged,\n        \"pages_new\": self.pages_new,\n        \"pages_deleted\": self.pages_deleted,\n        \"total_bytes\": self.total_bytes,\n        \"total_fetch_time_ms\": self.total_fetch_time_ms,\n        \"total_extraction_time_ms\": self.total_extraction_time_ms,\n        \"duration_seconds\": self.duration_seconds,\n        \"avg_fetch_latency_ms\": self.avg_fetch_latency_ms,\n        \"pages_per_second\": self.pages_per_second,\n        \"domains_crawled\": len(self.domains),\n        \"status_codes\": dict(self.status_codes),\n        \"errors_by_type\": dict(self.errors_by_type),\n    }\n</code></pre>"},{"location":"reference/ragcrawl/utils/metrics/#ragcrawl.utils.metrics.DomainMetrics","title":"<code>DomainMetrics(requests=0, successes=0, failures=0, total_latency_ms=0.0, total_bytes=0, status_codes=(lambda: defaultdict(int))(), errors=(lambda: defaultdict(int))())</code>  <code>dataclass</code>","text":"<p>Metrics for a single domain.</p>"},{"location":"reference/ragcrawl/utils/metrics/#ragcrawl.utils.metrics.DomainMetrics.avg_latency_ms","title":"<code>avg_latency_ms</code>  <code>property</code>","text":"<p>Average latency in milliseconds.</p>"},{"location":"reference/ragcrawl/utils/metrics/#ragcrawl.utils.metrics.DomainMetrics.success_rate","title":"<code>success_rate</code>  <code>property</code>","text":"<p>Success rate as a ratio.</p>"},{"location":"reference/ragcrawl/utils/metrics/#ragcrawl.utils.metrics.MetricsCollector","title":"<code>MetricsCollector()</code>","text":"<p>Collector for crawl metrics with thread-safe updates.</p> <p>Initialize the metrics collector.</p> Source code in <code>src/ragcrawl/utils/metrics.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize the metrics collector.\"\"\"\n    self.metrics = CrawlMetrics()\n    self._lock_placeholder = True  # Placeholder for thread safety if needed\n</code></pre>"},{"location":"reference/ragcrawl/utils/metrics/#ragcrawl.utils.metrics.MetricsCollector.finalize","title":"<code>finalize()</code>","text":"<p>Finalize metrics and return.</p> Source code in <code>src/ragcrawl/utils/metrics.py</code> <pre><code>def finalize(self) -&gt; CrawlMetrics:\n    \"\"\"Finalize metrics and return.\"\"\"\n    self.metrics.end_time = time.time()\n    return self.metrics\n</code></pre>"},{"location":"reference/ragcrawl/utils/metrics/#ragcrawl.utils.metrics.MetricsCollector.get_domain_stats","title":"<code>get_domain_stats()</code>","text":"<p>Get per-domain statistics.</p> Source code in <code>src/ragcrawl/utils/metrics.py</code> <pre><code>def get_domain_stats(self) -&gt; dict[str, dict[str, Any]]:\n    \"\"\"Get per-domain statistics.\"\"\"\n    return {\n        domain: {\n            \"requests\": dm.requests,\n            \"successes\": dm.successes,\n            \"failures\": dm.failures,\n            \"avg_latency_ms\": dm.avg_latency_ms,\n            \"total_bytes\": dm.total_bytes,\n            \"success_rate\": dm.success_rate,\n        }\n        for domain, dm in self.metrics.domains.items()\n    }\n</code></pre>"},{"location":"reference/ragcrawl/utils/metrics/#ragcrawl.utils.metrics.MetricsCollector.record_change","title":"<code>record_change(is_new=False)</code>","text":"<p>Record content change.</p> Source code in <code>src/ragcrawl/utils/metrics.py</code> <pre><code>def record_change(self, is_new: bool = False) -&gt; None:\n    \"\"\"Record content change.\"\"\"\n    self.metrics.pages_changed += 1\n    if is_new:\n        self.metrics.pages_new += 1\n</code></pre>"},{"location":"reference/ragcrawl/utils/metrics/#ragcrawl.utils.metrics.MetricsCollector.record_deletion","title":"<code>record_deletion()</code>","text":"<p>Record page deletion (tombstone).</p> Source code in <code>src/ragcrawl/utils/metrics.py</code> <pre><code>def record_deletion(self) -&gt; None:\n    \"\"\"Record page deletion (tombstone).\"\"\"\n    self.metrics.pages_deleted += 1\n</code></pre>"},{"location":"reference/ragcrawl/utils/metrics/#ragcrawl.utils.metrics.MetricsCollector.record_discovery","title":"<code>record_discovery(count=1)</code>","text":"<p>Record URL discovery.</p> Source code in <code>src/ragcrawl/utils/metrics.py</code> <pre><code>def record_discovery(self, count: int = 1) -&gt; None:\n    \"\"\"Record URL discovery.\"\"\"\n    self.metrics.pages_discovered += count\n</code></pre>"},{"location":"reference/ragcrawl/utils/metrics/#ragcrawl.utils.metrics.MetricsCollector.record_error","title":"<code>record_error(error_type, domain=None)</code>","text":"<p>Record an error.</p> Source code in <code>src/ragcrawl/utils/metrics.py</code> <pre><code>def record_error(self, error_type: str, domain: str | None = None) -&gt; None:\n    \"\"\"Record an error.\"\"\"\n    self.metrics.errors_by_type[error_type] += 1\n    if domain:\n        self.metrics.domains[domain].errors[error_type] += 1\n</code></pre>"},{"location":"reference/ragcrawl/utils/metrics/#ragcrawl.utils.metrics.MetricsCollector.record_extraction","title":"<code>record_extraction(latency_ms)</code>","text":"<p>Record extraction time.</p> Source code in <code>src/ragcrawl/utils/metrics.py</code> <pre><code>def record_extraction(self, latency_ms: float) -&gt; None:\n    \"\"\"Record extraction time.\"\"\"\n    self.metrics.total_extraction_time_ms += latency_ms\n</code></pre>"},{"location":"reference/ragcrawl/utils/metrics/#ragcrawl.utils.metrics.MetricsCollector.record_fetch","title":"<code>record_fetch(domain, status_code, latency_ms, bytes_downloaded, success=True)</code>","text":"<p>Record a fetch operation.</p> Source code in <code>src/ragcrawl/utils/metrics.py</code> <pre><code>def record_fetch(\n    self,\n    domain: str,\n    status_code: int,\n    latency_ms: float,\n    bytes_downloaded: int,\n    success: bool = True,\n) -&gt; None:\n    \"\"\"Record a fetch operation.\"\"\"\n    self.metrics.pages_crawled += 1\n    self.metrics.total_bytes += bytes_downloaded\n    self.metrics.total_fetch_time_ms += latency_ms\n    self.metrics.status_codes[status_code] += 1\n\n    if success:\n        self.metrics.pages_succeeded += 1\n    else:\n        self.metrics.pages_failed += 1\n\n    # Domain metrics\n    dm = self.metrics.domains[domain]\n    dm.requests += 1\n    dm.total_latency_ms += latency_ms\n    dm.total_bytes += bytes_downloaded\n    dm.status_codes[status_code] += 1\n    if success:\n        dm.successes += 1\n    else:\n        dm.failures += 1\n</code></pre>"},{"location":"reference/ragcrawl/utils/metrics/#ragcrawl.utils.metrics.MetricsCollector.record_skip","title":"<code>record_skip(reason='filtered')</code>","text":"<p>Record a skipped page.</p> Source code in <code>src/ragcrawl/utils/metrics.py</code> <pre><code>def record_skip(self, reason: str = \"filtered\") -&gt; None:\n    \"\"\"Record a skipped page.\"\"\"\n    self.metrics.pages_skipped += 1\n</code></pre>"},{"location":"reference/ragcrawl/utils/metrics/#ragcrawl.utils.metrics.MetricsCollector.record_unchanged","title":"<code>record_unchanged()</code>","text":"<p>Record unchanged page (304 or hash match).</p> Source code in <code>src/ragcrawl/utils/metrics.py</code> <pre><code>def record_unchanged(self) -&gt; None:\n    \"\"\"Record unchanged page (304 or hash match).\"\"\"\n    self.metrics.pages_unchanged += 1\n</code></pre>"},{"location":"user-guide/chunking/","title":"Chunking Guide","text":"<p>Split documents into chunks optimized for RAG (Retrieval-Augmented Generation).</p>"},{"location":"user-guide/chunking/#why-chunk","title":"Why Chunk?","text":"<p>LLMs have context limits. Chunking helps you:</p> <ul> <li>Fit context windows: Keep chunks under token limits</li> <li>Improve retrieval: Smaller, focused chunks match queries better</li> <li>Preserve structure: Maintain document hierarchy in chunks</li> </ul>"},{"location":"user-guide/chunking/#chunking-strategies","title":"Chunking Strategies","text":""},{"location":"user-guide/chunking/#heading-based-chunking","title":"Heading-Based Chunking","text":"<p>Splits content at Markdown headings, preserving document structure:</p> <pre><code>from ragcrawl.chunking.heading_chunker import HeadingChunker\n\nchunker = HeadingChunker(\n    min_level=1,        # Start splitting at H1\n    max_level=3,        # Stop at H3 (don't split H4+)\n    min_chunk_chars=100,  # Minimum chunk size\n)\n\nchunks = chunker.chunk(markdown_content)\n\nfor chunk in chunks:\n    print(f\"Heading: {' &gt; '.join(chunk.heading_path)}\")\n    print(f\"Content: {chunk.content[:100]}...\")\n    print()\n</code></pre> <p>Output: <pre><code>Heading: Getting Started\nContent: This guide helps you get started with...\n\nHeading: Getting Started &gt; Installation\nContent: Install the package using pip...\n\nHeading: Getting Started &gt; Configuration\nContent: Configure your settings in config.yaml...\n</code></pre></p>"},{"location":"user-guide/chunking/#token-based-chunking","title":"Token-Based Chunking","text":"<p>Splits content by token count with overlap:</p> <pre><code>from ragcrawl.chunking.token_chunker import TokenChunker\n\nchunker = TokenChunker(\n    max_tokens=500,      # Maximum tokens per chunk\n    overlap_tokens=50,   # Overlap between chunks\n    encoding_name=\"cl100k_base\",  # OpenAI tokenizer\n)\n\nchunks = chunker.chunk(content)\n\nfor chunk in chunks:\n    print(f\"Chunk {chunk.chunk_index}: {chunk.token_count} tokens\")\n</code></pre>"},{"location":"user-guide/chunking/#chunking-documents","title":"Chunking Documents","text":""},{"location":"user-guide/chunking/#single-document","title":"Single Document","text":"<pre><code>from ragcrawl.models.document import Document\nfrom ragcrawl.chunking.heading_chunker import HeadingChunker\n\ndoc = Document(\n    doc_id=\"doc123\",\n    url=\"https://example.com/guide\",\n    title=\"User Guide\",\n    content=\"# Getting Started\\n\\n...\",\n    # ... other fields\n)\n\nchunker = HeadingChunker()\nchunks = chunker.chunk(doc.content)\n\n# Associate chunks with document\nfor chunk in chunks:\n    chunk.doc_id = doc.doc_id\n</code></pre>"},{"location":"user-guide/chunking/#batch-processing","title":"Batch Processing","text":"<pre><code>from ragcrawl.chunking.heading_chunker import HeadingChunker\n\nchunker = HeadingChunker()\nall_chunks = []\n\nfor doc in documents:\n    chunks = chunker.chunk(doc.content)\n    for chunk in chunks:\n        chunk.doc_id = doc.doc_id\n        all_chunks.append(chunk)\n\nprint(f\"Created {len(all_chunks)} chunks from {len(documents)} documents\")\n</code></pre>"},{"location":"user-guide/chunking/#chunk-metadata","title":"Chunk Metadata","text":"<p>Each chunk includes metadata:</p> <pre><code>from ragcrawl.models.chunk import Chunk\n\nchunk = Chunk(\n    chunk_id=\"chunk_abc123\",\n    doc_id=\"doc123\",\n    content=\"The actual chunk content...\",\n    chunk_index=0,           # Position in document\n    char_count=500,          # Character count\n    token_count=120,         # Token count (if computed)\n    heading_path=[\"Guide\", \"Setup\"],  # Heading hierarchy\n)\n</code></pre>"},{"location":"user-guide/chunking/#configuration-examples","title":"Configuration Examples","text":""},{"location":"user-guide/chunking/#for-rag-systems","title":"For RAG Systems","text":"<p>Optimize for semantic search:</p> <pre><code># Heading-based for structured content\nheading_chunker = HeadingChunker(\n    min_level=2,          # Keep H1 content together\n    max_level=3,\n    min_chunk_chars=200,  # Avoid tiny chunks\n)\n\n# Token-based for unstructured content\ntoken_chunker = TokenChunker(\n    max_tokens=256,       # Smaller chunks for better matching\n    overlap_tokens=30,    # Overlap for context continuity\n)\n</code></pre>"},{"location":"user-guide/chunking/#for-summarization","title":"For Summarization","text":"<p>Larger chunks preserve more context:</p> <pre><code>chunker = TokenChunker(\n    max_tokens=1000,\n    overlap_tokens=100,\n)\n</code></pre>"},{"location":"user-guide/chunking/#for-qa","title":"For Q&amp;A","text":"<p>Balance chunk size and specificity:</p> <pre><code>chunker = HeadingChunker(\n    min_level=2,\n    max_level=4,          # More granular splitting\n    min_chunk_chars=100,\n)\n</code></pre>"},{"location":"user-guide/chunking/#hybrid-chunking","title":"Hybrid Chunking","text":"<p>Combine strategies for best results:</p> <pre><code>from ragcrawl.chunking.heading_chunker import HeadingChunker\nfrom ragcrawl.chunking.token_chunker import TokenChunker\n\n# First split by headings\nheading_chunker = HeadingChunker(max_level=2)\nheading_chunks = heading_chunker.chunk(content)\n\n# Then split large sections by tokens\ntoken_chunker = TokenChunker(max_tokens=500, overlap_tokens=50)\nfinal_chunks = []\n\nfor chunk in heading_chunks:\n    if chunk.token_count &gt; 500:\n        # Split large sections\n        sub_chunks = token_chunker.chunk(chunk.content)\n        for sub in sub_chunks:\n            sub.heading_path = chunk.heading_path\n            final_chunks.append(sub)\n    else:\n        final_chunks.append(chunk)\n</code></pre>"},{"location":"user-guide/chunking/#exporting-chunks","title":"Exporting Chunks","text":""},{"location":"user-guide/chunking/#to-json","title":"To JSON","text":"<pre><code>import json\n\nchunks_data = [\n    {\n        \"chunk_id\": chunk.chunk_id,\n        \"doc_id\": chunk.doc_id,\n        \"content\": chunk.content,\n        \"heading_path\": chunk.heading_path,\n        \"token_count\": chunk.token_count,\n    }\n    for chunk in chunks\n]\n\nwith open(\"chunks.json\", \"w\") as f:\n    json.dump(chunks_data, f, indent=2)\n</code></pre>"},{"location":"user-guide/chunking/#to-vector-database-format","title":"To Vector Database Format","text":"<pre><code># Format for Pinecone, Weaviate, etc.\nvectors = []\nfor chunk in chunks:\n    vectors.append({\n        \"id\": chunk.chunk_id,\n        \"text\": chunk.content,\n        \"metadata\": {\n            \"doc_id\": chunk.doc_id,\n            \"heading\": \" &gt; \".join(chunk.heading_path or []),\n            \"char_count\": chunk.char_count,\n        },\n    })\n</code></pre>"},{"location":"user-guide/chunking/#best-practices","title":"Best Practices","text":"<ol> <li>Match chunk size to your model: GPT-4 handles larger chunks than smaller models</li> <li>Use overlap for continuity: Prevents information loss at boundaries</li> <li>Preserve structure: Heading paths help with retrieval and citation</li> <li>Test chunk quality: Evaluate retrieval performance with your queries</li> <li>Consider content type: Code needs different chunking than prose</li> </ol>"},{"location":"user-guide/crawling/","title":"Crawling Guide","text":"<p>This guide covers all aspects of crawling websites with ragcrawl.</p>"},{"location":"user-guide/crawling/#how-crawling-works","title":"How Crawling Works","text":"<ol> <li>Seed URLs: The crawler starts from one or more seed URLs</li> <li>Fetch: Each page is fetched via HTTP or browser rendering</li> <li>Extract: Content is converted to Markdown, links are extracted</li> <li>Filter: Links are filtered based on domain, patterns, and depth</li> <li>Queue: New links are added to the priority queue (frontier)</li> <li>Store: Page content and metadata are stored</li> <li>Repeat: Process continues until limits are reached</li> </ol>"},{"location":"user-guide/crawling/#cli-usage","title":"CLI Usage","text":""},{"location":"user-guide/crawling/#basic-crawl","title":"Basic Crawl","text":"<pre><code>ragcrawl crawl https://docs.example.com\n</code></pre>"},{"location":"user-guide/crawling/#full-options","title":"Full Options","text":"<pre><code>ragcrawl crawl https://docs.example.com \\\n    --max-pages 500 \\\n    --max-depth 10 \\\n    --output ./output \\\n    --output-mode multi \\\n    --storage ./crawler.duckdb \\\n    --include \"/docs/.*\" \\\n    --exclude \"/admin/.*\" \\\n    --robots \\\n    --js \\\n    --export-json ./docs.json \\\n    --export-jsonl ./docs.jsonl \\\n    --verbose\n</code></pre>"},{"location":"user-guide/crawling/#cli-options-reference","title":"CLI Options Reference","text":"Option Short Description <code>--max-pages</code> <code>-m</code> Maximum pages to crawl (default: from config) <code>--max-depth</code> <code>-d</code> Maximum crawl depth (default: from config) <code>--output</code> <code>-o</code> Output directory <code>--output-mode</code> <code>single</code> or <code>multi</code> page output <code>--storage</code> <code>-s</code> DuckDB storage path (default: <code>~/.ragcrawl/ragcrawl.duckdb</code>) <code>--include</code> <code>-i</code> Include URL patterns (regex, repeatable) <code>--exclude</code> <code>-e</code> Exclude URL patterns (regex, repeatable) <code>--robots/--no-robots</code> Respect robots.txt (default: enabled) <code>--js/--no-js</code> Enable JavaScript rendering <code>--export-json</code> Export documents to JSON file <code>--export-jsonl</code> Export documents to JSONL file <code>--verbose</code> <code>-v</code> Verbose output"},{"location":"user-guide/crawling/#multiple-seeds","title":"Multiple Seeds","text":"<p>You can specify multiple seed URLs:</p> <pre><code>ragcrawl crawl https://docs.example.com https://api.example.com\n</code></pre>"},{"location":"user-guide/crawling/#crawl-modes","title":"Crawl Modes","text":""},{"location":"user-guide/crawling/#http-mode-default","title":"HTTP Mode (Default)","text":"<p>Fast crawling using HTTP requests. Best for static sites:</p> <pre><code>from ragcrawl.config.crawler_config import CrawlerConfig, FetchMode\n\nconfig = CrawlerConfig(\n    seeds=[\"https://docs.example.com\"],\n    fetch_mode=FetchMode.HTTP,\n)\n</code></pre>"},{"location":"user-guide/crawling/#browser-mode","title":"Browser Mode","text":"<p>Uses headless Chromium for JavaScript-heavy sites:</p> <pre><code>config = CrawlerConfig(\n    seeds=[\"https://app.example.com\"],\n    fetch_mode=FetchMode.BROWSER,\n)\n</code></pre> <p>CLI: <pre><code>ragcrawl crawl https://app.example.com --js\n</code></pre></p>"},{"location":"user-guide/crawling/#hybrid-mode","title":"Hybrid Mode","text":"<p>Tries HTTP first, falls back to browser on failure:</p> <pre><code>config = CrawlerConfig(\n    seeds=[\"https://example.com\"],\n    fetch_mode=FetchMode.HYBRID,\n)\n</code></pre>"},{"location":"user-guide/crawling/#url-filtering","title":"URL Filtering","text":""},{"location":"user-guide/crawling/#domain-restrictions","title":"Domain Restrictions","text":"<p>By default, the crawler stays within the seed domains:</p> <pre><code>config = CrawlerConfig(\n    seeds=[\"https://docs.example.com\"],\n    allowed_domains=[\"docs.example.com\", \"api.example.com\"],\n    allow_subdomains=True,  # Also allows sub.docs.example.com\n)\n</code></pre>"},{"location":"user-guide/crawling/#path-patterns","title":"Path Patterns","text":"<p>Use regex patterns to include or exclude URLs:</p> <pre><code>config = CrawlerConfig(\n    seeds=[\"https://example.com\"],\n    include_patterns=[\n        r\"/docs/.*\",      # Only crawl /docs/\n        r\"/api/v\\d+/.*\",  # API versioned paths\n    ],\n    exclude_patterns=[\n        r\"/admin/.*\",     # Skip admin pages\n        r\".*\\.pdf$\",      # Skip PDFs\n    ],\n)\n</code></pre>"},{"location":"user-guide/crawling/#depth-limiting","title":"Depth Limiting","text":"<p>Control how deep the crawler goes from seed URLs:</p> <pre><code>config = CrawlerConfig(\n    seeds=[\"https://example.com\"],\n    max_depth=3,  # Maximum 3 clicks from seed\n)\n</code></pre>"},{"location":"user-guide/crawling/#rate-limiting","title":"Rate Limiting","text":""},{"location":"user-guide/crawling/#per-domain-rate-limits","title":"Per-Domain Rate Limits","text":"<p>Respect website resources:</p> <pre><code>config = CrawlerConfig(\n    seeds=[\"https://example.com\"],\n    requests_per_second=2.0,  # Max 2 requests/second per domain\n    concurrent_requests=5,     # Max 5 concurrent requests total\n)\n</code></pre>"},{"location":"user-guide/crawling/#delays","title":"Delays","text":"<p>Add delays between requests:</p> <pre><code>config = CrawlerConfig(\n    seeds=[\"https://example.com\"],\n    delay_range=(1.0, 3.0),  # Random delay 1-3 seconds\n)\n</code></pre>"},{"location":"user-guide/crawling/#robotstxt-compliance","title":"Robots.txt Compliance","text":""},{"location":"user-guide/crawling/#strict-mode-default","title":"Strict Mode (Default)","text":"<p>Respects all robots.txt directives:</p> <pre><code>from ragcrawl.config.crawler_config import RobotsMode\n\nconfig = CrawlerConfig(\n    seeds=[\"https://example.com\"],\n    robots_mode=RobotsMode.STRICT,\n)\n</code></pre>"},{"location":"user-guide/crawling/#off-mode","title":"Off Mode","text":"<p>Ignores robots.txt (use responsibly):</p> <pre><code>config = CrawlerConfig(\n    seeds=[\"https://example.com\"],\n    robots_mode=RobotsMode.OFF,\n)\n</code></pre>"},{"location":"user-guide/crawling/#page-limits","title":"Page Limits","text":""},{"location":"user-guide/crawling/#total-pages","title":"Total Pages","text":"<pre><code>config = CrawlerConfig(\n    seeds=[\"https://example.com\"],\n    max_pages=1000,  # Stop after 1000 pages\n)\n</code></pre>"},{"location":"user-guide/crawling/#per-domain-limits","title":"Per-Domain Limits","text":"<pre><code>config = CrawlerConfig(\n    seeds=[\"https://example.com\", \"https://other.com\"],\n    max_pages_per_domain=500,  # Max 500 pages per domain\n)\n</code></pre>"},{"location":"user-guide/crawling/#error-handling","title":"Error Handling","text":""},{"location":"user-guide/crawling/#retry-configuration","title":"Retry Configuration","text":"<pre><code>config = CrawlerConfig(\n    seeds=[\"https://example.com\"],\n    max_retries=3,           # Retry failed requests\n    retry_delay=5.0,         # Wait 5 seconds between retries\n    timeout=30.0,            # 30 second timeout per request\n)\n</code></pre>"},{"location":"user-guide/crawling/#circuit-breaker","title":"Circuit Breaker","text":"<p>The crawler automatically stops requesting from failing domains:</p> <pre><code>config = CrawlerConfig(\n    seeds=[\"https://example.com\"],\n    circuit_breaker_threshold=10,  # Stop after 10 consecutive failures\n    circuit_breaker_reset=300,     # Reset after 5 minutes\n)\n</code></pre>"},{"location":"user-guide/crawling/#hooks-and-callbacks","title":"Hooks and Callbacks","text":""},{"location":"user-guide/crawling/#on-page-crawled","title":"On Page Crawled","text":"<pre><code>from ragcrawl.hooks.callbacks import HookManager\n\ndef on_page(url: str, content: str, metadata: dict):\n    print(f\"Crawled: {url} ({len(content)} chars)\")\n\nhooks = HookManager()\nhooks.on_page_crawled(on_page)\n\nconfig = CrawlerConfig(\n    seeds=[\"https://example.com\"],\n    hooks=hooks,\n)\n</code></pre>"},{"location":"user-guide/crawling/#on-error","title":"On Error","text":"<pre><code>def on_error(url: str, error: Exception):\n    print(f\"Error crawling {url}: {error}\")\n\nhooks.on_error(on_error)\n</code></pre>"},{"location":"user-guide/crawling/#content-processing","title":"Content Processing","text":""},{"location":"user-guide/crawling/#redaction","title":"Redaction","text":"<p>Remove sensitive content before storage:</p> <pre><code>from ragcrawl.hooks.callbacks import PatternRedactor\n\nredactor = PatternRedactor([\n    r\"\\b\\d{3}-\\d{2}-\\d{4}\\b\",  # SSN pattern\n    r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b\",  # Email\n])\n\nhooks = HookManager()\nhooks.add_redactor(redactor)\n</code></pre>"},{"location":"user-guide/crawling/#complete-example","title":"Complete Example","text":"<pre><code>import asyncio\nfrom ragcrawl.config.crawler_config import (\n    CrawlerConfig,\n    FetchMode,\n    RobotsMode,\n)\nfrom ragcrawl.config.output_config import OutputConfig, OutputMode\nfrom ragcrawl.config.storage_config import DuckDBConfig, StorageConfig\nfrom ragcrawl.core.crawl_job import CrawlJob\n\nasync def crawl_documentation():\n    config = CrawlerConfig(\n        # Seeds\n        seeds=[\"https://docs.example.com\"],\n\n        # Limits\n        max_pages=1000,\n        max_depth=10,\n\n        # Filtering\n        include_patterns=[r\"/docs/.*\", r\"/tutorials/.*\"],\n        exclude_patterns=[r\"/admin/.*\"],\n\n        # Fetching\n        fetch_mode=FetchMode.HTTP,\n        robots_mode=RobotsMode.STRICT,\n        requests_per_second=2.0,\n        concurrent_requests=5,\n        timeout=30.0,\n\n        # Storage\n        storage=StorageConfig(\n            backend=DuckDBConfig(path=\"./docs.duckdb\")\n        ),\n\n        # Output\n        output=OutputConfig(\n            mode=OutputMode.MULTI,\n            root_dir=\"./docs-output\",\n            include_metadata=True,\n            rewrite_links=True,\n        ),\n    )\n\n    job = CrawlJob(config)\n    result = await job.run()\n\n    if result.success:\n        print(f\"Successfully crawled {result.stats.pages_crawled} pages\")\n        print(f\"Failed: {result.stats.pages_failed}\")\n        print(f\"Duration: {result.duration_seconds:.1f}s\")\n    else:\n        print(f\"Crawl failed: {result.error}\")\n\nasyncio.run(crawl_documentation())\n</code></pre>"},{"location":"user-guide/exporting/","title":"Exporting Guide","text":"<p>Export crawled content in various formats.</p>"},{"location":"user-guide/exporting/#export-formats","title":"Export Formats","text":""},{"location":"user-guide/exporting/#json-export","title":"JSON Export","text":"<p>Export all documents to a single JSON file:</p> <pre><code>from ragcrawl.export.json_exporter import JSONExporter\nfrom pathlib import Path\n\nexporter = JSONExporter(indent=2)\nexporter.export_documents(documents, Path(\"./docs.json\"))\n</code></pre> <p>Output format: <pre><code>[\n  {\n    \"doc_id\": \"abc123\",\n    \"url\": \"https://example.com/page1\",\n    \"title\": \"Page 1\",\n    \"content\": \"# Page 1\\n\\nContent here...\",\n    \"fetched_at\": \"2024-01-15T10:30:00Z\",\n    \"status_code\": 200,\n    \"word_count\": 500\n  },\n  {\n    \"doc_id\": \"def456\",\n    \"url\": \"https://example.com/page2\",\n    \"title\": \"Page 2\",\n    \"content\": \"# Page 2\\n\\nMore content...\",\n    \"fetched_at\": \"2024-01-15T10:31:00Z\",\n    \"status_code\": 200,\n    \"word_count\": 300\n  }\n]\n</code></pre></p>"},{"location":"user-guide/exporting/#jsonl-export-streaming","title":"JSONL Export (Streaming)","text":"<p>Export one document per line for streaming/large datasets:</p> <pre><code>from ragcrawl.export.json_exporter import JSONLExporter\nfrom pathlib import Path\n\nexporter = JSONLExporter()\nexporter.export_documents(documents, Path(\"./docs.jsonl\"))\n</code></pre> <p>Output format: <pre><code>{\"doc_id\":\"abc123\",\"url\":\"https://example.com/page1\",\"title\":\"Page 1\",...}\n{\"doc_id\":\"def456\",\"url\":\"https://example.com/page2\",\"title\":\"Page 2\",...}\n</code></pre></p>"},{"location":"user-guide/exporting/#cli-export","title":"CLI Export","text":""},{"location":"user-guide/exporting/#during-crawl","title":"During Crawl","text":"<pre><code># Export to JSON\nragcrawl crawl https://example.com --export-json ./docs.json\n\n# Export to JSONL\nragcrawl crawl https://example.com --export-jsonl ./docs.jsonl\n\n# Both formats\nragcrawl crawl https://example.com \\\n    --export-json ./docs.json \\\n    --export-jsonl ./docs.jsonl\n</code></pre>"},{"location":"user-guide/exporting/#from-storage","title":"From Storage","text":"<p>Export previously crawled content:</p> <pre><code>from ragcrawl.storage.backend import create_storage_backend\nfrom ragcrawl.config.storage_config import DuckDBConfig, StorageConfig\nfrom ragcrawl.export.json_exporter import JSONExporter\nfrom ragcrawl.models.document import Document\n\n# Connect to storage\nconfig = StorageConfig(backend=DuckDBConfig(path=\"./crawler.duckdb\"))\nbackend = create_storage_backend(config)\nbackend.initialize()\n\n# Get site\nsite = backend.list_sites()[0]\n\n# Get all pages with latest versions\ndocuments = []\npages = backend.list_pages(site.site_id)\n\nfor page in pages:\n    if page.is_tombstone:\n        continue\n\n    version = backend.get_latest_version(page.page_id)\n    if version:\n        doc = Document(\n            doc_id=page.page_id,\n            url=page.url,\n            title=version.title,\n            content=version.markdown,\n            fetched_at=version.crawled_at,\n            status_code=version.status_code,\n            content_type=version.content_type,\n            word_count=version.word_count,\n        )\n        documents.append(doc)\n\n# Export\nexporter = JSONExporter()\nexporter.export_documents(documents, Path(\"./export.json\"))\n\nbackend.close()\n</code></pre>"},{"location":"user-guide/exporting/#custom-export-fields","title":"Custom Export Fields","text":""},{"location":"user-guide/exporting/#select-specific-fields","title":"Select Specific Fields","text":"<pre><code>import json\nfrom pathlib import Path\n\ndef export_minimal(documents, output_path):\n    \"\"\"Export only essential fields.\"\"\"\n    data = [\n        {\n            \"url\": doc.url,\n            \"title\": doc.title,\n            \"content\": doc.content,\n        }\n        for doc in documents\n    ]\n\n    with open(output_path, \"w\") as f:\n        json.dump(data, f, indent=2)\n</code></pre>"},{"location":"user-guide/exporting/#add-custom-fields","title":"Add Custom Fields","text":"<pre><code>def export_with_metadata(documents, output_path, site_name):\n    \"\"\"Export with additional metadata.\"\"\"\n    data = [\n        {\n            \"id\": doc.doc_id,\n            \"source\": site_name,\n            \"url\": doc.url,\n            \"title\": doc.title,\n            \"content\": doc.content,\n            \"crawled_at\": doc.fetched_at.isoformat(),\n            \"word_count\": doc.word_count,\n            \"char_count\": doc.char_count,\n        }\n        for doc in documents\n    ]\n\n    with open(output_path, \"w\") as f:\n        json.dump(data, f, indent=2, default=str)\n</code></pre>"},{"location":"user-guide/exporting/#export-for-rag-systems","title":"Export for RAG Systems","text":""},{"location":"user-guide/exporting/#openailangchain-format","title":"OpenAI/LangChain Format","text":"<pre><code>def export_for_langchain(documents, output_path):\n    \"\"\"Export in LangChain Document format.\"\"\"\n    data = [\n        {\n            \"page_content\": doc.content,\n            \"metadata\": {\n                \"source\": doc.url,\n                \"title\": doc.title,\n                \"language\": doc.language,\n            },\n        }\n        for doc in documents\n    ]\n\n    with open(output_path, \"w\") as f:\n        json.dump(data, f, indent=2)\n</code></pre>"},{"location":"user-guide/exporting/#vector-database-format","title":"Vector Database Format","text":"<pre><code>def export_for_pinecone(documents, chunks):\n    \"\"\"Export chunks with embeddings format.\"\"\"\n    records = []\n\n    for chunk in chunks:\n        doc = next(d for d in documents if d.doc_id == chunk.doc_id)\n        records.append({\n            \"id\": chunk.chunk_id,\n            \"text\": chunk.content,\n            \"metadata\": {\n                \"doc_id\": chunk.doc_id,\n                \"url\": doc.url,\n                \"title\": doc.title,\n                \"heading\": \" &gt; \".join(chunk.heading_path or []),\n                \"chunk_index\": chunk.chunk_index,\n            },\n        })\n\n    return records\n</code></pre>"},{"location":"user-guide/exporting/#incremental-export","title":"Incremental Export","text":""},{"location":"user-guide/exporting/#export-changes-only","title":"Export Changes Only","text":"<pre><code>from ragcrawl.export.events import EventEmitter\n\nemitter = EventEmitter()\nchanged_docs = []\n\n@emitter.on(\"page_changed\")\ndef collect_change(event):\n    changed_docs.append(event.document)\n\n# After sync completes\nif changed_docs:\n    exporter = JSONExporter()\n    exporter.export_documents(changed_docs, Path(\"./changes.json\"))\n</code></pre>"},{"location":"user-guide/exporting/#append-to-jsonl","title":"Append to JSONL","text":"<pre><code>def append_to_jsonl(documents, output_path):\n    \"\"\"Append new documents to existing JSONL file.\"\"\"\n    import json\n\n    with open(output_path, \"a\") as f:\n        for doc in documents:\n            line = json.dumps(doc.model_dump(), default=str)\n            f.write(line + \"\\n\")\n</code></pre>"},{"location":"user-guide/exporting/#compression","title":"Compression","text":""},{"location":"user-guide/exporting/#gzip-export","title":"Gzip Export","text":"<pre><code>import gzip\nimport json\n\ndef export_gzipped(documents, output_path):\n    \"\"\"Export as gzipped JSON.\"\"\"\n    data = [doc.model_dump() for doc in documents]\n\n    with gzip.open(output_path, \"wt\", encoding=\"utf-8\") as f:\n        json.dump(data, f, default=str)\n</code></pre>"},{"location":"user-guide/exporting/#read-gzipped","title":"Read Gzipped","text":"<pre><code>import gzip\nimport json\n\nwith gzip.open(\"docs.json.gz\", \"rt\") as f:\n    documents = json.load(f)\n</code></pre>"},{"location":"user-guide/exporting/#best-practices","title":"Best Practices","text":"<ol> <li>Use JSONL for large datasets: Better for streaming and memory efficiency</li> <li>Include source URLs: Essential for citation and verification</li> <li>Add timestamps: Track when content was crawled</li> <li>Compress large exports: Save disk space and transfer time</li> <li>Export incrementally: Only export changes for efficiency</li> </ol>"},{"location":"user-guide/syncing/","title":"Syncing Guide","text":"<p>Keep your knowledge base up-to-date with incremental syncing.</p>"},{"location":"user-guide/syncing/#how-sync-works","title":"How Sync Works","text":"<p>The sync process efficiently updates your knowledge base:</p> <ol> <li>Sitemap Check: Parse sitemap.xml for recently changed URLs</li> <li>Conditional Requests: Use ETag/Last-Modified headers</li> <li>Content Hashing: Compare content hashes to detect changes</li> <li>Tombstones: Mark deleted pages (404/410 responses)</li> </ol>"},{"location":"user-guide/syncing/#basic-sync","title":"Basic Sync","text":""},{"location":"user-guide/syncing/#cli","title":"CLI","text":"<pre><code># Find your site ID\nragcrawl sites\n\n# Sync the site\nragcrawl sync site_abc123\n\n# Sync with options\nragcrawl sync site_abc123 \\\n    --max-pages 500 \\\n    --max-age 24 \\\n    --output ./updates \\\n    --verbose\n</code></pre>"},{"location":"user-guide/syncing/#cli-options-reference","title":"CLI Options Reference","text":"Option Short Description <code>--storage</code> <code>-s</code> DuckDB storage path (default: <code>~/.ragcrawl/ragcrawl.duckdb</code>) <code>--max-pages</code> <code>-m</code> Maximum pages to sync <code>--max-age</code> Only check pages older than N hours <code>--output</code> <code>-o</code> Output directory for updates <code>--verbose</code> <code>-v</code> Verbose output"},{"location":"user-guide/syncing/#listing-sites-and-runs","title":"Listing Sites and Runs","text":"<p>Before syncing, you may need to find your site ID:</p> <pre><code># List all crawled sites\nragcrawl sites\n\n# List all crawl runs\nragcrawl list\n\n# List runs for a specific site\nragcrawl runs site_abc123 --limit 10\n\n# Filter runs by status\nragcrawl list --status completed\nragcrawl list --site site_abc123\n</code></pre>"},{"location":"user-guide/syncing/#python-api","title":"Python API","text":"<pre><code>import asyncio\nfrom ragcrawl.config.sync_config import SyncConfig\nfrom ragcrawl.core.sync_job import SyncJob\n\nasync def sync_site():\n    config = SyncConfig(\n        site_id=\"site_abc123\",\n    )\n\n    job = SyncJob(config)\n    result = await job.run()\n\n    if result.success:\n        print(f\"Checked: {result.stats.pages_crawled}\")\n        print(f\"Changed: {result.stats.pages_changed}\")\n        print(f\"Deleted: {result.stats.pages_deleted}\")\n\nasyncio.run(sync_site())\n</code></pre>"},{"location":"user-guide/syncing/#sync-strategies","title":"Sync Strategies","text":""},{"location":"user-guide/syncing/#sitemap-based-fastest","title":"Sitemap-Based (Fastest)","text":"<p>Prioritizes pages listed in sitemap.xml with recent lastmod dates:</p> <pre><code>config = SyncConfig(\n    site_id=\"site_abc123\",\n    use_sitemap=True,\n    sitemap_url=\"https://example.com/sitemap.xml\",  # Auto-detected if not specified\n)\n</code></pre>"},{"location":"user-guide/syncing/#conditional-headers","title":"Conditional Headers","text":"<p>Uses HTTP caching headers to skip unchanged content:</p> <pre><code>config = SyncConfig(\n    site_id=\"site_abc123\",\n    use_conditional_requests=True,  # Default: True\n)\n</code></pre> <p>The crawler sends: - <code>If-None-Match: &lt;etag&gt;</code> if ETag is stored - <code>If-Modified-Since: &lt;date&gt;</code> if Last-Modified is stored</p> <p>304 Not Modified responses are skipped efficiently.</p>"},{"location":"user-guide/syncing/#content-hash-diffing","title":"Content Hash Diffing","text":"<p>Compares content hashes for pages without caching headers:</p> <pre><code>config = SyncConfig(\n    site_id=\"site_abc123\",\n    use_hash_diffing=True,  # Default: True\n)\n</code></pre>"},{"location":"user-guide/syncing/#sync-options","title":"Sync Options","text":""},{"location":"user-guide/syncing/#page-limits","title":"Page Limits","text":"<pre><code>config = SyncConfig(\n    site_id=\"site_abc123\",\n    max_pages=500,  # Maximum pages to check\n)\n</code></pre>"},{"location":"user-guide/syncing/#age-based-filtering","title":"Age-Based Filtering","text":"<p>Only check pages older than a certain age:</p> <pre><code>config = SyncConfig(\n    site_id=\"site_abc123\",\n    max_age_hours=24,  # Only check pages not synced in last 24 hours\n)\n</code></pre>"},{"location":"user-guide/syncing/#priority-based-selection","title":"Priority-Based Selection","text":"<p>Check high-priority pages first:</p> <pre><code>config = SyncConfig(\n    site_id=\"site_abc123\",\n    priority_patterns=[\n        r\"/docs/.*\",      # Check docs first\n        r\"/api/.*\",       # Then API docs\n    ],\n)\n</code></pre>"},{"location":"user-guide/syncing/#handling-changes","title":"Handling Changes","text":""},{"location":"user-guide/syncing/#change-events","title":"Change Events","text":"<p>Subscribe to change events during sync:</p> <pre><code>from ragcrawl.export.events import EventEmitter, ChangeEvent\n\nemitter = EventEmitter()\n\n@emitter.on(\"page_changed\")\ndef handle_change(event: ChangeEvent):\n    print(f\"Changed: {event.url}\")\n    print(f\"  Old hash: {event.old_hash}\")\n    print(f\"  New hash: {event.new_hash}\")\n\n@emitter.on(\"page_deleted\")\ndef handle_deletion(event: ChangeEvent):\n    print(f\"Deleted: {event.url}\")\n\nconfig = SyncConfig(\n    site_id=\"site_abc123\",\n    event_emitter=emitter,\n)\n</code></pre>"},{"location":"user-guide/syncing/#output-changes-only","title":"Output Changes Only","text":"<p>Export only changed content:</p> <pre><code>from ragcrawl.config.output_config import OutputConfig, OutputMode\n\nconfig = SyncConfig(\n    site_id=\"site_abc123\",\n    output=OutputConfig(\n        mode=OutputMode.MULTI,\n        root_dir=\"./updates\",\n    ),\n)\n\njob = SyncJob(config)\nresult = await job.run()\n\n# result.changed_pages contains list of changed URLs\n# result.documents contains only changed documents\n</code></pre>"},{"location":"user-guide/syncing/#tombstones","title":"Tombstones","text":"<p>Pages returning 404 or 410 are marked as \"tombstones\":</p> <pre><code># Check for tombstones\nfrom ragcrawl.storage.backend import create_storage_backend\n\nbackend = create_storage_backend(storage_config)\npages = backend.list_pages(site_id)\n\nfor page in pages:\n    if page.is_tombstone:\n        print(f\"Deleted page: {page.url}\")\n</code></pre>"},{"location":"user-guide/syncing/#sync-scheduling","title":"Sync Scheduling","text":""},{"location":"user-guide/syncing/#manual-scheduling","title":"Manual Scheduling","text":"<pre><code># Run via cron\n0 * * * * cd /app &amp;&amp; ragcrawl sync site_abc123 &gt;&gt; /var/log/sync.log 2&gt;&amp;1\n</code></pre>"},{"location":"user-guide/syncing/#python-scheduler","title":"Python Scheduler","text":"<pre><code>import asyncio\nfrom datetime import datetime\n\nasync def scheduled_sync():\n    while True:\n        print(f\"Starting sync at {datetime.now()}\")\n\n        config = SyncConfig(site_id=\"site_abc123\")\n        job = SyncJob(config)\n        result = await job.run()\n\n        if result.stats.pages_changed &gt; 0:\n            print(f\"Updated {result.stats.pages_changed} pages\")\n\n        # Wait 1 hour\n        await asyncio.sleep(3600)\n\nasyncio.run(scheduled_sync())\n</code></pre>"},{"location":"user-guide/syncing/#complete-example","title":"Complete Example","text":"<pre><code>import asyncio\nfrom ragcrawl.config.sync_config import SyncConfig\nfrom ragcrawl.config.output_config import OutputConfig, OutputMode\nfrom ragcrawl.config.storage_config import DuckDBConfig, StorageConfig\nfrom ragcrawl.core.sync_job import SyncJob\nfrom ragcrawl.export.events import EventEmitter\n\nasync def sync_and_export():\n    # Set up event handling\n    emitter = EventEmitter()\n\n    @emitter.on(\"page_changed\")\n    def on_change(event):\n        print(f\"\ud83d\udcdd Updated: {event.url}\")\n\n    @emitter.on(\"page_deleted\")\n    def on_delete(event):\n        print(f\"\ud83d\uddd1\ufe0f Deleted: {event.url}\")\n\n    config = SyncConfig(\n        site_id=\"site_abc123\",\n\n        # Sync options\n        max_pages=1000,\n        max_age_hours=24,\n        use_sitemap=True,\n        use_conditional_requests=True,\n\n        # Storage\n        storage=StorageConfig(\n            backend=DuckDBConfig(path=\"./crawler.duckdb\")\n        ),\n\n        # Output changes\n        output=OutputConfig(\n            mode=OutputMode.MULTI,\n            root_dir=\"./updates\",\n        ),\n\n        event_emitter=emitter,\n    )\n\n    job = SyncJob(config)\n    result = await job.run()\n\n    print(f\"\\nSync Summary:\")\n    print(f\"  Pages checked: {result.stats.pages_crawled}\")\n    print(f\"  Pages changed: {result.stats.pages_changed}\")\n    print(f\"  Pages deleted: {result.stats.pages_deleted}\")\n    print(f\"  Duration: {result.duration_seconds:.1f}s\")\n\n    if result.changed_pages:\n        print(f\"\\nChanged pages:\")\n        for url in result.changed_pages[:10]:\n            print(f\"  - {url}\")\n\nasyncio.run(sync_and_export())\n</code></pre>"}]}